nohup: ignoring input
/home/jingxi/anaconda3/envs/personality_analysis/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Running onpineapple
2018-04-17 08:11:14.511459: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-04-17 08:11:15.076208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:84:00.0
totalMemory: 10.92GiB freeMemory: 10.23GiB
2018-04-17 08:11:15.076290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1
2018-04-17 08:11:16.746219: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1

JingxiNet/JingxiNet/fea_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796454: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/fea_fc1/bias/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796489: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/bias/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796500: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796508: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796517: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796525: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796534: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796542: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796551: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796572: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796581: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/bias/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796605: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/bias/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796614: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796621: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796629: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/biases/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796636: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/biases/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796645: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/w/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796652: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/w/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796660: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/biases/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796668: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/biases/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796676: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/w/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796683: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/w/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796691: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/biases/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796698: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/biases/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796713: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/w/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796721: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/w/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796729: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/bias/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796736: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/bias/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796744: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/Matrix/Momentum/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796751: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/Matrix/Momentum/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796759: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/bias/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796766: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796773: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796780: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/Initializer/random_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796788: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/Initializer/random_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/Initializer/random_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796795: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/Initializer/random_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796802: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796809: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796819: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796827: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796834: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796841: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796848: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796855: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796862: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796869: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796876: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796883: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796909: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796915: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796922: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796929: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796939: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796946: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796953: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/Initializer/random_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796960: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/Initializer/random_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/Initializer/random_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796966: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/Initializer/random_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796973: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796980: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796986: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.796993: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Initializer/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797000: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Initializer/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Initializer/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797007: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Initializer/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Initializer/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797014: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Initializer/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Initializer/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797021: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Initializer/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797027: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797034: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797043: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797050: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Initializer/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797057: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Initializer/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Initializer/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797064: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Initializer/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Initializer/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797071: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Initializer/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Initializer/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797077: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Initializer/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797084: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797090: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797097: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797104: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Initializer/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797110: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Initializer/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Initializer/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797117: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Initializer/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Initializer/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797124: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Initializer/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Initializer/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797131: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Initializer/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797137: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797144: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797154: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797161: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797168: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/Initializer/random_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797175: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/Initializer/random_normal/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/Initializer/random_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797182: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/Initializer/random_normal: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797189: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Variable: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:16.797197: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Variable: (VariableV2)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/Variable/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:16.797204: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Variable/Assign: (Assign)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797216: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/init: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/fea_fc1/bias/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797224: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/bias/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797231: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797238: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797244: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797251: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/bias/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797261: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/bias/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797268: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/biases/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797275: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/biases/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/w/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797282: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/w/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/biases/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797289: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/biases/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/w/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797295: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/w/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/biases/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797302: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/biases/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/w/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797308: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/w/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/bias/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797315: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/bias/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/Matrix/Momentum/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797322: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/Matrix/Momentum/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/bias/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797329: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias/Initializer/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/Initializer/random_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797335: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/Initializer/random_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/Initializer/random_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797342: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/Initializer/random_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/Initializer/random_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797349: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/Initializer/random_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797358: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797365: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797372: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/Initializer/random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797379: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias/Initializer/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797385: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797392: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797399: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/Initializer/random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797405: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias/Initializer/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/Initializer/random_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797412: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/Initializer/random_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/Initializer/random_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797419: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/Initializer/random_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/Initializer/random_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797426: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/Initializer/random_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797433: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases/Initializer/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Initializer/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797439: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Initializer/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Initializer/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797446: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Initializer/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/Initializer/random_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797456: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/Initializer/random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797463: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases/Initializer/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Initializer/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797470: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Initializer/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Initializer/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797476: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Initializer/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/Initializer/random_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797483: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/Initializer/random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797489: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases/Initializer/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Initializer/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797496: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Initializer/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Initializer/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797503: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Initializer/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/Initializer/random_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797509: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/Initializer/random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797516: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias/Initializer/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/Initializer/random_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797522: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/Initializer/random_normal/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/Initializer/random_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797529: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/Initializer/random_normal/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/Initializer/random_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:16.797536: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/Initializer/random_normal/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Variable/initial_value: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:16.797544: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Variable/initial_value: (Const)/job:localhost/replica:0/task:0/device:CPU:0
Graph defined and initialized in 3.17039704323s.
[<tf.Variable 'JingxiNet/Variable:0' shape=() dtype=int32_ref>, <tf.Variable 'JingxiNet/au_fc1/Matrix:0' shape=(68, 32) dtype=float32_ref>, <tf.Variable 'JingxiNet/au_fc1/bias:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_conv1/w:0' shape=(5, 5, 3, 16) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_conv1/biases:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_conv2/w:0' shape=(7, 7, 16, 16) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_conv2/biases:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_conv3/w:0' shape=(9, 9, 16, 16) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_conv3/biases:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_fc1/Matrix:0' shape=(1024, 128) dtype=float32_ref>, <tf.Variable 'JingxiNet/vd_fc1/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'JingxiNet/rnn/lstm_cell/kernel:0' shape=(288, 640) dtype=float32_ref>, <tf.Variable 'JingxiNet/rnn/lstm_cell/bias:0' shape=(640,) dtype=float32_ref>, <tf.Variable 'JingxiNet/rnn/lstm_cell/projection/kernel:0' shape=(160, 128) dtype=float32_ref>, <tf.Variable 'JingxiNet/fea_fc1/Matrix:0' shape=(128, 5) dtype=float32_ref>, <tf.Variable 'JingxiNet/fea_fc1/bias:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/au_fc1/Matrix/Momentum:0' shape=(68, 32) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/au_fc1/bias/Momentum:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_conv1/w/Momentum:0' shape=(5, 5, 3, 16) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_conv1/biases/Momentum:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_conv2/w/Momentum:0' shape=(7, 7, 16, 16) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_conv2/biases/Momentum:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_conv3/w/Momentum:0' shape=(9, 9, 16, 16) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_conv3/biases/Momentum:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum:0' shape=(1024, 128) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/vd_fc1/bias/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum:0' shape=(288, 640) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum:0' shape=(640,) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum:0' shape=(160, 128) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum:0' shape=(128, 5) dtype=float32_ref>, <tf.Variable 'JingxiNet/JingxiNet/fea_fc1/bias/Momentum:0' shape=(5,) dtype=float32_ref>]
epoch:0, batch1, load frames use: 3.9805150032s
JingxiNet/JingxiNet/fea_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807735: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807769: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807778: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807786: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807809: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807833: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807848: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807868: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807875: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807882: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807890: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807897: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807905: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807913: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807920: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807934: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc_2: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807944: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc_2: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807953: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc_2: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807966: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc_2: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc_1: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807975: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc_1: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807983: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.807990: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808000: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808007: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808015: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808024: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc_2: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808033: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc_2: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808042: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808049: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808057: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808069: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808077: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808085: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808093: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808101: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808109: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808118: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808126: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808133: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc_1: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808142: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc_1: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808181: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808192: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808201: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808209: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808232: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808241: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808249: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808257: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808266: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808274: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808282: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc_1: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808291: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc_1: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808299: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808310: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808320: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808327: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808335: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc_1: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808344: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc_1: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808352: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808360: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808368: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808376: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808383: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808392: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc_2: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808405: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc_2: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808414: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808422: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808429: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808438: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808446: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808453: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc_1: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808463: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc_1: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808470: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808478: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808487: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808498: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808507: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808516: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808524: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808531: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc: (StackV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808540: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc: (StackV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808548: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808556: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808565: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/GreaterEqual/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808575: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/GreaterEqual/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/f_count_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808583: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/f_count_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Merge: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808592: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Merge: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Fill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808605: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Fill: (Fill)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay/Cast_1: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808614: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/Cast_1: (Cast)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/RealDiv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808622: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/RealDiv: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808631: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808640: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808648: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808658: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808665: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808673: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808682: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808691: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808700: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/transpose_grad/InvertPermutation: (InvertPermutation): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808709: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/transpose_grad/InvertPermutation: (InvertPermutation)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808717: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808726: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808736: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808751: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808759: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808767: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808776: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808783: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808792: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808801: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808810: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808819: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808829: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808854: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808863: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/transpose_grad/InvertPermutation: (InvertPermutation): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808872: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/transpose_grad/InvertPermutation: (InvertPermutation)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808881: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/mod: (FloorMod): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808890: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/mod: (FloorMod)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808901: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808909: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808917: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808925: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808932: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808940: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808948: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808956: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808963: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808971: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808979: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808987: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.808995: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809002: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809010: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809018: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809025: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809037: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809044: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809052: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809061: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/ShapeN: (ShapeN): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809098: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/ShapeN: (ShapeN)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809107: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809115: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/Reshape_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809124: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/Reshape_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809132: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809141: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/MaxPool: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809149: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/MaxPool: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/ShapeN: (ShapeN): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809157: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/ShapeN: (ShapeN)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809166: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809174: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Reshape_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809182: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Reshape_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809190: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu_1: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809202: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu_1: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/MaxPool_1: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809211: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/MaxPool_1: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/ShapeN: (ShapeN): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809219: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/ShapeN: (ShapeN)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809227: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809235: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Reshape_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809243: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Reshape_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809252: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu_2: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809260: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu_2: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/MaxPool_2: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809268: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/MaxPool_2: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_2_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809276: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_2_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_2: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809285: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_2: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809293: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809301: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809310: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809318: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu_3: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809327: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu_3: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809337: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809346: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809354: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809363: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809371: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/ShapeN: (ShapeN): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809379: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/ShapeN: (ShapeN)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/ConcatOffset: (ConcatOffset): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809387: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/ConcatOffset: (ConcatOffset)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809396: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809404: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/Neg: (Neg)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/RealDiv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809412: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/RealDiv_1: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/RealDiv_2: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809421: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/RealDiv_2: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809429: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809438: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809446: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/div: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809454: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809466: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809474: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809483: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809491: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809499: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809507: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/Floor: (Floor)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809516: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809524: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809532: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_3_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809540: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_3_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_3: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809548: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_3: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809556: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809564: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809572: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809580: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809595: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809607: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1: (StridedSlice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Less/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809615: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Less/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Less: (Less): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809624: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Less: (Less)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/LoopCond: (LoopCond): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809632: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/LoopCond: (LoopCond)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809640: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/f_count_2: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809648: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/f_count_2: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/b_count_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809656: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/b_count_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Merge_1: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809664: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Merge_1: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/GreaterEqual: (GreaterEqual): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809672: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/GreaterEqual: (GreaterEqual)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/b_count_2: (LoopCond): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809680: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/b_count_2: (LoopCond)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809689: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc_3: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809697: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc_3: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809705: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc_3: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809713: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc_3: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809725: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc_3: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809733: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc_3: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809741: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc_3: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809750: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc_3: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Switch_1: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809758: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Switch_1: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809766: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2_1: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809790: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2_1: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809797: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN: (ShapeN): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809806: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN: (ShapeN)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/mod: (FloorMod): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809815: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/mod: (FloorMod)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ConcatOffset: (ConcatOffset): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809823: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ConcatOffset: (ConcatOffset)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809831: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809838: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809852: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809860: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809867: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809875: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809882: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809890: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809898: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809905: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809912: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809920: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809928: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809939: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809947: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809955: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809962: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809970: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809977: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2_1: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809984: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.809993: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810001: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2: (StackPopV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810008: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2: (StackPopV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/b_sync: (ControlTrigger): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810020: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/b_sync: (ControlTrigger)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/NextIteration_1: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810029: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/NextIteration_1: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810036: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810044: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810052: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810060: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810069: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810077: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArray_1: (TensorArrayV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810085: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArray_1: (TensorArrayV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810092: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810100: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3: (TensorArrayGatherV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810107: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3: (TensorArrayGatherV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810116: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810127: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/transpose_grad/transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810136: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/transpose_grad/transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_3_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810144: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_3_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810153: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810162: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810170: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810179: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810187: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810196: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810204: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/mul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810212: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/mul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810220: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810229: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810241: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/RealDiv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810250: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/RealDiv: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810258: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810266: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810275: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810282: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/Slice_1: (Slice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810291: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/Slice_1: (Slice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/Slice: (Slice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810299: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/Slice: (Slice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810308: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810315: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Relu_3_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810324: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Relu_3_grad/ReluGrad: (ReluGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810333: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810341: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810350: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810362: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810371: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810378: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810385: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810394: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810403: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810411: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810419: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810445: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/MatMul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_2_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810454: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_2_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/MaxPool_2_grad/MaxPoolGrad: (MaxPoolGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810462: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/MaxPool_2_grad/MaxPoolGrad: (MaxPoolGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Relu_2_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810470: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Relu_2_grad/ReluGrad: (ReluGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Reshape_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810482: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Reshape_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810495: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/BiasAddGrad: (BiasAddGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810504: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810511: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810518: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/BiasAdd_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810529: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/Conv2DBackpropInput: (Conv2DBackpropInput): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810538: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/Conv2DBackpropInput: (Conv2DBackpropInput)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810546: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810553: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810560: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv3/Conv2D_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/MaxPool_1_grad/MaxPoolGrad: (MaxPoolGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810568: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/MaxPool_1_grad/MaxPoolGrad: (MaxPoolGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Relu_1_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810579: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Relu_1_grad/ReluGrad: (ReluGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Reshape_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810587: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Reshape_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810599: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/BiasAddGrad: (BiasAddGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810608: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810615: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810623: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/BiasAdd_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810632: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/Conv2DBackpropInput: (Conv2DBackpropInput): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810641: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/Conv2DBackpropInput: (Conv2DBackpropInput)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810649: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810656: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810663: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv2/Conv2D_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/MaxPool_grad/MaxPoolGrad: (MaxPoolGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810672: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/MaxPool_grad/MaxPoolGrad: (MaxPoolGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Relu_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810680: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Relu_grad/ReluGrad: (ReluGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/Reshape_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810688: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/Reshape_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810696: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/BiasAddGrad: (BiasAddGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810709: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810716: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810723: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/BiasAdd_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810731: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/Conv2DBackpropInput: (Conv2DBackpropInput): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810740: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/Conv2DBackpropInput: (Conv2DBackpropInput)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810748: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810755: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_conv1/Conv2D_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810762: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810771: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810782: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810790: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810798: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810810: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810817: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810825: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810833: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810844: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810853: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810860: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/MatMul_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810867: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayReadV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810874: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayReadV3/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3: (TensorArrayScatterV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810881: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3: (TensorArrayScatterV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810891: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810898: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/gradient_flow: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810908: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayGrad/gradient_flow: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayReadV3/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810917: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayReadV3/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayReadV3: (TensorArrayReadV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810924: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayReadV3: (TensorArrayReadV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810932: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArray: (TensorArrayV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810940: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArray: (TensorArrayV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810947: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810954: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810962: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge_1: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810987: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge_1: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch_1: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.810995: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch_1: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Exit_1: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811006: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Exit_1: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811014: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811021: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/TensorArraySizeV3: (TensorArraySizeV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811028: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/TensorArraySizeV3: (TensorArraySizeV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811039: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3: (TensorArrayGatherV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811047: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3: (TensorArrayGatherV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811055: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_4_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811064: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_4_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_4: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811073: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_4: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811082: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Neg: (Neg)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/RealDiv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811091: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/RealDiv_1: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/RealDiv_2: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811100: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/RealDiv_2: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811108: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811117: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811126: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/div: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811134: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811143: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811152: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811164: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811173: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811181: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811190: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/Floor: (Floor)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811201: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811210: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811218: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811227: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811236: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811244: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811253: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811263: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Sigmoid: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_5_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811272: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_5_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_5: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811281: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_5: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811290: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811318: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811330: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811338: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811346: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2: (ExpandDims)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811355: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros_1: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811363: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros_1: (Fill)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter_3: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811371: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter_3: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge_3: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811380: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge_3: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch_3: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811387: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch_3: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Exit_3: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811395: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Exit_3: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/zeros_like_1: (ZerosLike): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811403: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/zeros_like_1: (ZerosLike)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Exit_3_grad/b_exit: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811411: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Exit_3_grad/b_exit: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Switch_3_grad/b_switch: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811420: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Switch_3_grad/b_switch: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_3_grad/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811427: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_3_grad/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_3_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811435: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_3_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_3_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811442: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_3_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity_3: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811453: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity_3: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPushV2_1: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811461: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/StackPushV2_1: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811468: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/concat: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811475: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811484: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811492: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/split: (Split): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811501: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/split: (Split)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811509: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811516: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_2: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811523: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_2: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811530: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811537: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811544: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Tanh: (Tanh): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811560: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Tanh: (Tanh)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811567: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811574: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811581: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_1: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811589: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_1: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811596: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811603: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811610: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811618: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811625: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811632: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811641: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811651: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811659: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811666: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811673: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811681: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811690: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811698: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros: (Fill)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter_2: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811706: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter_2: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge_2: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811714: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge_2: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch_2: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811722: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch_2: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Exit_2: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811730: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Exit_2: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811739: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Exit_2_grad/b_exit: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811747: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Exit_2_grad/b_exit: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Switch_2_grad/b_switch: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811755: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Switch_2_grad/b_switch: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_2_grad/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811766: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_2_grad/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_2_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811775: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_2_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_2_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811782: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_2_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811789: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity_2: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811796: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811804: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811811: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811819: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811826: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811833: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/add_1: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811846: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/add_1: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration_2: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811873: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration_2: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Tanh_1: (Tanh): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811880: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Tanh_1: (Tanh)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811891: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811899: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811906: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/StackPushV2_1: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/mul_2: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811914: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/mul_2: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/StackPushV2: (StackPushV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811921: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/StackPushV2: (StackPushV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811930: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811938: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration_3: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811947: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration_3: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811954: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration_1: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811963: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration_1: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.811972: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/gradients/JingxiNet/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811981: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811990: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Square: (Square): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.811999: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Square: (Square)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Sum_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812011: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Sum_grad/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812020: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812028: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/Neg: (Neg)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/RealDiv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812037: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/RealDiv_1: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/RealDiv_2: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812046: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/RealDiv_2: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812054: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812063: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812072: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812081: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812089: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Sum_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812097: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Sum_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Sum_grad/Tile: (Tile): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812106: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Sum_grad/Tile: (Tile)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Square_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812115: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Square_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Square_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812124: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Square_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812133: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812166: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/Neg: (Neg)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812175: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812184: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812192: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812201: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/sub_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812208: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/sub_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_5_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812216: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_5_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Sigmoid_grad/SigmoidGrad: (SigmoidGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812225: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Sigmoid_grad/SigmoidGrad: (SigmoidGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812234: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812242: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812262: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812270: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812279: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812286: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812297: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812306: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812314: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812323: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812330: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812337: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/MatMul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812346: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812354: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812363: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812371: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812379: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812388: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812397: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812407: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/mul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812416: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812425: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812433: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/RealDiv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812441: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/RealDiv: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812450: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812458: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812467: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812474: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Reshape_4_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812483: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Reshape_4_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/transpose_grad/transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812491: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/transpose_grad/transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3: (TensorArrayScatterV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812499: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3: (TensorArrayScatterV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Exit_1_grad/b_exit: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812507: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Exit_1_grad/b_exit: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Switch_1_grad/b_switch: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812519: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Switch_1_grad/b_switch: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_1_grad/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812527: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_1_grad/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812535: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Merge_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812542: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Merge_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812549: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3: (TensorArrayGradV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/gradient_flow: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812557: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/gradient_flow: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3: (TensorArrayReadV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812564: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3: (TensorArrayReadV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812573: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812580: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Switch_1_grad_1/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812589: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Switch_1_grad_1/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812596: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/AddN: (AddN): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812606: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/AddN: (AddN)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812615: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812623: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812632: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812639: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/Add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812648: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/Add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812656: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812664: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812672: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812681: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812689: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812698: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812712: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812721: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812730: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812737: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Tanh_1_grad/TanhGrad: (TanhGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812746: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Tanh_1_grad/TanhGrad: (TanhGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/AddN_1: (AddN): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812754: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/AddN_1: (AddN)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812762: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812771: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812780: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812788: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812797: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812804: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812813: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812824: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812833: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812842: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812850: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812858: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812867: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812874: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Tanh_grad/TanhGrad: (TanhGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812883: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Tanh_grad/TanhGrad: (TanhGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812890: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_1_grad/SigmoidGrad: (SigmoidGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812899: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_1_grad/SigmoidGrad: (SigmoidGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812908: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812917: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812946: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812957: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812966: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812975: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812984: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.812993: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813001: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Switch_2_grad_1/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813010: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Switch_2_grad_1/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813018: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_grad/SigmoidGrad: (SigmoidGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813027: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_grad/SigmoidGrad: (SigmoidGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813036: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813045: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813057: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813066: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813077: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813085: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813093: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_2_grad/SigmoidGrad: (SigmoidGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813102: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_2_grad/SigmoidGrad: (SigmoidGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/split_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813111: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/split_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813120: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/BiasAddGrad: (BiasAddGrad)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813129: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813136: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/Add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813146: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/Add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813157: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813169: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813178: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813187: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813198: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813206: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/Add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813215: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/Add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813224: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813251: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/Slice_1: (Slice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813259: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/Slice_1: (Slice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/Slice: (Slice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813268: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/Slice: (Slice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813279: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813290: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/Switch_3_grad_1/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813299: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/Switch_3_grad_1/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813306: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813314: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/Add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813323: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/Add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813331: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813340: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/div: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Variable: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.813347: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Variable: (VariableV2)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/Variable/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.813355: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Variable/read: (Identity)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/ExponentialDecay/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813364: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/Cast: (Cast)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813373: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/truediv: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813382: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/Floor: (Floor)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay/Pow: (Pow): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813390: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/Pow: (Pow)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813399: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/fea_fc1/bias/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813410: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/fea_fc1/bias/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/fea_fc1/Matrix/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813418: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/fea_fc1/Matrix/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/rnn/lstm_cell/projection/kernel/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813426: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/rnn/lstm_cell/projection/kernel/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/rnn/lstm_cell/bias/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813434: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/rnn/lstm_cell/bias/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/rnn/lstm_cell/kernel/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813443: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/rnn/lstm_cell/kernel/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_fc1/bias/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813453: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_fc1/bias/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_fc1/Matrix/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813461: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_fc1/Matrix/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_conv3/biases/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813469: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_conv3/biases/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_conv3/w/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813476: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_conv3/w/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_conv2/biases/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813483: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_conv2/biases/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_conv2/w/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813491: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_conv2/w/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_conv1/biases/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813500: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_conv1/biases/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/vd_conv1/w/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813507: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/vd_conv1/w/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/au_fc1/bias/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813518: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/au_fc1/bias/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update_JingxiNet/au_fc1/Matrix/ApplyMomentum: (ApplyMomentum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813526: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update_JingxiNet/au_fc1/Matrix/ApplyMomentum: (ApplyMomentum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/update: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813535: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/update: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum: (AssignAdd): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.813542: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum: (AssignAdd)/job:localhost/replica:0/task:0/device:CPU:0
_arg_JingxiNet/audio_pl_0_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.813549: I tensorflow/core/common_runtime/placer.cc:874] _arg_JingxiNet/audio_pl_0_0: (_Arg)/job:localhost/replica:0/task:0/device:CPU:0
_arg_JingxiNet/ground_truth_0_1: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.813556: I tensorflow/core/common_runtime/placer.cc:874] _arg_JingxiNet/ground_truth_0_1: (_Arg)/job:localhost/replica:0/task:0/device:CPU:0
_arg_JingxiNet/video_pl_0_2: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.813563: I tensorflow/core/common_runtime/placer.cc:874] _arg_JingxiNet/video_pl_0_2: (_Arg)/job:localhost/replica:0/task:0/device:CPU:0
_retval_JingxiNet/div_0_0: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.813570: I tensorflow/core/common_runtime/placer.cc:874] _retval_JingxiNet/div_0_0: (_Retval)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/Momentum/momentum: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813580: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/momentum: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813589: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/vd_fc1/add_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813598: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/au_fc1/add_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813606: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout/div_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813615: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout/div_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813624: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayReadV3/Enter_1_grad/b_acc: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813633: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter_grad/b_acc: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc_1/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813645: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc_1/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813655: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/ShapeN/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813679: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter_grad/b_acc: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813687: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_grad/MatMul_1/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813695: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/BroadcastGradientArgs/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813703: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul_1/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813710: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/mul/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813718: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813726: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_1_grad/BroadcastGradientArgs/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813734: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul_1/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813743: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/mul/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813757: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813765: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_grad/BroadcastGradientArgs/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813772: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813780: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_1_grad/BroadcastGradientArgs/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813790: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter_grad/b_acc: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813800: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul_1/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813808: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/mul/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813815: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc_1/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813823: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/mul_2_grad/BroadcastGradientArgs/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813831: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1_grad/MatMul_1/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc/max_size: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813839: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc/max_size: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813860: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/dropout_1/div_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813869: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/fea_fc1/add_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Sum_grad/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813878: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Sum_grad/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813888: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/div_grad/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813899: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/div_grad/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/b_count: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813908: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/b_count: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/f_count: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813917: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/f_count: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813925: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813934: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay/Cast_2/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813943: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/Cast_2/x: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay/Cast_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813952: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/Cast_1/x: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/ExponentialDecay/learning_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813961: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/ExponentialDecay/learning_rate: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/div/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813970: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/div/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813979: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_5/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813988: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_5/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.813997: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814010: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/keep_prob: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814021: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/keep_prob: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_4/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814031: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_4/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/concat_1/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814040: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/concat_1/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/concat_1/values_0: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814048: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/concat_1/values_0: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814057: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814066: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814075: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814082: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814090: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814098: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814108: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814117: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814126: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814135: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/time: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814148: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/time: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1/stack_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814158: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1/stack_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1/stack_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814167: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1/stack_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1/stack: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814176: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1/stack: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros_1/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814185: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros_1/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat_1/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814194: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat_1/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/Const_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814205: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/Const_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2/dim: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814216: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2/dim: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814225: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814234: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814243: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814252: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814261: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814269: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814279: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat_1/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814290: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat_1/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat_1/values_0: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814304: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat_1/values_0: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814313: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814321: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814330: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_3/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814339: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_3/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814348: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814357: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/keep_prob: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814366: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/keep_prob: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814375: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_2/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814384: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_2/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814396: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814405: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814414: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_1/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814423: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_1/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814432: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/mod/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814441: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/mod/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814450: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/concat_grad/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/split_grad/concat/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814463: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/split_grad/concat/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814473: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/rnn/while/rnn/lstm_cell/add_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/Add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814483: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/Add/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814492: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/add/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814501: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/add/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/split/split_dim: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814510: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/split/split_dim: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/concat/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814519: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/concat/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/gradients/JingxiNet/Square_grad/mul/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:21.814528: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/gradients/JingxiNet/Square_grad/mul/x: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Momentum/value: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:21.814536: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Momentum/value: (Const)/job:localhost/replica:0/task:0/device:CPU:0
step 0/400000: train loss: 0.715272
JingxiNet/fea_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840492: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840532: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840541: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840548: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840559: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840570: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840605: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840628: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840636: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840644: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840650: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840657: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840665: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840671: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840678: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840686: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840694: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840701: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840710: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840717: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840724: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840730: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840741: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840747: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840756: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840762: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840777: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840783: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840790: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840796: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840802: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840809: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840815: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840821: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840828: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840834: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840841: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840847: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840853: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840867: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840874: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840882: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840890: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840897: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840904: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840912: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/MaxPool: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840919: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/MaxPool: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840927: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840935: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840944: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu_1: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840951: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu_1: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/MaxPool_1: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840959: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/MaxPool_1: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840967: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/Conv2D: (Conv2D)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840974: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840981: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu_2: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840989: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu_2: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/MaxPool_2: (MaxPool): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.840999: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/MaxPool_2: (MaxPool)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_2: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841007: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_2: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841014: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841022: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Relu_3: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841029: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Relu_3: (Relu)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841037: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841044: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841052: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841060: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841067: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/div: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841075: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841081: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841089: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841096: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841103: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841111: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/Floor: (Floor)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841118: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_3: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841125: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_3: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841137: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841157: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841171: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841184: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841192: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/Shape_1: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841199: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1: (StridedSlice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Less/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841206: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Less/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Less: (Less): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841214: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Less: (Less)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/LoopCond: (LoopCond): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841221: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/LoopCond: (LoopCond)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841228: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841235: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841242: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841250: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArray_1: (TensorArrayV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841256: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArray_1: (TensorArrayV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayReadV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841263: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayReadV3/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3: (TensorArrayScatterV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841269: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3: (TensorArrayScatterV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayReadV3/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841280: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayReadV3/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayReadV3: (TensorArrayReadV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841287: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayReadV3: (TensorArrayReadV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArray: (TensorArrayV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841293: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArray: (TensorArrayV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841300: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3/Enter: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter_1: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841308: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter_1: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge_1: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841315: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge_1: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch_1: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841321: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch_1: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Exit_1: (Exit): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841331: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Exit_1: (Exit)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/TensorArraySizeV3: (TensorArraySizeV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841338: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/TensorArraySizeV3: (TensorArraySizeV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841344: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/range: (Range)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3: (TensorArrayGatherV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841350: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/TensorArrayGatherV3: (TensorArrayGatherV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841358: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_4: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841365: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_4: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841373: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/div: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841380: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841389: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841397: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841405: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841412: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841420: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/Floor: (Floor)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841427: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841435: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841442: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841449: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Sigmoid: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_5: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841457: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_5: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841464: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity_1: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841472: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/Shape: (Shape)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841479: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841486: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2: (ExpandDims)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841494: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros_1: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841502: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros_1: (Fill)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter_3: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841509: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter_3: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge_3: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841520: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge_3: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch_3: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841528: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch_3: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity_3: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841536: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity_3: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841544: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/concat: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841564: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841571: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/BiasAdd: (BiasAdd)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/split: (Split): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841579: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/split: (Split)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_2: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841586: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_2: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Tanh: (Tanh): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841594: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Tanh: (Tanh)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_1: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841601: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid_1: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841609: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/mul_1: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841616: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841624: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Sigmoid: (Sigmoid)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841631: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841639: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat: (ConcatV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841647: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros: (Fill)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Enter_2: (Enter): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841657: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Enter_2: (Enter)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Merge_2: (Merge): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841665: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Merge_2: (Merge)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Switch_2: (Switch): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841671: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Switch_2: (Switch)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841678: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/Identity_2: (Identity)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841686: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/add_1: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841693: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/add_1: (Add)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration_2: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841703: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration_2: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/Tanh_1: (Tanh): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841711: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/Tanh_1: (Tanh)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/mul_2: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841718: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/mul_2: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841724: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration_3: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841732: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration_3: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841738: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/NextIteration_1: (NextIteration): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841746: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/NextIteration_1: (NextIteration)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841753: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Square: (Square): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841761: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Square: (Square)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841768: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841780: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/div: (RealDiv)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/training_loss: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.841788: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/training_loss: (ScalarSummary)/job:localhost/replica:0/task:0/device:CPU:0
_arg_JingxiNet/audio_pl_0_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.841795: I tensorflow/core/common_runtime/placer.cc:874] _arg_JingxiNet/audio_pl_0_0: (_Arg)/job:localhost/replica:0/task:0/device:CPU:0
_arg_JingxiNet/ground_truth_0_1: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.841801: I tensorflow/core/common_runtime/placer.cc:874] _arg_JingxiNet/ground_truth_0_1: (_Arg)/job:localhost/replica:0/task:0/device:CPU:0
_arg_JingxiNet/video_pl_0_2: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.841807: I tensorflow/core/common_runtime/placer.cc:874] _arg_JingxiNet/video_pl_0_2: (_Arg)/job:localhost/replica:0/task:0/device:CPU:0
_retval_JingxiNet/div_0_0: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.841813: I tensorflow/core/common_runtime/placer.cc:874] _retval_JingxiNet/div_0_0: (_Retval)/job:localhost/replica:0/task:0/device:CPU:0
_retval_JingxiNet/training_loss_0_1: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.841819: I tensorflow/core/common_runtime/placer.cc:874] _retval_JingxiNet/training_loss_0_1: (_Retval)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/training_loss/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.841826: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/training_loss/tags: (Const)/job:localhost/replica:0/task:0/device:CPU:0
JingxiNet/div/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841834: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/div/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841841: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_5/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841849: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_5/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841856: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841864: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout_1/keep_prob: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841871: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout_1/keep_prob: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_4/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841879: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_4/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/concat_1/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841887: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/concat_1/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/concat_1/values_0: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841896: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/concat_1/values_0: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841907: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841915: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841922: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841928: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayStack/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841935: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayStack/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841946: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841953: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841961: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841969: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841976: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/TensorArrayUnstack/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/time: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841983: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/time: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1/stack_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841991: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1/stack_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1/stack_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.841998: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1/stack_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice_1/stack: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842005: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice_1/stack: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros_1/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842013: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros_1/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat_1/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842027: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat_1/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/Const_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842035: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/Const_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2/dim: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842042: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims_2/dim: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842050: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/zeros/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/concat/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842058: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/concat/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842065: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/LSTMCellZeroState/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842072: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/LSTMCellZeroState/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842082: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842089: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842097: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat_1/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842104: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat_1/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat_1/values_0: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842112: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat_1/values_0: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842119: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/range/delta: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842126: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/range/start: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Rank: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842134: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Rank: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_3/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842141: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_3/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842148: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842159: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/dropout/keep_prob: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842167: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/dropout/keep_prob: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/concat/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842174: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/concat/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_2/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842182: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_2/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842189: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842219: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842227: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape_1/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842235: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape_1/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842243: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842250: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/add/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842258: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/add/y: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/split/split_dim: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842266: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/split/split_dim: (Const)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/while/rnn/lstm_cell/concat/axis: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.842275: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/while/rnn/lstm_cell/concat/axis: (Const)/job:localhost/replica:0/task:0/device:GPU:0
step 0/400000: validation loss: 0.702539
JingxiNet/JingxiNet/fea_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921336: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921386: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/fea_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921416: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/projection/kernel/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921429: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921441: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/rnn/lstm_cell/kernel/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921453: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921464: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921476: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv3/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921488: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv3/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921499: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv2/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921511: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv2/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/biases/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921523: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/biases/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/vd_conv1/w/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921534: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/vd_conv1/w/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/bias/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921546: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/bias/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/JingxiNet/au_fc1/Matrix/Momentum: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921557: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/JingxiNet/au_fc1/Matrix/Momentum: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921569: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/fea_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921581: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/fea_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921598: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/projection/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921610: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/rnn/lstm_cell/kernel: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921622: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/rnn/lstm_cell/kernel: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921634: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921646: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921658: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv3/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921669: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv3/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921681: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv2/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921693: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv2/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/biases: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921704: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/biases: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/vd_conv1/w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921716: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/vd_conv1/w: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/bias: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921727: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/bias: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/au_fc1/Matrix: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-17 08:11:24.921744: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/au_fc1/Matrix: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0
JingxiNet/Variable: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.921789: I tensorflow/core/common_runtime/placer.cc:874] JingxiNet/Variable: (VariableV2)/job:localhost/replica:0/task:0/device:CPU:0
save/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.921823: I tensorflow/core/common_runtime/placer.cc:874] save/SaveV2: (SaveV2)/job:localhost/replica:0/task:0/device:CPU:0
save/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.921852: I tensorflow/core/common_runtime/placer.cc:874] save/control_dependency: (Identity)/job:localhost/replica:0/task:0/device:CPU:0
_arg_save/Const_0_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.921871: I tensorflow/core/common_runtime/placer.cc:874] _arg_save/Const_0_0: (_Arg)/job:localhost/replica:0/task:0/device:CPU:0
_retval_save/control_dependency_0_0: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.921896: I tensorflow/core/common_runtime/placer.cc:874] _retval_save/control_dependency_0_0: (_Retval)/job:localhost/replica:0/task:0/device:CPU:0
save/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.921919: I tensorflow/core/common_runtime/placer.cc:874] save/SaveV2/shape_and_slices: (Const)/job:localhost/replica:0/task:0/device:CPU:0
save/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-04-17 08:11:24.921940: I tensorflow/core/common_runtime/placer.cc:874] save/SaveV2/tensor_names: (Const)/job:localhost/replica:0/task:0/device:CPU:0
epoch:0, batch2, load frames use: 4.49411296844s
step 1/400000: train loss: 0.651064
epoch:0, batch3, load frames use: 4.08237504959s
step 2/400000: train loss: 0.671490
epoch:0, batch4, load frames use: 4.13413596153s
step 3/400000: train loss: 0.690777
epoch:0, batch5, load frames use: 4.40944194794s
step 4/400000: train loss: 0.681350
epoch:0, batch6, load frames use: 4.08289599419s
step 5/400000: train loss: 0.658795
epoch:0, batch7, load frames use: 5.84189581871s
step 6/400000: train loss: 0.607464
epoch:0, batch8, load frames use: 3.97043204308s
step 7/400000: train loss: 0.652516
epoch:0, batch9, load frames use: 4.73575687408s
step 8/400000: train loss: 0.711317
epoch:0, batch10, load frames use: 4.9720261097s
step 9/400000: train loss: 0.689042
epoch:0, batch11, load frames use: 4.26797008514s
step 10/400000: train loss: 0.612874
epoch:0, batch12, load frames use: 4.15258288383s
step 11/400000: train loss: 0.808179
epoch:0, batch13, load frames use: 4.31380701065s
step 12/400000: train loss: 0.641255
epoch:0, batch14, load frames use: 4.1967010498s
step 13/400000: train loss: 0.769057
epoch:0, batch15, load frames use: 4.51182484627s
step 14/400000: train loss: 0.651569
epoch:0, batch16, load frames use: 4.41202306747s
step 15/400000: train loss: 0.715437
epoch:0, batch17, load frames use: 4.65144395828s
step 16/400000: train loss: 0.711825
epoch:0, batch18, load frames use: 4.65120601654s
step 17/400000: train loss: 0.659810
epoch:0, batch19, load frames use: 4.49109888077s
step 18/400000: train loss: 0.641283
epoch:0, batch20, load frames use: 5.09244894981s
step 19/400000: train loss: 0.848350
epoch:0, batch21, load frames use: 4.46326398849s
step 20/400000: train loss: 0.624283
epoch:0, batch22, load frames use: 4.68821501732s
step 21/400000: train loss: 0.684851
epoch:0, batch23, load frames use: 5.32407093048s
step 22/400000: train loss: 0.634512
epoch:0, batch24, load frames use: 4.27292180061s
step 23/400000: train loss: 0.580765
epoch:0, batch25, load frames use: 5.30603480339s
step 24/400000: train loss: 0.563225
epoch:0, batch26, load frames use: 4.4728500843s
step 25/400000: train loss: 0.650625
epoch:0, batch27, load frames use: 4.86414694786s
step 26/400000: train loss: 0.606831
epoch:0, batch28, load frames use: 4.52588701248s
step 27/400000: train loss: 0.659839
epoch:0, batch29, load frames use: 5.04400610924s
step 28/400000: train loss: 0.656740
epoch:0, batch30, load frames use: 4.3959710598s
step 29/400000: train loss: 0.678932
epoch:0, batch31, load frames use: 5.71752786636s
step 30/400000: train loss: 0.687916
epoch:0, batch32, load frames use: 4.12913894653s
step 31/400000: train loss: 0.639822
epoch:0, batch33, load frames use: 4.60004210472s
step 32/400000: train loss: 0.696854
epoch:0, batch34, load frames use: 4.58682990074s
step 33/400000: train loss: 0.730207
epoch:0, batch35, load frames use: 4.47569584846s
step 34/400000: train loss: 0.740529
epoch:0, batch36, load frames use: 4.2332098484s
step 35/400000: train loss: 0.703750
epoch:0, batch37, load frames use: 5.5777900219s
step 36/400000: train loss: 0.716150
epoch:0, batch38, load frames use: 4.31166100502s
step 37/400000: train loss: 0.671205
epoch:0, batch39, load frames use: 4.72518205643s
step 38/400000: train loss: 0.576055
epoch:0, batch40, load frames use: 4.14969301224s
step 39/400000: train loss: 0.672491
epoch:0, batch41, load frames use: 5.6110727787s
step 40/400000: train loss: 0.730863
epoch:0, batch42, load frames use: 4.084004879s
step 41/400000: train loss: 0.627774
epoch:0, batch43, load frames use: 5.0725440979s
step 42/400000: train loss: 0.662768
epoch:0, batch44, load frames use: 4.42059707642s
step 43/400000: train loss: 0.513815
epoch:0, batch45, load frames use: 4.6114590168s
step 44/400000: train loss: 0.610807
epoch:0, batch46, load frames use: 4.31648111343s
step 45/400000: train loss: 0.747500
epoch:1, batch1, load frames use: 4.94308710098s
step 46/400000: train loss: 0.625149
epoch:1, batch2, load frames use: 4.13565301895s
step 47/400000: train loss: 0.557358
epoch:1, batch3, load frames use: 5.83352398872s
step 48/400000: train loss: 0.533460
epoch:1, batch4, load frames use: 4.02582097054s
step 49/400000: train loss: 0.687140
epoch:1, batch5, load frames use: 5.15837407112s
step 50/400000: train loss: 0.604026
epoch:1, batch6, load frames use: 4.15780711174s
step 51/400000: train loss: 0.668580
epoch:1, batch7, load frames use: 5.16825389862s
step 52/400000: train loss: 0.613687
epoch:1, batch8, load frames use: 3.96555280685s
step 53/400000: train loss: 0.649218
epoch:1, batch9, load frames use: 5.2292509079s
step 54/400000: train loss: 0.608754
epoch:1, batch10, load frames use: 4.26608610153s
step 55/400000: train loss: 0.641963
epoch:1, batch11, load frames use: 5.38376402855s
step 56/400000: train loss: 0.657078
epoch:1, batch12, load frames use: 5.03721904755s
step 57/400000: train loss: 0.584367
epoch:1, batch13, load frames use: 4.28790092468s
step 58/400000: train loss: 0.626571
epoch:1, batch14, load frames use: 4.49030709267s
step 59/400000: train loss: 0.563361
epoch:1, batch15, load frames use: 4.3957669735s
step 60/400000: train loss: 0.741150
epoch:1, batch16, load frames use: 4.05920696259s
step 61/400000: train loss: 0.771664
epoch:1, batch17, load frames use: 4.42236423492s
step 62/400000: train loss: 0.707959
epoch:1, batch18, load frames use: 4.2607228756s
step 63/400000: train loss: 0.622248
epoch:1, batch19, load frames use: 5.50490307808s
step 64/400000: train loss: 0.494683
epoch:1, batch20, load frames use: 3.90957522392s
step 65/400000: train loss: 0.707061
epoch:1, batch21, load frames use: 5.20845389366s
step 66/400000: train loss: 0.698528
epoch:1, batch22, load frames use: 4.03873896599s
step 67/400000: train loss: 0.589718
epoch:1, batch23, load frames use: 4.84793496132s
step 68/400000: train loss: 0.673387
epoch:1, batch24, load frames use: 3.91380691528s
step 69/400000: train loss: 0.606984
epoch:1, batch25, load frames use: 4.00571799278s
step 70/400000: train loss: 0.736303
epoch:1, batch26, load frames use: 4.07615113258s
step 71/400000: train loss: 0.527081
epoch:1, batch27, load frames use: 4.21251606941s
step 72/400000: train loss: 0.701725
epoch:1, batch28, load frames use: 4.29590296745s
step 73/400000: train loss: 0.719137
epoch:1, batch29, load frames use: 4.17341184616s
step 74/400000: train loss: 0.616586
epoch:1, batch30, load frames use: 5.38350105286s
step 75/400000: train loss: 0.724027
epoch:1, batch31, load frames use: 4.33741307259s
step 76/400000: train loss: 0.715186
epoch:1, batch32, load frames use: 4.29776287079s
step 77/400000: train loss: 0.710237
epoch:1, batch33, load frames use: 5.10495495796s
step 78/400000: train loss: 0.664222
epoch:1, batch34, load frames use: 4.4021730423s
step 79/400000: train loss: 0.638860
epoch:1, batch35, load frames use: 4.42926001549s
step 80/400000: train loss: 0.702578
epoch:1, batch36, load frames use: 5.60569286346s
step 81/400000: train loss: 0.566722
epoch:1, batch37, load frames use: 4.05882787704s
step 82/400000: train loss: 0.738506
epoch:1, batch38, load frames use: 4.29010915756s
step 83/400000: train loss: 0.645366
epoch:1, batch39, load frames use: 5.46964097023s
step 84/400000: train loss: 0.769285
epoch:1, batch40, load frames use: 4.17782688141s
step 85/400000: train loss: 0.607299
epoch:1, batch41, load frames use: 4.14615416527s
step 86/400000: train loss: 0.682229
epoch:1, batch42, load frames use: 4.61509299278s
step 87/400000: train loss: 0.659512
epoch:1, batch43, load frames use: 4.41497397423s
step 88/400000: train loss: 0.581007
epoch:1, batch44, load frames use: 4.19964003563s
step 89/400000: train loss: 0.659500
epoch:1, batch45, load frames use: 3.92497110367s
step 90/400000: train loss: 0.616338
epoch:1, batch46, load frames use: 3.77095508575s
step 91/400000: train loss: 0.676909
epoch:2, batch1, load frames use: 4.08846712112s
step 92/400000: train loss: 0.623344
epoch:2, batch2, load frames use: 4.02107596397s
step 93/400000: train loss: 0.701369
epoch:2, batch3, load frames use: 5.02685880661s
step 94/400000: train loss: 0.769338
epoch:2, batch4, load frames use: 4.3913090229s
step 95/400000: train loss: 0.640472
epoch:2, batch5, load frames use: 4.93160915375s
step 96/400000: train loss: 0.658437
epoch:2, batch6, load frames use: 4.44165802002s
step 97/400000: train loss: 0.797723
epoch:2, batch7, load frames use: 5.52235507965s
step 98/400000: train loss: 0.724761
epoch:2, batch8, load frames use: 4.25955486298s
step 99/400000: train loss: 0.752748
epoch:2, batch9, load frames use: 4.94901490211s
step 100/400000: train loss: 0.651479
step 100/400000: validation loss: 0.658571
epoch:2, batch10, load frames use: 4.34430289268s
step 101/400000: train loss: 0.687683
epoch:2, batch11, load frames use: 4.87309789658s
step 102/400000: train loss: 0.602000
epoch:2, batch12, load frames use: 4.58542084694s
step 103/400000: train loss: 0.625433
epoch:2, batch13, load frames use: 4.26259613037s
step 104/400000: train loss: 0.601890
epoch:2, batch14, load frames use: 5.45843100548s
step 105/400000: train loss: 0.625862
epoch:2, batch15, load frames use: 4.2105178833s
step 106/400000: train loss: 0.684852
epoch:2, batch16, load frames use: 5.0572450161s
step 107/400000: train loss: 0.673435
epoch:2, batch17, load frames use: 4.28641915321s
step 108/400000: train loss: 0.620042
epoch:2, batch18, load frames use: 4.76497387886s
step 109/400000: train loss: 0.658331
epoch:2, batch19, load frames use: 4.18411207199s
step 110/400000: train loss: 0.683290
epoch:2, batch20, load frames use: 5.20505714417s
step 111/400000: train loss: 0.603611
epoch:2, batch21, load frames use: 4.26009392738s
step 112/400000: train loss: 0.621490
epoch:2, batch22, load frames use: 5.17540597916s
step 113/400000: train loss: 0.608556
epoch:2, batch23, load frames use: 4.24945282936s
step 114/400000: train loss: 0.737851
epoch:2, batch24, load frames use: 5.49881911278s
step 115/400000: train loss: 0.573917
epoch:2, batch25, load frames use: 4.03944802284s
step 116/400000: train loss: 0.663447
epoch:2, batch26, load frames use: 5.49379110336s
step 117/400000: train loss: 0.601395
epoch:2, batch27, load frames use: 4.30455303192s
step 118/400000: train loss: 0.650634
epoch:2, batch28, load frames use: 3.98885703087s
step 119/400000: train loss: 0.572789
epoch:2, batch29, load frames use: 5.82021999359s
step 120/400000: train loss: 0.612759
epoch:2, batch30, load frames use: 4.05455589294s
step 121/400000: train loss: 0.671959
epoch:2, batch31, load frames use: 4.2711391449s
step 122/400000: train loss: 0.693737
epoch:2, batch32, load frames use: 5.63608908653s
step 123/400000: train loss: 0.567551
epoch:2, batch33, load frames use: 4.30389690399s
step 124/400000: train loss: 0.598900
epoch:2, batch34, load frames use: 4.2142701149s
step 125/400000: train loss: 0.579783
epoch:2, batch35, load frames use: 5.55610394478s
step 126/400000: train loss: 0.596428
epoch:2, batch36, load frames use: 4.0357811451s
step 127/400000: train loss: 0.706046
epoch:2, batch37, load frames use: 4.77975392342s
step 128/400000: train loss: 0.571468
epoch:2, batch38, load frames use: 4.7041258812s
step 129/400000: train loss: 0.514617
epoch:2, batch39, load frames use: 4.78876709938s
step 130/400000: train loss: 0.570222
epoch:2, batch40, load frames use: 4.28435301781s
step 131/400000: train loss: 0.692653
epoch:2, batch41, load frames use: 5.22463297844s
step 132/400000: train loss: 0.575773
epoch:2, batch42, load frames use: 4.22196793556s
step 133/400000: train loss: 0.583823
epoch:2, batch43, load frames use: 5.09516692162s
step 134/400000: train loss: 0.653538
epoch:2, batch44, load frames use: 4.24835681915s
step 135/400000: train loss: 0.649177
epoch:2, batch45, load frames use: 4.54096293449s
step 136/400000: train loss: 0.651947
epoch:2, batch46, load frames use: 4.17844200134s
step 137/400000: train loss: 0.633258
epoch:3, batch1, load frames use: 5.52395892143s
step 138/400000: train loss: 0.668978
epoch:3, batch2, load frames use: 4.34058618546s
step 139/400000: train loss: 0.697770
epoch:3, batch3, load frames use: 5.44511198997s
step 140/400000: train loss: 0.502059
epoch:3, batch4, load frames use: 4.1104619503s
step 141/400000: train loss: 0.636019
epoch:3, batch5, load frames use: 5.40409684181s
step 142/400000: train loss: 0.742906
epoch:3, batch6, load frames use: 4.411465168s
step 143/400000: train loss: 0.773607
epoch:3, batch7, load frames use: 5.34510111809s
step 144/400000: train loss: 0.616131
epoch:3, batch8, load frames use: 3.98099589348s
step 145/400000: train loss: 0.605125
epoch:3, batch9, load frames use: 5.33454489708s
step 146/400000: train loss: 0.651023
epoch:3, batch10, load frames use: 4.10441017151s
step 147/400000: train loss: 0.560017
epoch:3, batch11, load frames use: 4.73305583s
step 148/400000: train loss: 0.648558
epoch:3, batch12, load frames use: 4.2877240181s
step 149/400000: train loss: 0.608147
epoch:3, batch13, load frames use: 5.30100512505s
step 150/400000: train loss: 0.592586
epoch:3, batch14, load frames use: 4.18881797791s
step 151/400000: train loss: 0.636848
epoch:3, batch15, load frames use: 5.27017807961s
step 152/400000: train loss: 0.565906
epoch:3, batch16, load frames use: 4.16393709183s
step 153/400000: train loss: 0.667081
epoch:3, batch17, load frames use: 4.89986205101s
step 154/400000: train loss: 0.683046
epoch:3, batch18, load frames use: 4.28309416771s
step 155/400000: train loss: 0.588816
epoch:3, batch19, load frames use: 5.43726801872s
step 156/400000: train loss: 0.750478
epoch:3, batch20, load frames use: 4.27905201912s
step 157/400000: train loss: 0.632146
epoch:3, batch21, load frames use: 5.28700399399s
step 158/400000: train loss: 0.679775
epoch:3, batch22, load frames use: 4.56054401398s
step 159/400000: train loss: 0.677073
epoch:3, batch23, load frames use: 4.70030117035s
step 160/400000: train loss: 0.633732
epoch:3, batch24, load frames use: 4.39274096489s
step 161/400000: train loss: 0.605306
epoch:3, batch25, load frames use: 5.12518215179s
step 162/400000: train loss: 0.645467
epoch:3, batch26, load frames use: 4.23294401169s
step 163/400000: train loss: 0.650875
epoch:3, batch27, load frames use: 4.80069804192s
step 164/400000: train loss: 0.557499
epoch:3, batch28, load frames use: 4.04791998863s
step 165/400000: train loss: 0.694285
epoch:3, batch29, load frames use: 5.30590820312s
step 166/400000: train loss: 0.557531
epoch:3, batch30, load frames use: 4.11265707016s
step 167/400000: train loss: 0.683413
epoch:3, batch31, load frames use: 5.44156599045s
step 168/400000: train loss: 0.724607
epoch:3, batch32, load frames use: 4.22434616089s
step 169/400000: train loss: 0.600230
epoch:3, batch33, load frames use: 5.21265482903s
step 170/400000: train loss: 0.717136
epoch:3, batch34, load frames use: 4.14160203934s
step 171/400000: train loss: 0.538993
epoch:3, batch35, load frames use: 5.1375169754s
step 172/400000: train loss: 0.625751
epoch:3, batch36, load frames use: 4.14359402657s
step 173/400000: train loss: 0.606823
epoch:3, batch37, load frames use: 4.97877311707s
step 174/400000: train loss: 0.511634
epoch:3, batch38, load frames use: 4.22772502899s
step 175/400000: train loss: 0.638160
epoch:3, batch39, load frames use: 5.20824313164s
step 176/400000: train loss: 0.684444
epoch:3, batch40, load frames use: 4.68245697021s
step 177/400000: train loss: 0.631318
epoch:3, batch41, load frames use: 4.91037487984s
step 178/400000: train loss: 0.573997
epoch:3, batch42, load frames use: 4.12897205353s
step 179/400000: train loss: 0.686968
epoch:3, batch43, load frames use: 4.76618885994s
step 180/400000: train loss: 0.674821
epoch:3, batch44, load frames use: 4.05464100838s
step 181/400000: train loss: 0.755277
epoch:3, batch45, load frames use: 5.35307216644s
step 182/400000: train loss: 0.568672
epoch:3, batch46, load frames use: 4.81925582886s
step 183/400000: train loss: 0.609008
epoch:4, batch1, load frames use: 4.36950492859s
step 184/400000: train loss: 0.670681
epoch:4, batch2, load frames use: 4.87908411026s
step 185/400000: train loss: 0.672617
epoch:4, batch3, load frames use: 4.11121416092s
step 186/400000: train loss: 0.615656
epoch:4, batch4, load frames use: 4.14736104012s
step 187/400000: train loss: 0.614114
epoch:4, batch5, load frames use: 4.11232805252s
step 188/400000: train loss: 0.612024
epoch:4, batch6, load frames use: 3.56513214111s
step 189/400000: train loss: 0.684104
epoch:4, batch7, load frames use: 4.62732982635s
step 190/400000: train loss: 0.646287
epoch:4, batch8, load frames use: 3.94164705276s
step 191/400000: train loss: 0.636118
epoch:4, batch9, load frames use: 5.10045099258s
step 192/400000: train loss: 0.676808
epoch:4, batch10, load frames use: 4.31487703323s
step 193/400000: train loss: 0.563778
epoch:4, batch11, load frames use: 4.90434384346s
step 194/400000: train loss: 0.747601
epoch:4, batch12, load frames use: 4.38967394829s
step 195/400000: train loss: 0.637526
epoch:4, batch13, load frames use: 5.15328097343s
step 196/400000: train loss: 0.644660
epoch:4, batch14, load frames use: 4.06141400337s
step 197/400000: train loss: 0.669313
epoch:4, batch15, load frames use: 5.02258205414s
step 198/400000: train loss: 0.608772
epoch:4, batch16, load frames use: 4.27098894119s
step 199/400000: train loss: 0.666539
epoch:4, batch17, load frames use: 5.29609394073s
step 200/400000: train loss: 0.701982
step 200/400000: validation loss: 0.679418
epoch:4, batch18, load frames use: 4.20476198196s
step 201/400000: train loss: 0.615259
epoch:4, batch19, load frames use: 5.19845104218s
step 202/400000: train loss: 0.630903
epoch:4, batch20, load frames use: 4.13357210159s
step 203/400000: train loss: 0.648698
epoch:4, batch21, load frames use: 5.33340287209s
step 204/400000: train loss: 0.684854
epoch:4, batch22, load frames use: 4.44034218788s
step 205/400000: train loss: 0.606205
epoch:4, batch23, load frames use: 4.3978741169s
step 206/400000: train loss: 0.568709
epoch:4, batch24, load frames use: 4.38873720169s
step 207/400000: train loss: 0.591726
epoch:4, batch25, load frames use: 4.33114290237s
step 208/400000: train loss: 0.685651
epoch:4, batch26, load frames use: 3.99040389061s
step 209/400000: train loss: 0.725553
epoch:4, batch27, load frames use: 4.69013404846s
step 210/400000: train loss: 0.719561
epoch:4, batch28, load frames use: 3.62182402611s
step 211/400000: train loss: 0.641194
epoch:4, batch29, load frames use: 5.47392678261s
step 212/400000: train loss: 0.482433
epoch:4, batch30, load frames use: 3.73681688309s
step 213/400000: train loss: 0.557838
epoch:4, batch31, load frames use: 4.85633897781s
step 214/400000: train loss: 0.511337
epoch:4, batch32, load frames use: 4.00134515762s
step 215/400000: train loss: 0.648189
epoch:4, batch33, load frames use: 4.4127471447s
step 216/400000: train loss: 0.620078
epoch:4, batch34, load frames use: 4.25635814667s
step 217/400000: train loss: 0.604485
epoch:4, batch35, load frames use: 4.2323141098s
step 218/400000: train loss: 0.773641
epoch:4, batch36, load frames use: 4.11478590965s
step 219/400000: train loss: 0.646935
epoch:4, batch37, load frames use: 4.02300906181s
step 220/400000: train loss: 0.752798
epoch:4, batch38, load frames use: 4.33647990227s
step 221/400000: train loss: 0.718372
epoch:4, batch39, load frames use: 4.04436802864s
step 222/400000: train loss: 0.711210
epoch:4, batch40, load frames use: 5.33063197136s
step 223/400000: train loss: 0.681293
epoch:4, batch41, load frames use: 4.23196005821s
step 224/400000: train loss: 0.523037
epoch:4, batch42, load frames use: 5.24901509285s
step 225/400000: train loss: 0.657615
epoch:4, batch43, load frames use: 5.13909506798s
step 226/400000: train loss: 0.642787
epoch:4, batch44, load frames use: 4.05639410019s
step 227/400000: train loss: 0.614454
epoch:4, batch45, load frames use: 5.3160200119s
step 228/400000: train loss: 0.680173
epoch:4, batch46, load frames use: 5.08668708801s
step 229/400000: train loss: 0.574024
epoch:5, batch1, load frames use: 4.13231420517s
step 230/400000: train loss: 0.754690
epoch:5, batch2, load frames use: 4.20259213448s
step 231/400000: train loss: 0.708674
epoch:5, batch3, load frames use: 4.36817002296s
step 232/400000: train loss: 0.687062
epoch:5, batch4, load frames use: 3.9200489521s
step 233/400000: train loss: 0.652771
epoch:5, batch5, load frames use: 4.31264996529s
step 234/400000: train loss: 0.676322
epoch:5, batch6, load frames use: 4.15419793129s
step 235/400000: train loss: 0.742358
epoch:5, batch7, load frames use: 4.57397603989s
step 236/400000: train loss: 0.633643
epoch:5, batch8, load frames use: 3.81271409988s
step 237/400000: train loss: 0.750274
epoch:5, batch9, load frames use: 4.27692890167s
step 238/400000: train loss: 0.659799
epoch:5, batch10, load frames use: 4.27169299126s
step 239/400000: train loss: 0.606055
epoch:5, batch11, load frames use: 4.28423094749s
step 240/400000: train loss: 0.636734
epoch:5, batch12, load frames use: 4.19795584679s
step 241/400000: train loss: 0.627672
epoch:5, batch13, load frames use: 3.81185412407s
step 242/400000: train loss: 0.636373
epoch:5, batch14, load frames use: 4.77629804611s
step 243/400000: train loss: 0.601781
epoch:5, batch15, load frames use: 3.9720761776s
step 244/400000: train loss: 0.629606
epoch:5, batch16, load frames use: 3.99780797958s
step 245/400000: train loss: 0.581448
epoch:5, batch17, load frames use: 4.45702314377s
step 246/400000: train loss: 0.584262
epoch:5, batch18, load frames use: 4.41768193245s
step 247/400000: train loss: 0.511514
epoch:5, batch19, load frames use: 4.67523002625s
step 248/400000: train loss: 0.639509
epoch:5, batch20, load frames use: 5.13896512985s
step 249/400000: train loss: 0.575886
epoch:5, batch21, load frames use: 4.79649496078s
step 250/400000: train loss: 0.553179
epoch:5, batch22, load frames use: 5.19564700127s
step 251/400000: train loss: 0.658820
epoch:5, batch23, load frames use: 4.5378639698s
step 252/400000: train loss: 0.660590
epoch:5, batch24, load frames use: 4.73462295532s
step 253/400000: train loss: 0.634582
epoch:5, batch25, load frames use: 5.00335502625s
step 254/400000: train loss: 0.572692
epoch:5, batch26, load frames use: 4.51007604599s
step 255/400000: train loss: 0.614549
epoch:5, batch27, load frames use: 4.73033595085s
step 256/400000: train loss: 0.668833
epoch:5, batch28, load frames use: 4.72515392303s
step 257/400000: train loss: 0.562506
epoch:5, batch29, load frames use: 5.0640130043s
step 258/400000: train loss: 0.662531
epoch:5, batch30, load frames use: 5.62753295898s
step 259/400000: train loss: 0.608815
epoch:5, batch31, load frames use: 3.90104794502s
step 260/400000: train loss: 0.689666
epoch:5, batch32, load frames use: 4.84886813164s
step 261/400000: train loss: 0.613982
epoch:5, batch33, load frames use: 4.65010309219s
step 262/400000: train loss: 0.589099
epoch:5, batch34, load frames use: 4.80691599846s
step 263/400000: train loss: 0.576907
epoch:5, batch35, load frames use: 4.47563314438s
step 264/400000: train loss: 0.677487
epoch:5, batch36, load frames use: 5.1344730854s
step 265/400000: train loss: 0.668485
epoch:5, batch37, load frames use: 4.33346986771s
step 266/400000: train loss: 0.694598
epoch:5, batch38, load frames use: 4.2722120285s
step 267/400000: train loss: 0.643472
epoch:5, batch39, load frames use: 4.64884114265s
step 268/400000: train loss: 0.635319
epoch:5, batch40, load frames use: 7.01052498817s
step 269/400000: train loss: 0.567323
epoch:5, batch41, load frames use: 6.56325507164s
step 270/400000: train loss: 0.663505
epoch:5, batch42, load frames use: 5.56849789619s
step 271/400000: train loss: 0.603566
epoch:5, batch43, load frames use: 8.67326998711s
step 272/400000: train loss: 0.616527
epoch:5, batch44, load frames use: 4.74185490608s
step 273/400000: train loss: 0.645798
epoch:5, batch45, load frames use: 4.92284607887s
step 274/400000: train loss: 0.657555
epoch:5, batch46, load frames use: 5.70043301582s
step 275/400000: train loss: 0.701669
epoch:6, batch1, load frames use: 4.25512719154s
step 276/400000: train loss: 0.651434
epoch:6, batch2, load frames use: 4.51732683182s
step 277/400000: train loss: 0.528209
epoch:6, batch3, load frames use: 3.91546106339s
step 278/400000: train loss: 0.612903
epoch:6, batch4, load frames use: 3.93049001694s
step 279/400000: train loss: 0.652758
epoch:6, batch5, load frames use: 4.36034297943s
step 280/400000: train loss: 0.621543
epoch:6, batch6, load frames use: 4.2218978405s
step 281/400000: train loss: 0.644683
epoch:6, batch7, load frames use: 3.80182909966s
step 282/400000: train loss: 0.599019
epoch:6, batch8, load frames use: 3.64256882668s
step 283/400000: train loss: 0.575914
epoch:6, batch9, load frames use: 4.70298600197s
step 284/400000: train loss: 0.553685
epoch:6, batch10, load frames use: 3.97505283356s
step 285/400000: train loss: 0.624902
epoch:6, batch11, load frames use: 4.40217709541s
step 286/400000: train loss: 0.591784
epoch:6, batch12, load frames use: 3.60365581512s
step 287/400000: train loss: 0.727030
epoch:6, batch13, load frames use: 4.01435208321s
step 288/400000: train loss: 0.557689
epoch:6, batch14, load frames use: 4.18336606026s
step 289/400000: train loss: 0.617643
epoch:6, batch15, load frames use: 4.44728708267s
step 290/400000: train loss: 0.618084
epoch:6, batch16, load frames use: 4.37553286552s
step 291/400000: train loss: 0.684307
epoch:6, batch17, load frames use: 3.89324498177s
step 292/400000: train loss: 0.577508
epoch:6, batch18, load frames use: 4.83248090744s
step 293/400000: train loss: 0.657277
epoch:6, batch19, load frames use: 3.99197101593s
step 294/400000: train loss: 0.521190
epoch:6, batch20, load frames use: 3.76171088219s
step 295/400000: train loss: 0.597748
epoch:6, batch21, load frames use: 3.59394598007s
step 296/400000: train loss: 0.648953
epoch:6, batch22, load frames use: 4.12292003632s
step 297/400000: train loss: 0.491948
epoch:6, batch23, load frames use: 4.7867128849s
step 298/400000: train loss: 0.535342
epoch:6, batch24, load frames use: 4.97588515282s
step 299/400000: train loss: 0.680893
epoch:6, batch25, load frames use: 4.10660195351s
step 300/400000: train loss: 0.700743
step 300/400000: validation loss: 0.672072
epoch:6, batch26, load frames use: 5.19409608841s
step 301/400000: train loss: 0.665846
epoch:6, batch27, load frames use: 4.91929793358s
step 302/400000: train loss: 0.628512
epoch:6, batch28, load frames use: 4.11994695663s
step 303/400000: train loss: 0.762942
epoch:6, batch29, load frames use: 4.57569384575s
step 304/400000: train loss: 0.633113
epoch:6, batch30, load frames use: 4.16122603416s
step 305/400000: train loss: 0.666727
epoch:6, batch31, load frames use: 3.93410396576s
step 306/400000: train loss: 0.642598
epoch:6, batch32, load frames use: 3.55483603477s
step 307/400000: train loss: 0.616914
epoch:6, batch33, load frames use: 3.88311100006s
step 308/400000: train loss: 0.705974
epoch:6, batch34, load frames use: 3.85841298103s
step 309/400000: train loss: 0.604929
epoch:6, batch35, load frames use: 4.85890102386s
step 310/400000: train loss: 0.589604
epoch:6, batch36, load frames use: 4.33359003067s
step 311/400000: train loss: 0.649432
epoch:6, batch37, load frames use: 4.93090796471s
step 312/400000: train loss: 0.661971
epoch:6, batch38, load frames use: 4.29298114777s
step 313/400000: train loss: 0.595564
epoch:6, batch39, load frames use: 4.75154995918s
step 314/400000: train loss: 0.603005
epoch:6, batch40, load frames use: 3.9495549202s
step 315/400000: train loss: 0.738515
epoch:6, batch41, load frames use: 3.81405091286s
step 316/400000: train loss: 0.699435
epoch:6, batch42, load frames use: 3.87936496735s
step 317/400000: train loss: 0.768502
epoch:6, batch43, load frames use: 3.88646197319s
step 318/400000: train loss: 0.570999
epoch:6, batch44, load frames use: 4.35486793518s
step 319/400000: train loss: 0.619897
epoch:6, batch45, load frames use: 4.183355093s
step 320/400000: train loss: 0.466689
epoch:6, batch46, load frames use: 4.50519394875s
step 321/400000: train loss: 0.660900
epoch:7, batch1, load frames use: 4.58591890335s
step 322/400000: train loss: 0.549387
epoch:7, batch2, load frames use: 3.82124304771s
step 323/400000: train loss: 0.629310
epoch:7, batch3, load frames use: 3.57610821724s
step 324/400000: train loss: 0.595557
epoch:7, batch4, load frames use: 3.70064997673s
step 325/400000: train loss: 0.674988
epoch:7, batch5, load frames use: 3.95278501511s
step 326/400000: train loss: 0.642230
epoch:7, batch6, load frames use: 4.35415220261s
step 327/400000: train loss: 0.670361
epoch:7, batch7, load frames use: 5.05587005615s
step 328/400000: train loss: 0.581399
epoch:7, batch8, load frames use: 4.00314116478s
step 329/400000: train loss: 0.578037
epoch:7, batch9, load frames use: 4.21682906151s
step 330/400000: train loss: 0.730342
epoch:7, batch10, load frames use: 3.86331009865s
step 331/400000: train loss: 0.663922
epoch:7, batch11, load frames use: 4.04151105881s
step 332/400000: train loss: 0.649467
epoch:7, batch12, load frames use: 4.1368188858s
step 333/400000: train loss: 0.651129
epoch:7, batch13, load frames use: 4.68016314507s
step 334/400000: train loss: 0.640297
epoch:7, batch14, load frames use: 4.91720700264s
step 335/400000: train loss: 0.579919
epoch:7, batch15, load frames use: 4.99606394768s
step 336/400000: train loss: 0.618392
epoch:7, batch16, load frames use: 4.88708996773s
step 337/400000: train loss: 0.590113
epoch:7, batch17, load frames use: 5.21987199783s
step 338/400000: train loss: 0.577747
epoch:7, batch18, load frames use: 3.80472421646s
step 339/400000: train loss: 0.642348
epoch:7, batch19, load frames use: 4.57648897171s
step 340/400000: train loss: 0.643369
epoch:7, batch20, load frames use: 5.1059179306s
step 341/400000: train loss: 0.728158
epoch:7, batch21, load frames use: 4.02698421478s
step 342/400000: train loss: 0.593365
epoch:7, batch22, load frames use: 4.79370999336s
step 343/400000: train loss: 0.670999
epoch:7, batch23, load frames use: 4.62350702286s
step 344/400000: train loss: 0.660036
epoch:7, batch24, load frames use: 3.87576794624s
step 345/400000: train loss: 0.624397
epoch:7, batch25, load frames use: 4.05206799507s
step 346/400000: train loss: 0.589875
epoch:7, batch26, load frames use: 4.1650390625s
step 347/400000: train loss: 0.580020
epoch:7, batch27, load frames use: 3.83883905411s
step 348/400000: train loss: 0.562377
epoch:7, batch28, load frames use: 4.08514499664s
step 349/400000: train loss: 0.547763
epoch:7, batch29, load frames use: 4.66907000542s
step 350/400000: train loss: 0.653324
epoch:7, batch30, load frames use: 4.42178201675s
step 351/400000: train loss: 0.598462
epoch:7, batch31, load frames use: 4.81023907661s
step 352/400000: train loss: 0.570229
epoch:7, batch32, load frames use: 4.02969002724s
step 353/400000: train loss: 0.573178
epoch:7, batch33, load frames use: 4.76710414886s
step 354/400000: train loss: 0.701023
epoch:7, batch34, load frames use: 4.08946609497s
step 355/400000: train loss: 0.542740
epoch:7, batch35, load frames use: 3.43196082115s
step 356/400000: train loss: 0.525736
epoch:7, batch36, load frames use: 4.04607796669s
step 357/400000: train loss: 0.601862
epoch:7, batch37, load frames use: 4.11805081367s
step 358/400000: train loss: 0.590785
epoch:7, batch38, load frames use: 4.17874097824s
step 359/400000: train loss: 0.806852
epoch:7, batch39, load frames use: 4.7411031723s
step 360/400000: train loss: 0.564894
epoch:7, batch40, load frames use: 4.14456009865s
step 361/400000: train loss: 0.651234
epoch:7, batch41, load frames use: 5.1930410862s
step 362/400000: train loss: 0.648462
epoch:7, batch42, load frames use: 4.04185318947s
step 363/400000: train loss: 0.529905
epoch:7, batch43, load frames use: 4.46610116959s
step 364/400000: train loss: 0.500632
epoch:7, batch44, load frames use: 3.96085500717s
step 365/400000: train loss: 0.550873
epoch:7, batch45, load frames use: 3.97730207443s
step 366/400000: train loss: 0.669235
epoch:7, batch46, load frames use: 3.55652809143s
step 367/400000: train loss: 0.483388
epoch:8, batch1, load frames use: 4.06245017052s
step 368/400000: train loss: 0.581326
epoch:8, batch2, load frames use: 3.81985902786s
step 369/400000: train loss: 0.615664
epoch:8, batch3, load frames use: 5.07779598236s
step 370/400000: train loss: 0.556661
epoch:8, batch4, load frames use: 3.72813892365s
step 371/400000: train loss: 0.607277
epoch:8, batch5, load frames use: 3.88620686531s
step 372/400000: train loss: 0.746311
epoch:8, batch6, load frames use: 4.16819381714s
step 373/400000: train loss: 0.622900
epoch:8, batch7, load frames use: 4.14924287796s
step 374/400000: train loss: 0.563526
epoch:8, batch8, load frames use: 3.59104204178s
step 375/400000: train loss: 0.553371
epoch:8, batch9, load frames use: 5.55605006218s
step 376/400000: train loss: 0.533093
epoch:8, batch10, load frames use: 3.75654006004s
step 377/400000: train loss: 0.711333
epoch:8, batch11, load frames use: 4.55305194855s
step 378/400000: train loss: 0.598108
epoch:8, batch12, load frames use: 4.27258110046s
step 379/400000: train loss: 0.653991
epoch:8, batch13, load frames use: 4.06224417686s
step 380/400000: train loss: 0.653298
epoch:8, batch14, load frames use: 4.40418195724s
step 381/400000: train loss: 0.614297
epoch:8, batch15, load frames use: 4.07073903084s
step 382/400000: train loss: 0.626358
epoch:8, batch16, load frames use: 3.89219093323s
step 383/400000: train loss: 0.605881
epoch:8, batch17, load frames use: 3.75050783157s
step 384/400000: train loss: 0.629990
epoch:8, batch18, load frames use: 4.11123800278s
step 385/400000: train loss: 0.677386
epoch:8, batch19, load frames use: 4.23378705978s
step 386/400000: train loss: 0.587988
epoch:8, batch20, load frames use: 3.93944597244s
step 387/400000: train loss: 0.550750
epoch:8, batch21, load frames use: 4.38327097893s
step 388/400000: train loss: 0.580255
epoch:8, batch22, load frames use: 3.77736091614s
step 389/400000: train loss: 0.549268
epoch:8, batch23, load frames use: 3.80689787865s
step 390/400000: train loss: 0.578234
epoch:8, batch24, load frames use: 4.01887106895s
step 391/400000: train loss: 0.543309
epoch:8, batch25, load frames use: 4.09470105171s
step 392/400000: train loss: 0.612335
epoch:8, batch26, load frames use: 4.18194317818s
step 393/400000: train loss: 0.647214
epoch:8, batch27, load frames use: 3.8390750885s
step 394/400000: train loss: 0.611852
epoch:8, batch28, load frames use: 4.69604492188s
step 395/400000: train loss: 0.572751
epoch:8, batch29, load frames use: 4.27908706665s
step 396/400000: train loss: 0.615923
epoch:8, batch30, load frames use: 5.09039902687s
step 397/400000: train loss: 0.555289
epoch:8, batch31, load frames use: 3.6673541069s
step 398/400000: train loss: 0.601735
epoch:8, batch32, load frames use: 4.56111001968s
step 399/400000: train loss: 0.537813
epoch:8, batch33, load frames use: 4.23340082169s
step 400/400000: train loss: 0.593207
step 400/400000: validation loss: 0.560941
epoch:8, batch34, load frames use: 4.43516921997s
step 401/400000: train loss: 0.603544
epoch:8, batch35, load frames use: 4.25082802773s
step 402/400000: train loss: 0.647032
epoch:8, batch36, load frames use: 4.28390097618s
step 403/400000: train loss: 0.685404
epoch:8, batch37, load frames use: 4.16532802582s
step 404/400000: train loss: 0.621159
epoch:8, batch38, load frames use: 4.17748808861s
step 405/400000: train loss: 0.554654
epoch:8, batch39, load frames use: 4.11562490463s
step 406/400000: train loss: 0.581907
epoch:8, batch40, load frames use: 3.73446702957s
step 407/400000: train loss: 0.624897
epoch:8, batch41, load frames use: 4.31277298927s
step 408/400000: train loss: 0.609183
epoch:8, batch42, load frames use: 4.18854522705s
step 409/400000: train loss: 0.572492
epoch:8, batch43, load frames use: 4.69485878944s
step 410/400000: train loss: 0.540394
epoch:8, batch44, load frames use: 4.0153028965s
step 411/400000: train loss: 0.457752
epoch:8, batch45, load frames use: 4.25571489334s
step 412/400000: train loss: 0.750389
epoch:8, batch46, load frames use: 3.67145395279s
step 413/400000: train loss: 0.710893
epoch:9, batch1, load frames use: 3.75952506065s
step 414/400000: train loss: 0.436360
epoch:9, batch2, load frames use: 3.86431503296s
step 415/400000: train loss: 0.519375
epoch:9, batch3, load frames use: 3.89940500259s
step 416/400000: train loss: 0.585306
epoch:9, batch4, load frames use: 4.50744986534s
step 417/400000: train loss: 0.580879
epoch:9, batch5, load frames use: 3.77397298813s
step 418/400000: train loss: 0.697980
epoch:9, batch6, load frames use: 4.66598010063s
step 419/400000: train loss: 0.542906
epoch:9, batch7, load frames use: 3.94445991516s
step 420/400000: train loss: 0.646694
epoch:9, batch8, load frames use: 3.89825105667s
step 421/400000: train loss: 0.601509
epoch:9, batch9, load frames use: 4.10775494576s
step 422/400000: train loss: 0.595612
epoch:9, batch10, load frames use: 4.1164059639s
step 423/400000: train loss: 0.581368
epoch:9, batch11, load frames use: 4.24686789513s
step 424/400000: train loss: 0.639806
epoch:9, batch12, load frames use: 3.9240629673s
step 425/400000: train loss: 0.635014
epoch:9, batch13, load frames use: 4.71241998672s
step 426/400000: train loss: 0.471886
epoch:9, batch14, load frames use: 3.78531098366s
step 427/400000: train loss: 0.743008
epoch:9, batch15, load frames use: 4.20215606689s
step 428/400000: train loss: 0.592396
epoch:9, batch16, load frames use: 4.05876088142s
step 429/400000: train loss: 0.628916
epoch:9, batch17, load frames use: 4.10940885544s
step 430/400000: train loss: 0.626580
epoch:9, batch18, load frames use: 4.33295202255s
step 431/400000: train loss: 0.735311
epoch:9, batch19, load frames use: 3.7993490696s
step 432/400000: train loss: 0.528602
epoch:9, batch20, load frames use: 5.48997688293s
step 433/400000: train loss: 0.621832
epoch:9, batch21, load frames use: 3.87833595276s
step 434/400000: train loss: 0.722609
epoch:9, batch22, load frames use: 5.30458784103s
step 435/400000: train loss: 0.563296
epoch:9, batch23, load frames use: 4.10921788216s
step 436/400000: train loss: 0.632424
epoch:9, batch24, load frames use: 4.19874811172s
step 437/400000: train loss: 0.603690
epoch:9, batch25, load frames use: 5.08824896812s
step 438/400000: train loss: 0.664464
epoch:9, batch26, load frames use: 3.95641517639s
step 439/400000: train loss: 0.665664
epoch:9, batch27, load frames use: 3.8388710022s
step 440/400000: train loss: 0.582382
epoch:9, batch28, load frames use: 4.19755601883s
step 441/400000: train loss: 0.515471
epoch:9, batch29, load frames use: 4.11515283585s
step 442/400000: train loss: 0.654224
epoch:9, batch30, load frames use: 4.32164120674s
step 443/400000: train loss: 0.642480
epoch:9, batch31, load frames use: 4.12180495262s
step 444/400000: train loss: 0.701373
epoch:9, batch32, load frames use: 4.34556698799s
step 445/400000: train loss: 0.620092
epoch:9, batch33, load frames use: 4.8154399395s
step 446/400000: train loss: 0.650920
epoch:9, batch34, load frames use: 4.07568502426s
step 447/400000: train loss: 0.696158
epoch:9, batch35, load frames use: 3.98725199699s
step 448/400000: train loss: 0.592645
epoch:9, batch36, load frames use: 3.84601688385s
step 449/400000: train loss: 0.593614
epoch:9, batch37, load frames use: 3.94563603401s
step 450/400000: train loss: 0.628191
epoch:9, batch38, load frames use: 4.38866496086s
step 451/400000: train loss: 0.587646
epoch:9, batch39, load frames use: 4.08903503418s
step 452/400000: train loss: 0.632674
epoch:9, batch40, load frames use: 4.65615701675s
step 453/400000: train loss: 0.509327
epoch:9, batch41, load frames use: 4.97439599037s
step 454/400000: train loss: 0.563788
epoch:9, batch42, load frames use: 4.72489595413s
step 455/400000: train loss: 0.514093
epoch:9, batch43, load frames use: 4.14340519905s
step 456/400000: train loss: 0.703932
epoch:9, batch44, load frames use: 5.03288602829s
step 457/400000: train loss: 0.664657
epoch:9, batch45, load frames use: 5.12180089951s
step 458/400000: train loss: 0.536199
epoch:9, batch46, load frames use: 4.32609796524s
step 459/400000: train loss: 0.591798
epoch:10, batch1, load frames use: 5.17091703415s
step 460/400000: train loss: 0.606186
epoch:10, batch2, load frames use: 4.06687808037s
step 461/400000: train loss: 0.617621
epoch:10, batch3, load frames use: 3.99164104462s
step 462/400000: train loss: 0.651811
epoch:10, batch4, load frames use: 3.8986260891s
step 463/400000: train loss: 0.495733
epoch:10, batch5, load frames use: 3.7292098999s
step 464/400000: train loss: 0.565358
epoch:10, batch6, load frames use: 3.67951488495s
step 465/400000: train loss: 0.586901
epoch:10, batch7, load frames use: 5.37460207939s
step 466/400000: train loss: 0.562031
epoch:10, batch8, load frames use: 3.58175420761s
step 467/400000: train loss: 0.555136
epoch:10, batch9, load frames use: 4.71207404137s
step 468/400000: train loss: 0.504128
epoch:10, batch10, load frames use: 3.95800209045s
step 469/400000: train loss: 0.628841
epoch:10, batch11, load frames use: 3.41266417503s
step 470/400000: train loss: 0.651982
epoch:10, batch12, load frames use: 3.57598686218s
step 471/400000: train loss: 0.638126
epoch:10, batch13, load frames use: 3.95481610298s
step 472/400000: train loss: 0.624998
epoch:10, batch14, load frames use: 4.66223406792s
step 473/400000: train loss: 0.564844
epoch:10, batch15, load frames use: 4.29233813286s
step 474/400000: train loss: 0.575086
epoch:10, batch16, load frames use: 4.95907402039s
step 475/400000: train loss: 0.533416
epoch:10, batch17, load frames use: 5.15319681168s
step 476/400000: train loss: 0.625620
epoch:10, batch18, load frames use: 4.21621894836s
step 477/400000: train loss: 0.587423
epoch:10, batch19, load frames use: 4.96520709991s
step 478/400000: train loss: 0.681866
epoch:10, batch20, load frames use: 4.14146995544s
step 479/400000: train loss: 0.523634
epoch:10, batch21, load frames use: 5.22256803513s
step 480/400000: train loss: 0.548788
epoch:10, batch22, load frames use: 4.36799097061s
step 481/400000: train loss: 0.674373
epoch:10, batch23, load frames use: 5.37623310089s
step 482/400000: train loss: 0.510047
epoch:10, batch24, load frames use: 4.24973011017s
step 483/400000: train loss: 0.681350
epoch:10, batch25, load frames use: 4.29230499268s
step 484/400000: train loss: 0.559009
epoch:10, batch26, load frames use: 5.33852791786s
step 485/400000: train loss: 0.618346
epoch:10, batch27, load frames use: 4.29081201553s
step 486/400000: train loss: 0.542657
epoch:10, batch28, load frames use: 4.9988360405s
step 487/400000: train loss: 0.497353
epoch:10, batch29, load frames use: 4.13449192047s
step 488/400000: train loss: 0.524225
epoch:10, batch30, load frames use: 4.70996284485s
step 489/400000: train loss: 0.562405
epoch:10, batch31, load frames use: 4.4562921524s
step 490/400000: train loss: 0.537339
epoch:10, batch32, load frames use: 4.28807091713s
step 491/400000: train loss: 0.534059
epoch:10, batch33, load frames use: 3.86119699478s
step 492/400000: train loss: 0.519790
epoch:10, batch34, load frames use: 4.14674019814s
step 493/400000: train loss: 0.784539
epoch:10, batch35, load frames use: 4.10812902451s
step 494/400000: train loss: 0.507738
epoch:10, batch36, load frames use: 3.9850730896s
step 495/400000: train loss: 0.655686
epoch:10, batch37, load frames use: 3.89670205116s
step 496/400000: train loss: 0.558636
epoch:10, batch38, load frames use: 3.94541716576s
step 497/400000: train loss: 0.612489
epoch:10, batch39, load frames use: 4.00740218163s
step 498/400000: train loss: 0.574052
epoch:10, batch40, load frames use: 3.94507408142s
step 499/400000: train loss: 0.518663
epoch:10, batch41, load frames use: 4.30813217163s
step 500/400000: train loss: 0.599034
step 500/400000: validation loss: 0.591675
epoch:10, batch42, load frames use: 4.98603606224s
step 501/400000: train loss: 0.629436
epoch:10, batch43, load frames use: 5.12041091919s
step 502/400000: train loss: 0.607769
epoch:10, batch44, load frames use: 4.79330801964s
step 503/400000: train loss: 0.590061
epoch:10, batch45, load frames use: 4.95599079132s
step 504/400000: train loss: 0.586850
epoch:10, batch46, load frames use: 4.96652317047s
step 505/400000: train loss: 0.496789
epoch:11, batch1, load frames use: 4.68260812759s
step 506/400000: train loss: 0.610661
epoch:11, batch2, load frames use: 4.11411905289s
step 507/400000: train loss: 0.515781
epoch:11, batch3, load frames use: 4.05437803268s
step 508/400000: train loss: 0.533440
epoch:11, batch4, load frames use: 4.10086607933s
step 509/400000: train loss: 0.564177
epoch:11, batch5, load frames use: 3.81831097603s
step 510/400000: train loss: 0.478227
epoch:11, batch6, load frames use: 4.283659935s
step 511/400000: train loss: 0.575300
epoch:11, batch7, load frames use: 4.90295100212s
step 512/400000: train loss: 0.647182
epoch:11, batch8, load frames use: 3.89744496346s
step 513/400000: train loss: 0.665183
epoch:11, batch9, load frames use: 4.02235293388s
step 514/400000: train loss: 0.602703
epoch:11, batch10, load frames use: 3.81585502625s
step 515/400000: train loss: 0.566708
epoch:11, batch11, load frames use: 3.62873291969s
step 516/400000: train loss: 0.452110
epoch:11, batch12, load frames use: 4.71318793297s
step 517/400000: train loss: 0.553052
epoch:11, batch13, load frames use: 4.44249391556s
step 518/400000: train loss: 0.664989
epoch:11, batch14, load frames use: 4.78631401062s
step 519/400000: train loss: 0.626890
epoch:11, batch15, load frames use: 4.04432582855s
step 520/400000: train loss: 0.566693
epoch:11, batch16, load frames use: 4.28687691689s
step 521/400000: train loss: 0.550781
epoch:11, batch17, load frames use: 4.10042500496s
step 522/400000: train loss: 0.637565
epoch:11, batch18, load frames use: 4.08629393578s
step 523/400000: train loss: 0.571723
epoch:11, batch19, load frames use: 3.70379805565s
step 524/400000: train loss: 0.625902
epoch:11, batch20, load frames use: 3.68780589104s
step 525/400000: train loss: 0.545750
epoch:11, batch21, load frames use: 4.60265111923s
step 526/400000: train loss: 0.618308
epoch:11, batch22, load frames use: 4.5301630497s
step 527/400000: train loss: 0.510690
epoch:11, batch23, load frames use: 4.74516797066s
step 528/400000: train loss: 0.509342
epoch:11, batch24, load frames use: 4.02521896362s
step 529/400000: train loss: 0.454766
epoch:11, batch25, load frames use: 4.0195441246s
step 530/400000: train loss: 0.516829
epoch:11, batch26, load frames use: 4.07475209236s
step 531/400000: train loss: 0.444463
epoch:11, batch27, load frames use: 4.97624182701s
step 532/400000: train loss: 0.614619
epoch:11, batch28, load frames use: 3.51518082619s
step 533/400000: train loss: 0.553902
epoch:11, batch29, load frames use: 3.68021082878s
step 534/400000: train loss: 0.619548
epoch:11, batch30, load frames use: 3.82039284706s
step 535/400000: train loss: 0.663117
epoch:11, batch31, load frames use: 3.92270684242s
step 536/400000: train loss: 0.563303
epoch:11, batch32, load frames use: 4.33117818832s
step 537/400000: train loss: 0.657115
epoch:11, batch33, load frames use: 3.76084709167s
step 538/400000: train loss: 0.505170
epoch:11, batch34, load frames use: 3.66336798668s
step 539/400000: train loss: 0.523784
epoch:11, batch35, load frames use: 3.57887887955s
step 540/400000: train loss: 0.570687
epoch:11, batch36, load frames use: 3.9750289917s
step 541/400000: train loss: 0.603192
epoch:11, batch37, load frames use: 4.08128809929s
step 542/400000: train loss: 0.567575
epoch:11, batch38, load frames use: 4.13526701927s
step 543/400000: train loss: 0.668678
epoch:11, batch39, load frames use: 4.75878190994s
step 544/400000: train loss: 0.653037
epoch:11, batch40, load frames use: 4.32182312012s
step 545/400000: train loss: 0.634666
epoch:11, batch41, load frames use: 4.43851304054s
step 546/400000: train loss: 0.554309
epoch:11, batch42, load frames use: 4.04644489288s
step 547/400000: train loss: 0.569447
epoch:11, batch43, load frames use: 3.88605308533s
step 548/400000: train loss: 0.731086
epoch:11, batch44, load frames use: 3.71980500221s
step 549/400000: train loss: 0.544079
epoch:11, batch45, load frames use: 4.70384597778s
step 550/400000: train loss: 0.636354
epoch:11, batch46, load frames use: 4.89011001587s
step 551/400000: train loss: 0.528623
epoch:12, batch1, load frames use: 4.87718892097s
step 552/400000: train loss: 0.534840
epoch:12, batch2, load frames use: 4.1772480011s
step 553/400000: train loss: 0.571641
epoch:12, batch3, load frames use: 4.4913392067s
step 554/400000: train loss: 0.490336
epoch:12, batch4, load frames use: 4.03891801834s
step 555/400000: train loss: 0.535926
epoch:12, batch5, load frames use: 3.91088294983s
step 556/400000: train loss: 0.694901
epoch:12, batch6, load frames use: 3.8503639698s
step 557/400000: train loss: 0.559575
epoch:12, batch7, load frames use: 4.97175788879s
step 558/400000: train loss: 0.572551
epoch:12, batch8, load frames use: 4.61835718155s
step 559/400000: train loss: 0.581411
epoch:12, batch9, load frames use: 3.7294421196s
step 560/400000: train loss: 0.578257
epoch:12, batch10, load frames use: 3.93508815765s
step 561/400000: train loss: 0.647269
epoch:12, batch11, load frames use: 4.41197681427s
step 562/400000: train loss: 0.562414
epoch:12, batch12, load frames use: 4.3605029583s
step 563/400000: train loss: 0.501850
epoch:12, batch13, load frames use: 3.72378110886s
step 564/400000: train loss: 0.595128
epoch:12, batch14, load frames use: 4.77909183502s
step 565/400000: train loss: 0.693994
epoch:12, batch15, load frames use: 4.03637504578s
step 566/400000: train loss: 0.545072
epoch:12, batch16, load frames use: 4.17033290863s
step 567/400000: train loss: 0.595077
epoch:12, batch17, load frames use: 4.61706900597s
step 568/400000: train loss: 0.678295
epoch:12, batch18, load frames use: 3.92105984688s
step 569/400000: train loss: 0.511364
epoch:12, batch19, load frames use: 4.18117785454s
step 570/400000: train loss: 0.615005
epoch:12, batch20, load frames use: 5.18830895424s
step 571/400000: train loss: 0.599692
epoch:12, batch21, load frames use: 4.67080187798s
step 572/400000: train loss: 0.484567
epoch:12, batch22, load frames use: 4.71650886536s
step 573/400000: train loss: 0.513417
epoch:12, batch23, load frames use: 4.79982089996s
step 574/400000: train loss: 0.561997
epoch:12, batch24, load frames use: 4.82919692993s
step 575/400000: train loss: 0.544468
epoch:12, batch25, load frames use: 4.5419318676s
step 576/400000: train loss: 0.591144
epoch:12, batch26, load frames use: 4.45305109024s
step 577/400000: train loss: 0.596821
epoch:12, batch27, load frames use: 4.80155396461s
step 578/400000: train loss: 0.519731
epoch:12, batch28, load frames use: 4.56904983521s
step 579/400000: train loss: 0.536756
epoch:12, batch29, load frames use: 4.68511009216s
step 580/400000: train loss: 0.555859
epoch:12, batch30, load frames use: 4.76234102249s
step 581/400000: train loss: 0.576238
epoch:12, batch31, load frames use: 4.72222685814s
step 582/400000: train loss: 0.505012
epoch:12, batch32, load frames use: 4.59030818939s
step 583/400000: train loss: 0.528695
epoch:12, batch33, load frames use: 4.7673201561s
step 584/400000: train loss: 0.582522
epoch:12, batch34, load frames use: 4.6750061512s
step 585/400000: train loss: 0.506499
epoch:12, batch35, load frames use: 4.74973392487s
step 586/400000: train loss: 0.519539
epoch:12, batch36, load frames use: 3.96493792534s
step 587/400000: train loss: 0.516074
epoch:12, batch37, load frames use: 4.61636781693s
step 588/400000: train loss: 0.484537
epoch:12, batch38, load frames use: 5.03461003304s
step 589/400000: train loss: 0.573360
epoch:12, batch39, load frames use: 4.54639792442s
step 590/400000: train loss: 0.515143
epoch:12, batch40, load frames use: 5.008934021s
step 591/400000: train loss: 0.605513
epoch:12, batch41, load frames use: 4.51428508759s
step 592/400000: train loss: 0.552069
epoch:12, batch42, load frames use: 4.84230589867s
step 593/400000: train loss: 0.584152
epoch:12, batch43, load frames use: 4.84701609612s
step 594/400000: train loss: 0.599687
epoch:12, batch44, load frames use: 4.69585609436s
step 595/400000: train loss: 0.567003
epoch:12, batch45, load frames use: 4.87498116493s
step 596/400000: train loss: 0.563777
epoch:12, batch46, load frames use: 4.65235900879s
step 597/400000: train loss: 0.540511
epoch:13, batch1, load frames use: 4.60761594772s
step 598/400000: train loss: 0.572476
epoch:13, batch2, load frames use: 4.69811105728s
step 599/400000: train loss: 0.501718
epoch:13, batch3, load frames use: 4.59559988976s
step 600/400000: train loss: 0.562503
step 600/400000: validation loss: 0.543523
epoch:13, batch4, load frames use: 4.80038285255s
step 601/400000: train loss: 0.555259
epoch:13, batch5, load frames use: 4.47180104256s
step 602/400000: train loss: 0.484907
epoch:13, batch6, load frames use: 5.22833681107s
step 603/400000: train loss: 0.507452
epoch:13, batch7, load frames use: 5.04471707344s
step 604/400000: train loss: 0.570821
epoch:13, batch8, load frames use: 4.50436615944s
step 605/400000: train loss: 0.612012
epoch:13, batch9, load frames use: 5.03001022339s
step 606/400000: train loss: 0.572012
epoch:13, batch10, load frames use: 5.12082195282s
step 607/400000: train loss: 0.497686
epoch:13, batch11, load frames use: 4.11860799789s
step 608/400000: train loss: 0.532109
epoch:13, batch12, load frames use: 4.83385705948s
step 609/400000: train loss: 0.568490
epoch:13, batch13, load frames use: 5.04451179504s
step 610/400000: train loss: 0.489939
epoch:13, batch14, load frames use: 3.99794006348s
step 611/400000: train loss: 0.475231
epoch:13, batch15, load frames use: 4.68963289261s
step 612/400000: train loss: 0.462551
epoch:13, batch16, load frames use: 4.97474503517s
step 613/400000: train loss: 0.562856
epoch:13, batch17, load frames use: 4.0231628418s
step 614/400000: train loss: 0.555694
epoch:13, batch18, load frames use: 5.1420211792s
step 615/400000: train loss: 0.558188
epoch:13, batch19, load frames use: 4.57038116455s
step 616/400000: train loss: 0.545962
epoch:13, batch20, load frames use: 4.50147080421s
step 617/400000: train loss: 0.701107
epoch:13, batch21, load frames use: 4.29969096184s
step 618/400000: train loss: 0.641069
epoch:13, batch22, load frames use: 3.71635603905s
step 619/400000: train loss: 0.704076
epoch:13, batch23, load frames use: 3.78624010086s
step 620/400000: train loss: 0.434798
epoch:13, batch24, load frames use: 3.65393614769s
step 621/400000: train loss: 0.539755
epoch:13, batch25, load frames use: 4.52552700043s
step 622/400000: train loss: 0.543881
epoch:13, batch26, load frames use: 4.11976695061s
step 623/400000: train loss: 0.634326
epoch:13, batch27, load frames use: 4.60115194321s
step 624/400000: train loss: 0.588695
epoch:13, batch28, load frames use: 4.94755601883s
step 625/400000: train loss: 0.527420
epoch:13, batch29, load frames use: 4.13713216782s
step 626/400000: train loss: 0.657061
epoch:13, batch30, load frames use: 4.39319491386s
step 627/400000: train loss: 0.671377
epoch:13, batch31, load frames use: 3.90436410904s
step 628/400000: train loss: 0.642523
epoch:13, batch32, load frames use: 3.84731197357s
step 629/400000: train loss: 0.620345
epoch:13, batch33, load frames use: 3.76943087578s
step 630/400000: train loss: 0.613038
epoch:13, batch34, load frames use: 3.87150406837s
step 631/400000: train loss: 0.560636
epoch:13, batch35, load frames use: 4.65192198753s
step 632/400000: train loss: 0.517697
epoch:13, batch36, load frames use: 4.02389812469s
step 633/400000: train loss: 0.515745
epoch:13, batch37, load frames use: 4.9107029438s
step 634/400000: train loss: 0.572924
epoch:13, batch38, load frames use: 4.21607613564s
step 635/400000: train loss: 0.623952
epoch:13, batch39, load frames use: 5.73509192467s
step 636/400000: train loss: 0.542606
epoch:13, batch40, load frames use: 4.21840405464s
step 637/400000: train loss: 0.625168
epoch:13, batch41, load frames use: 4.37357401848s
step 638/400000: train loss: 0.460419
epoch:13, batch42, load frames use: 5.11581897736s
step 639/400000: train loss: 0.570952
epoch:13, batch43, load frames use: 4.19969415665s
step 640/400000: train loss: 0.583227
epoch:13, batch44, load frames use: 4.88621306419s
step 641/400000: train loss: 0.549187
epoch:13, batch45, load frames use: 4.21966791153s
step 642/400000: train loss: 0.661636
epoch:13, batch46, load frames use: 5.6853852272s
step 643/400000: train loss: 0.534928
epoch:14, batch1, load frames use: 3.86174893379s
step 644/400000: train loss: 0.607501
epoch:14, batch2, load frames use: 3.96338605881s
step 645/400000: train loss: 0.640040
epoch:14, batch3, load frames use: 4.61304497719s
step 646/400000: train loss: 0.597950
epoch:14, batch4, load frames use: 3.75279903412s
step 647/400000: train loss: 0.497772
epoch:14, batch5, load frames use: 4.34017109871s
step 648/400000: train loss: 0.658005
epoch:14, batch6, load frames use: 3.86009502411s
step 649/400000: train loss: 0.577326
epoch:14, batch7, load frames use: 4.03566098213s
step 650/400000: train loss: 0.547488
epoch:14, batch8, load frames use: 4.55725502968s
step 651/400000: train loss: 0.602304
epoch:14, batch9, load frames use: 3.86120295525s
step 652/400000: train loss: 0.514528
epoch:14, batch10, load frames use: 4.69153499603s
step 653/400000: train loss: 0.609425
epoch:14, batch11, load frames use: 4.00060415268s
step 654/400000: train loss: 0.553947
epoch:14, batch12, load frames use: 4.99652004242s
step 655/400000: train loss: 0.615696
epoch:14, batch13, load frames use: 3.47615098953s
step 656/400000: train loss: 0.585256
epoch:14, batch14, load frames use: 3.71992897987s
step 657/400000: train loss: 0.639777
epoch:14, batch15, load frames use: 3.70906305313s
step 658/400000: train loss: 0.572995
epoch:14, batch16, load frames use: 4.42482209206s
step 659/400000: train loss: 0.492144
epoch:14, batch17, load frames use: 4.77654600143s
step 660/400000: train loss: 0.531265
epoch:14, batch18, load frames use: 3.71426391602s
step 661/400000: train loss: 0.576994
epoch:14, batch19, load frames use: 3.89213991165s
step 662/400000: train loss: 0.638460
epoch:14, batch20, load frames use: 3.67470407486s
step 663/400000: train loss: 0.597355
epoch:14, batch21, load frames use: 3.89730501175s
step 664/400000: train loss: 0.583550
epoch:14, batch22, load frames use: 4.47662687302s
step 665/400000: train loss: 0.426442
epoch:14, batch23, load frames use: 4.29238414764s
step 666/400000: train loss: 0.463277
epoch:14, batch24, load frames use: 5.43615698814s
step 667/400000: train loss: 0.567301
epoch:14, batch25, load frames use: 4.0205719471s
step 668/400000: train loss: 0.578657
epoch:14, batch26, load frames use: 4.25573992729s
step 669/400000: train loss: 0.484713
epoch:14, batch27, load frames use: 3.7314260006s
step 670/400000: train loss: 0.536015
epoch:14, batch28, load frames use: 3.83217406273s
step 671/400000: train loss: 0.591981
epoch:14, batch29, load frames use: 3.84022188187s
step 672/400000: train loss: 0.528852
epoch:14, batch30, load frames use: 4.10752606392s
step 673/400000: train loss: 0.652378
epoch:14, batch31, load frames use: 4.40096092224s
step 674/400000: train loss: 0.575223
epoch:14, batch32, load frames use: 3.84975194931s
step 675/400000: train loss: 0.522488
epoch:14, batch33, load frames use: 4.89085698128s
step 676/400000: train loss: 0.504234
epoch:14, batch34, load frames use: 5.23195195198s
step 677/400000: train loss: 0.525791
epoch:14, batch35, load frames use: 5.08992004395s
step 678/400000: train loss: 0.505994
epoch:14, batch36, load frames use: 4.80683803558s
step 679/400000: train loss: 0.488461
epoch:14, batch37, load frames use: 5.12137198448s
step 680/400000: train loss: 0.625594
epoch:14, batch38, load frames use: 4.39134287834s
step 681/400000: train loss: 0.537169
epoch:14, batch39, load frames use: 4.74343705177s
step 682/400000: train loss: 0.552209
epoch:14, batch40, load frames use: 4.0875480175s
step 683/400000: train loss: 0.625030
epoch:14, batch41, load frames use: 4.91529798508s
step 684/400000: train loss: 0.522855
epoch:14, batch42, load frames use: 4.30841708183s
step 685/400000: train loss: 0.540890
epoch:14, batch43, load frames use: 4.24457383156s
step 686/400000: train loss: 0.582923
epoch:14, batch44, load frames use: 4.05164504051s
step 687/400000: train loss: 0.646199
epoch:14, batch45, load frames use: 3.52543401718s
step 688/400000: train loss: 0.483199
epoch:14, batch46, load frames use: 3.74448895454s
step 689/400000: train loss: 0.397858
epoch:15, batch1, load frames use: 3.86733293533s
step 690/400000: train loss: 0.542424
epoch:15, batch2, load frames use: 4.01998901367s
step 691/400000: train loss: 0.598304
epoch:15, batch3, load frames use: 3.87096905708s
step 692/400000: train loss: 0.603515
epoch:15, batch4, load frames use: 3.67382502556s
step 693/400000: train loss: 0.607755
epoch:15, batch5, load frames use: 4.59017896652s
step 694/400000: train loss: 0.523451
epoch:15, batch6, load frames use: 4.15472602844s
step 695/400000: train loss: 0.487300
epoch:15, batch7, load frames use: 4.87261795998s
step 696/400000: train loss: 0.498804
epoch:15, batch8, load frames use: 5.06293106079s
step 697/400000: train loss: 0.497069
epoch:15, batch9, load frames use: 4.68416810036s
step 698/400000: train loss: 0.500907
epoch:15, batch10, load frames use: 4.19429588318s
step 699/400000: train loss: 0.639163
epoch:15, batch11, load frames use: 5.03282189369s
step 700/400000: train loss: 0.492590
step 700/400000: validation loss: 0.504571
epoch:15, batch12, load frames use: 4.70801377296s
step 701/400000: train loss: 0.542092
epoch:15, batch13, load frames use: 4.2729010582s
step 702/400000: train loss: 0.562564
epoch:15, batch14, load frames use: 4.45362901688s
step 703/400000: train loss: 0.602131
epoch:15, batch15, load frames use: 4.59048390388s
step 704/400000: train loss: 0.476311
epoch:15, batch16, load frames use: 3.84890198708s
step 705/400000: train loss: 0.545871
epoch:15, batch17, load frames use: 3.61694192886s
step 706/400000: train loss: 0.592090
epoch:15, batch18, load frames use: 3.91861009598s
step 707/400000: train loss: 0.583823
epoch:15, batch19, load frames use: 4.1269569397s
step 708/400000: train loss: 0.537497
epoch:15, batch20, load frames use: 3.83164191246s
step 709/400000: train loss: 0.452350
epoch:15, batch21, load frames use: 4.40926814079s
step 710/400000: train loss: 0.509308
epoch:15, batch22, load frames use: 3.94573092461s
step 711/400000: train loss: 0.489878
epoch:15, batch23, load frames use: 3.75647592545s
step 712/400000: train loss: 0.541183
epoch:15, batch24, load frames use: 3.91349887848s
step 713/400000: train loss: 0.465518
epoch:15, batch25, load frames use: 4.0948381424s
step 714/400000: train loss: 0.530964
epoch:15, batch26, load frames use: 4.6262691021s
step 715/400000: train loss: 0.613158
epoch:15, batch27, load frames use: 4.59137201309s
step 716/400000: train loss: 0.555056
epoch:15, batch28, load frames use: 4.19349718094s
step 717/400000: train loss: 0.556815
epoch:15, batch29, load frames use: 3.66990590096s
step 718/400000: train loss: 0.509246
epoch:15, batch30, load frames use: 3.96055102348s
step 719/400000: train loss: 0.583407
epoch:15, batch31, load frames use: 3.70078611374s
step 720/400000: train loss: 0.553382
epoch:15, batch32, load frames use: 5.05587792397s
step 721/400000: train loss: 0.489192
epoch:15, batch33, load frames use: 5.04749298096s
step 722/400000: train loss: 0.537854
epoch:15, batch34, load frames use: 4.04941511154s
step 723/400000: train loss: 0.572973
epoch:15, batch35, load frames use: 4.97299790382s
step 724/400000: train loss: 0.551587
epoch:15, batch36, load frames use: 4.8442838192s
step 725/400000: train loss: 0.477550
epoch:15, batch37, load frames use: 4.10196805s
step 726/400000: train loss: 0.521361
epoch:15, batch38, load frames use: 4.30433797836s
step 727/400000: train loss: 0.490011
epoch:15, batch39, load frames use: 4.06329107285s
step 728/400000: train loss: 0.608567
epoch:15, batch40, load frames use: 3.62193799019s
step 729/400000: train loss: 0.582473
epoch:15, batch41, load frames use: 4.37514305115s
step 730/400000: train loss: 0.633865
epoch:15, batch42, load frames use: 4.82535409927s
step 731/400000: train loss: 0.517485
epoch:15, batch43, load frames use: 5.21785402298s
step 732/400000: train loss: 0.560837
epoch:15, batch44, load frames use: 4.76507210732s
step 733/400000: train loss: 0.511748
epoch:15, batch45, load frames use: 4.21112585068s
step 734/400000: train loss: 0.623734
epoch:15, batch46, load frames use: 4.86414599419s
step 735/400000: train loss: 0.562264
epoch:16, batch1, load frames use: 4.87668585777s
step 736/400000: train loss: 0.573341
epoch:16, batch2, load frames use: 4.8197491169s
step 737/400000: train loss: 0.575443
epoch:16, batch3, load frames use: 4.44980502129s
step 738/400000: train loss: 0.558203
epoch:16, batch4, load frames use: 4.15866494179s
step 739/400000: train loss: 0.513598
epoch:16, batch5, load frames use: 3.93606901169s
step 740/400000: train loss: 0.539672
epoch:16, batch6, load frames use: 3.88843297958s
step 741/400000: train loss: 0.533557
epoch:16, batch7, load frames use: 3.81293082237s
step 742/400000: train loss: 0.539840
epoch:16, batch8, load frames use: 4.6535179615s
step 743/400000: train loss: 0.447954
epoch:16, batch9, load frames use: 4.80892586708s
step 744/400000: train loss: 0.536831
epoch:16, batch10, load frames use: 4.12813615799s
step 745/400000: train loss: 0.589685
epoch:16, batch11, load frames use: 3.75830006599s
step 746/400000: train loss: 0.541803
epoch:16, batch12, load frames use: 3.94791889191s
step 747/400000: train loss: 0.644427
epoch:16, batch13, load frames use: 3.70304608345s
step 748/400000: train loss: 0.535773
epoch:16, batch14, load frames use: 4.55887413025s
step 749/400000: train loss: 0.573856
epoch:16, batch15, load frames use: 4.03707194328s
step 750/400000: train loss: 0.618687
epoch:16, batch16, load frames use: 3.75712108612s
step 751/400000: train loss: 0.591918
epoch:16, batch17, load frames use: 4.04648399353s
step 752/400000: train loss: 0.630981
epoch:16, batch18, load frames use: 3.6211130619s
step 753/400000: train loss: 0.641681
epoch:16, batch19, load frames use: 4.28531193733s
step 754/400000: train loss: 0.511864
epoch:16, batch20, load frames use: 3.93089795113s
step 755/400000: train loss: 0.503046
epoch:16, batch21, load frames use: 3.79273295403s
step 756/400000: train loss: 0.552476
epoch:16, batch22, load frames use: 3.73477482796s
step 757/400000: train loss: 0.612040
epoch:16, batch23, load frames use: 4.87482118607s
step 758/400000: train loss: 0.593786
epoch:16, batch24, load frames use: 4.65154004097s
step 759/400000: train loss: 0.474971
epoch:16, batch25, load frames use: 4.54498696327s
step 760/400000: train loss: 0.635943
epoch:16, batch26, load frames use: 3.66257190704s
step 761/400000: train loss: 0.397672
epoch:16, batch27, load frames use: 3.76183390617s
step 762/400000: train loss: 0.469672
epoch:16, batch28, load frames use: 3.87384605408s
step 763/400000: train loss: 0.490774
epoch:16, batch29, load frames use: 4.42228293419s
step 764/400000: train loss: 0.532049
epoch:16, batch30, load frames use: 4.72211003304s
step 765/400000: train loss: 0.522498
epoch:16, batch31, load frames use: 4.09476995468s
step 766/400000: train loss: 0.597081
epoch:16, batch32, load frames use: 3.61489796638s
step 767/400000: train loss: 0.549599
epoch:16, batch33, load frames use: 3.67603087425s
step 768/400000: train loss: 0.494745
epoch:16, batch34, load frames use: 3.81677389145s
step 769/400000: train loss: 0.518996
epoch:16, batch35, load frames use: 4.70809793472s
step 770/400000: train loss: 0.528564
epoch:16, batch36, load frames use: 4.4150428772s
step 771/400000: train loss: 0.546323
epoch:16, batch37, load frames use: 4.21298694611s
step 772/400000: train loss: 0.540386
epoch:16, batch38, load frames use: 3.74271202087s
step 773/400000: train loss: 0.553642
epoch:16, batch39, load frames use: 3.79459786415s
step 774/400000: train loss: 0.509721
epoch:16, batch40, load frames use: 3.93204188347s
step 775/400000: train loss: 0.522739
epoch:16, batch41, load frames use: 4.00555205345s
step 776/400000: train loss: 0.491788
epoch:16, batch42, load frames use: 4.02699494362s
step 777/400000: train loss: 0.692255
epoch:16, batch43, load frames use: 3.89742803574s
step 778/400000: train loss: 0.473856
epoch:16, batch44, load frames use: 4.59426212311s
step 779/400000: train loss: 0.470303
epoch:16, batch45, load frames use: 4.07186698914s
step 780/400000: train loss: 0.525465
epoch:16, batch46, load frames use: 3.96245121956s
step 781/400000: train loss: 0.599377
epoch:17, batch1, load frames use: 3.955534935s
step 782/400000: train loss: 0.490925
epoch:17, batch2, load frames use: 3.60294914246s
step 783/400000: train loss: 0.518816
epoch:17, batch3, load frames use: 4.39530992508s
step 784/400000: train loss: 0.531640
epoch:17, batch4, load frames use: 3.68727588654s
step 785/400000: train loss: 0.540548
epoch:17, batch5, load frames use: 3.63580083847s
step 786/400000: train loss: 0.504475
epoch:17, batch6, load frames use: 4.06262683868s
step 787/400000: train loss: 0.572033
epoch:17, batch7, load frames use: 4.85995697975s
step 788/400000: train loss: 0.562330
epoch:17, batch8, load frames use: 3.62180900574s
step 789/400000: train loss: 0.591446
epoch:17, batch9, load frames use: 4.10121107101s
step 790/400000: train loss: 0.530469
epoch:17, batch10, load frames use: 3.85009098053s
step 791/400000: train loss: 0.522803
epoch:17, batch11, load frames use: 4.02124500275s
step 792/400000: train loss: 0.499045
epoch:17, batch12, load frames use: 4.77544283867s
step 793/400000: train loss: 0.466656
epoch:17, batch13, load frames use: 4.02265906334s
step 794/400000: train loss: 0.623313
epoch:17, batch14, load frames use: 3.94997477531s
step 795/400000: train loss: 0.588181
epoch:17, batch15, load frames use: 3.92402291298s
step 796/400000: train loss: 0.569213
epoch:17, batch16, load frames use: 3.88854885101s
step 797/400000: train loss: 0.561269
epoch:17, batch17, load frames use: 4.85747599602s
step 798/400000: train loss: 0.557668
epoch:17, batch18, load frames use: 4.35833716393s
step 799/400000: train loss: 0.513076
epoch:17, batch19, load frames use: 5.33735704422s
step 800/400000: train loss: 0.619459
step 800/400000: validation loss: 0.577354
epoch:17, batch20, load frames use: 3.92586302757s
step 801/400000: train loss: 0.609611
epoch:17, batch21, load frames use: 4.34796595573s
step 802/400000: train loss: 0.498710
epoch:17, batch22, load frames use: 3.84970879555s
step 803/400000: train loss: 0.563214
epoch:17, batch23, load frames use: 3.74401307106s
step 804/400000: train loss: 0.499238
epoch:17, batch24, load frames use: 3.7167699337s
step 805/400000: train loss: 0.469688
epoch:17, batch25, load frames use: 4.1492331028s
step 806/400000: train loss: 0.565207
epoch:17, batch26, load frames use: 4.71591091156s
step 807/400000: train loss: 0.541219
epoch:17, batch27, load frames use: 4.88186597824s
step 808/400000: train loss: 0.438474
epoch:17, batch28, load frames use: 4.48035502434s
step 809/400000: train loss: 0.610937
epoch:17, batch29, load frames use: 4.5075340271s
step 810/400000: train loss: 0.489603
epoch:17, batch30, load frames use: 4.70206403732s
step 811/400000: train loss: 0.535117
epoch:17, batch31, load frames use: 4.86294794083s
step 812/400000: train loss: 0.585344
epoch:17, batch32, load frames use: 4.35247206688s
step 813/400000: train loss: 0.540862
epoch:17, batch33, load frames use: 4.68013906479s
step 814/400000: train loss: 0.549104
epoch:17, batch34, load frames use: 4.86705589294s
step 815/400000: train loss: 0.485629
epoch:17, batch35, load frames use: 4.48089003563s
step 816/400000: train loss: 0.509128
epoch:17, batch36, load frames use: 4.91998004913s
step 817/400000: train loss: 0.583101
epoch:17, batch37, load frames use: 4.64041805267s
step 818/400000: train loss: 0.463686
epoch:17, batch38, load frames use: 4.47555994987s
step 819/400000: train loss: 0.442787
epoch:17, batch39, load frames use: 4.80343294144s
step 820/400000: train loss: 0.511491
epoch:17, batch40, load frames use: 4.67035317421s
step 821/400000: train loss: 0.502789
epoch:17, batch41, load frames use: 4.81571698189s
step 822/400000: train loss: 0.564575
epoch:17, batch42, load frames use: 4.50021100044s
step 823/400000: train loss: 0.649822
epoch:17, batch43, load frames use: 4.79604005814s
step 824/400000: train loss: 0.393003
epoch:17, batch44, load frames use: 4.74781489372s
step 825/400000: train loss: 0.447767
epoch:17, batch45, load frames use: 5.03586602211s
step 826/400000: train loss: 0.466938
epoch:17, batch46, load frames use: 4.2230618s
step 827/400000: train loss: 0.547794
epoch:18, batch1, load frames use: 4.07755303383s
step 828/400000: train loss: 0.536838
epoch:18, batch2, load frames use: 3.82858705521s
step 829/400000: train loss: 0.477130
epoch:18, batch3, load frames use: 4.12280201912s
step 830/400000: train loss: 0.440304
epoch:18, batch4, load frames use: 3.72721505165s
step 831/400000: train loss: 0.481734
epoch:18, batch5, load frames use: 4.09161615372s
step 832/400000: train loss: 0.587043
epoch:18, batch6, load frames use: 5.10004711151s
step 833/400000: train loss: 0.390164
epoch:18, batch7, load frames use: 3.68233799934s
step 834/400000: train loss: 0.612865
epoch:18, batch8, load frames use: 3.95953512192s
step 835/400000: train loss: 0.539271
epoch:18, batch9, load frames use: 3.65551304817s
step 836/400000: train loss: 0.474846
epoch:18, batch10, load frames use: 3.86044597626s
step 837/400000: train loss: 0.640256
epoch:18, batch11, load frames use: 4.48453378677s
step 838/400000: train loss: 0.581239
epoch:18, batch12, load frames use: 5.09727597237s
step 839/400000: train loss: 0.626547
epoch:18, batch13, load frames use: 6.64388108253s
step 840/400000: train loss: 0.608706
epoch:18, batch14, load frames use: 5.56117105484s
step 841/400000: train loss: 0.499866
epoch:18, batch15, load frames use: 5.76775979996s
step 842/400000: train loss: 0.476064
epoch:18, batch16, load frames use: 5.08776187897s
step 843/400000: train loss: 0.506164
epoch:18, batch17, load frames use: 5.73980879784s
step 844/400000: train loss: 0.472182
epoch:18, batch18, load frames use: 5.52557897568s
step 845/400000: train loss: 0.513895
epoch:18, batch19, load frames use: 5.99494409561s
step 846/400000: train loss: 0.461288
epoch:18, batch20, load frames use: 6.13959503174s
step 847/400000: train loss: 0.582364
epoch:18, batch21, load frames use: 5.46906709671s
step 848/400000: train loss: 0.505940
epoch:18, batch22, load frames use: 6.31556797028s
step 849/400000: train loss: 0.534275
epoch:18, batch23, load frames use: 6.11585187912s
step 850/400000: train loss: 0.467812
epoch:18, batch24, load frames use: 6.14154696465s
step 851/400000: train loss: 0.535813
epoch:18, batch25, load frames use: 5.95089483261s
step 852/400000: train loss: 0.534501
epoch:18, batch26, load frames use: 5.43841910362s
step 853/400000: train loss: 0.513179
epoch:18, batch27, load frames use: 5.11230516434s
step 854/400000: train loss: 0.548570
epoch:18, batch28, load frames use: 6.1809899807s
step 855/400000: train loss: 0.532863
epoch:18, batch29, load frames use: 3.8006529808s
step 856/400000: train loss: 0.491942
epoch:18, batch30, load frames use: 4.31892204285s
step 857/400000: train loss: 0.557523
epoch:18, batch31, load frames use: 4.20161008835s
step 858/400000: train loss: 0.453813
epoch:18, batch32, load frames use: 3.81148195267s
step 859/400000: train loss: 0.521633
epoch:18, batch33, load frames use: 3.74866104126s
step 860/400000: train loss: 0.543476
epoch:18, batch34, load frames use: 3.85154008865s
step 861/400000: train loss: 0.547162
epoch:18, batch35, load frames use: 4.96755886078s
step 862/400000: train loss: 0.498612
epoch:18, batch36, load frames use: 3.69037389755s
step 863/400000: train loss: 0.473972
epoch:18, batch37, load frames use: 3.86192107201s
step 864/400000: train loss: 0.513760
epoch:18, batch38, load frames use: 3.19276595116s
step 865/400000: train loss: 0.413530
epoch:18, batch39, load frames use: 3.3598818779s
step 866/400000: train loss: 0.563526
epoch:18, batch40, load frames use: 4.47370886803s
step 867/400000: train loss: 0.472125
epoch:18, batch41, load frames use: 3.81953406334s
step 868/400000: train loss: 0.546815
epoch:18, batch42, load frames use: 3.97180294991s
step 869/400000: train loss: 0.600405
epoch:18, batch43, load frames use: 3.56994199753s
step 870/400000: train loss: 0.483571
epoch:18, batch44, load frames use: 3.67320203781s
step 871/400000: train loss: 0.530062
epoch:18, batch45, load frames use: 3.53823399544s
step 872/400000: train loss: 0.487717
epoch:18, batch46, load frames use: 4.33460497856s
step 873/400000: train loss: 0.512149
epoch:19, batch1, load frames use: 3.65606403351s
step 874/400000: train loss: 0.524373
epoch:19, batch2, load frames use: 3.49433302879s
step 875/400000: train loss: 0.537198
epoch:19, batch3, load frames use: 4.27253079414s
step 876/400000: train loss: 0.536222
epoch:19, batch4, load frames use: 4.05429506302s
step 877/400000: train loss: 0.451949
epoch:19, batch5, load frames use: 4.73432207108s
step 878/400000: train loss: 0.511941
epoch:19, batch6, load frames use: 5.02329516411s
step 879/400000: train loss: 0.527603
epoch:19, batch7, load frames use: 3.54258394241s
step 880/400000: train loss: 0.481369
epoch:19, batch8, load frames use: 3.74749588966s
step 881/400000: train loss: 0.531427
epoch:19, batch9, load frames use: 3.60444188118s
step 882/400000: train loss: 0.546718
epoch:19, batch10, load frames use: 3.55094003677s
step 883/400000: train loss: 0.492638
epoch:19, batch11, load frames use: 4.40663599968s
step 884/400000: train loss: 0.603138
epoch:19, batch12, load frames use: 3.39119100571s
step 885/400000: train loss: 0.457437
epoch:19, batch13, load frames use: 3.93230819702s
step 886/400000: train loss: 0.449266
epoch:19, batch14, load frames use: 3.9594810009s
step 887/400000: train loss: 0.499978
epoch:19, batch15, load frames use: 4.22885704041s
step 888/400000: train loss: 0.530048
epoch:19, batch16, load frames use: 3.87740206718s
step 889/400000: train loss: 0.623710
epoch:19, batch17, load frames use: 3.68619799614s
step 890/400000: train loss: 0.542986
epoch:19, batch18, load frames use: 3.39808297157s
step 891/400000: train loss: 0.571588
epoch:19, batch19, load frames use: 3.6225669384s
step 892/400000: train loss: 0.442079
epoch:19, batch20, load frames use: 3.84918379784s
step 893/400000: train loss: 0.512217
epoch:19, batch21, load frames use: 3.58604478836s
step 894/400000: train loss: 0.469733
epoch:19, batch22, load frames use: 3.67472290993s
step 895/400000: train loss: 0.498473
epoch:19, batch23, load frames use: 3.93544197083s
step 896/400000: train loss: 0.489649
epoch:19, batch24, load frames use: 4.24827003479s
step 897/400000: train loss: 0.466196
epoch:19, batch25, load frames use: 4.63490796089s
step 898/400000: train loss: 0.553749
epoch:19, batch26, load frames use: 4.00961208344s
step 899/400000: train loss: 0.460087
epoch:19, batch27, load frames use: 3.59789896011s
step 900/400000: train loss: 0.495632
step 900/400000: validation loss: 0.478032
epoch:19, batch28, load frames use: 3.93963003159s
step 901/400000: train loss: 0.494965
epoch:19, batch29, load frames use: 3.59504508972s
step 902/400000: train loss: 0.513508
epoch:19, batch30, load frames use: 3.49625110626s
step 903/400000: train loss: 0.630229
epoch:19, batch31, load frames use: 4.17424297333s
step 904/400000: train loss: 0.493208
epoch:19, batch32, load frames use: 3.50528502464s
step 905/400000: train loss: 0.524639
epoch:19, batch33, load frames use: 3.8844461441s
step 906/400000: train loss: 0.493423
epoch:19, batch34, load frames use: 3.55599594116s
step 907/400000: train loss: 0.504916
epoch:19, batch35, load frames use: 4.77475190163s
step 908/400000: train loss: 0.639227
epoch:19, batch36, load frames use: 3.88808202744s
step 909/400000: train loss: 0.521854
epoch:19, batch37, load frames use: 3.33460903168s
step 910/400000: train loss: 0.582124
epoch:19, batch38, load frames use: 3.63657093048s
step 911/400000: train loss: 0.538312
epoch:19, batch39, load frames use: 3.6586420536s
step 912/400000: train loss: 0.544338
epoch:19, batch40, load frames use: 4.47022414207s
step 913/400000: train loss: 0.644030
epoch:19, batch41, load frames use: 3.45607280731s
step 914/400000: train loss: 0.489194
epoch:19, batch42, load frames use: 3.77321696281s
step 915/400000: train loss: 0.506266
epoch:19, batch43, load frames use: 3.61581897736s
step 916/400000: train loss: 0.461654
epoch:19, batch44, load frames use: 4.878718853s
step 917/400000: train loss: 0.539984
epoch:19, batch45, load frames use: 4.02145791054s
step 918/400000: train loss: 0.485329
epoch:19, batch46, load frames use: 4.39218592644s
step 919/400000: train loss: 0.541128
epoch:20, batch1, load frames use: 4.29593110085s
step 920/400000: train loss: 0.382757
epoch:20, batch2, load frames use: 3.70746898651s
step 921/400000: train loss: 0.570071
epoch:20, batch3, load frames use: 4.36454105377s
step 922/400000: train loss: 0.452050
epoch:20, batch4, load frames use: 3.62986207008s
step 923/400000: train loss: 0.416959
epoch:20, batch5, load frames use: 3.70499706268s
step 924/400000: train loss: 0.560027
epoch:20, batch6, load frames use: 4.46487212181s
step 925/400000: train loss: 0.506613
epoch:20, batch7, load frames use: 3.96285390854s
step 926/400000: train loss: 0.491706
epoch:20, batch8, load frames use: 4.57540607452s
step 927/400000: train loss: 0.552475
epoch:20, batch9, load frames use: 3.75053095818s
step 928/400000: train loss: 0.594832
epoch:20, batch10, load frames use: 3.20283794403s
step 929/400000: train loss: 0.546285
epoch:20, batch11, load frames use: 3.93895292282s
step 930/400000: train loss: 0.450966
epoch:20, batch12, load frames use: 3.39225792885s
step 931/400000: train loss: 0.482462
epoch:20, batch13, load frames use: 3.56802082062s
step 932/400000: train loss: 0.394669
epoch:20, batch14, load frames use: 3.51489210129s
step 933/400000: train loss: 0.509428
epoch:20, batch15, load frames use: 3.51380610466s
step 934/400000: train loss: 0.470876
epoch:20, batch16, load frames use: 4.04193305969s
step 935/400000: train loss: 0.460321
epoch:20, batch17, load frames use: 3.96536803246s
step 936/400000: train loss: 0.601363
epoch:20, batch18, load frames use: 3.56257295609s
step 937/400000: train loss: 0.505246
epoch:20, batch19, load frames use: 3.66930413246s
step 938/400000: train loss: 0.494485
epoch:20, batch20, load frames use: 5.17319321632s
step 939/400000: train loss: 0.464136
epoch:20, batch21, load frames use: 4.26376390457s
step 940/400000: train loss: 0.542301
epoch:20, batch22, load frames use: 4.43437504768s
step 941/400000: train loss: 0.567793
epoch:20, batch23, load frames use: 4.48036384583s
step 942/400000: train loss: 0.541674
epoch:20, batch24, load frames use: 5.36711907387s
step 943/400000: train loss: 0.514896
epoch:20, batch25, load frames use: 3.74290299416s
step 944/400000: train loss: 0.480219
epoch:20, batch26, load frames use: 3.5977370739s
step 945/400000: train loss: 0.471144
epoch:20, batch27, load frames use: 3.47065901756s
step 946/400000: train loss: 0.507501
epoch:20, batch28, load frames use: 3.71194791794s
step 947/400000: train loss: 0.575318
epoch:20, batch29, load frames use: 4.53186202049s
step 948/400000: train loss: 0.445218
epoch:20, batch30, load frames use: 4.55184793472s
step 949/400000: train loss: 0.459310
epoch:20, batch31, load frames use: 5.02418684959s
step 950/400000: train loss: 0.526414
epoch:20, batch32, load frames use: 4.48764586449s
step 951/400000: train loss: 0.471421
epoch:20, batch33, load frames use: 4.64616394043s
step 952/400000: train loss: 0.542236
epoch:20, batch34, load frames use: 4.68946814537s
step 953/400000: train loss: 0.499526
epoch:20, batch35, load frames use: 4.28432798386s
step 954/400000: train loss: 0.535738
epoch:20, batch36, load frames use: 3.58579993248s
step 955/400000: train loss: 0.479221
epoch:20, batch37, load frames use: 3.48181700706s
step 956/400000: train loss: 0.533253
epoch:20, batch38, load frames use: 3.64857196808s
step 957/400000: train loss: 0.544560
epoch:20, batch39, load frames use: 4.07001399994s
step 958/400000: train loss: 0.520984
epoch:20, batch40, load frames use: 3.82950997353s
step 959/400000: train loss: 0.486803
epoch:20, batch41, load frames use: 3.76266098022s
step 960/400000: train loss: 0.495429
epoch:20, batch42, load frames use: 3.70566487312s
step 961/400000: train loss: 0.432702
epoch:20, batch43, load frames use: 4.41930294037s
step 962/400000: train loss: 0.504547
epoch:20, batch44, load frames use: 4.7476348877s
step 963/400000: train loss: 0.631848
epoch:20, batch45, load frames use: 4.65413689613s
step 964/400000: train loss: 0.539887
epoch:20, batch46, load frames use: 4.25894498825s
step 965/400000: train loss: 0.432491
epoch:21, batch1, load frames use: 4.37312793732s
step 966/400000: train loss: 0.510090
epoch:21, batch2, load frames use: 3.48407506943s
step 967/400000: train loss: 0.450017
epoch:21, batch3, load frames use: 3.53942012787s
step 968/400000: train loss: 0.477074
epoch:21, batch4, load frames use: 3.45613789558s
step 969/400000: train loss: 0.500218
epoch:21, batch5, load frames use: 3.92565608025s
step 970/400000: train loss: 0.505556
epoch:21, batch6, load frames use: 4.46069216728s
step 971/400000: train loss: 0.500499
epoch:21, batch7, load frames use: 5.07962703705s
step 972/400000: train loss: 0.566383
epoch:21, batch8, load frames use: 4.5869269371s
step 973/400000: train loss: 0.469690
epoch:21, batch9, load frames use: 4.40607905388s
step 974/400000: train loss: 0.507446
epoch:21, batch10, load frames use: 4.57649612427s
step 975/400000: train loss: 0.484737
epoch:21, batch11, load frames use: 4.65669608116s
step 976/400000: train loss: 0.489395
epoch:21, batch12, load frames use: 4.31345891953s
step 977/400000: train loss: 0.495473
epoch:21, batch13, load frames use: 4.37570500374s
step 978/400000: train loss: 0.507244
epoch:21, batch14, load frames use: 3.74340200424s
step 979/400000: train loss: 0.676833
epoch:21, batch15, load frames use: 3.60513305664s
step 980/400000: train loss: 0.460434
epoch:21, batch16, load frames use: 3.64875507355s
step 981/400000: train loss: 0.482505
epoch:21, batch17, load frames use: 3.7488257885s
step 982/400000: train loss: 0.558828
epoch:21, batch18, load frames use: 4.05408883095s
step 983/400000: train loss: 0.461337
epoch:21, batch19, load frames use: 3.74262404442s
step 984/400000: train loss: 0.571656
epoch:21, batch20, load frames use: 3.33749103546s
step 985/400000: train loss: 0.480512
epoch:21, batch21, load frames use: 3.57612395287s
step 986/400000: train loss: 0.436818
epoch:21, batch22, load frames use: 3.49577212334s
step 987/400000: train loss: 0.428396
epoch:21, batch23, load frames use: 3.70539999008s
step 988/400000: train loss: 0.543016
epoch:21, batch24, load frames use: 3.55524301529s
step 989/400000: train loss: 0.502824
epoch:21, batch25, load frames use: 3.95895290375s
step 990/400000: train loss: 0.488614
epoch:21, batch26, load frames use: 4.71094512939s
step 991/400000: train loss: 0.415862
epoch:21, batch27, load frames use: 3.70547199249s
step 992/400000: train loss: 0.415158
epoch:21, batch28, load frames use: 3.40426301956s
step 993/400000: train loss: 0.580242
epoch:21, batch29, load frames use: 4.02514505386s
step 994/400000: train loss: 0.482963
epoch:21, batch30, load frames use: 3.88240909576s
step 995/400000: train loss: 0.435144
epoch:21, batch31, load frames use: 4.21378707886s
step 996/400000: train loss: 0.546717
epoch:21, batch32, load frames use: 4.60694289207s
step 997/400000: train loss: 0.515235
epoch:21, batch33, load frames use: 4.28795480728s
step 998/400000: train loss: 0.509968
epoch:21, batch34, load frames use: 3.61575698853s
step 999/400000: train loss: 0.543911
epoch:21, batch35, load frames use: 3.71516799927s
step 1000/400000: train loss: 0.574778
step 1000/400000: validation loss: 0.553221
epoch:21, batch36, load frames use: 3.46987509727s
step 1001/400000: train loss: 0.543696
epoch:21, batch37, load frames use: 3.72627711296s
step 1002/400000: train loss: 0.460545
epoch:21, batch38, load frames use: 3.80086112022s
step 1003/400000: train loss: 0.421982
epoch:21, batch39, load frames use: 3.5805850029s
step 1004/400000: train loss: 0.533676
epoch:21, batch40, load frames use: 3.58825111389s
step 1005/400000: train loss: 0.417629
epoch:21, batch41, load frames use: 3.75417995453s
step 1006/400000: train loss: 0.513557
epoch:21, batch42, load frames use: 4.03601789474s
step 1007/400000: train loss: 0.539595
epoch:21, batch43, load frames use: 4.01329803467s
step 1008/400000: train loss: 0.472404
epoch:21, batch44, load frames use: 3.79703998566s
step 1009/400000: train loss: 0.546479
epoch:21, batch45, load frames use: 3.82460999489s
step 1010/400000: train loss: 0.564197
epoch:21, batch46, load frames use: 3.47348189354s
step 1011/400000: train loss: 0.441100
epoch:22, batch1, load frames use: 4.15672087669s
step 1012/400000: train loss: 0.520328
epoch:22, batch2, load frames use: 3.88264298439s
step 1013/400000: train loss: 0.565679
epoch:22, batch3, load frames use: 3.55262207985s
step 1014/400000: train loss: 0.466858
epoch:22, batch4, load frames use: 3.89344787598s
step 1015/400000: train loss: 0.491067
epoch:22, batch5, load frames use: 4.92328310013s
step 1016/400000: train loss: 0.466619
epoch:22, batch6, load frames use: 3.86080694199s
step 1017/400000: train loss: 0.506217
epoch:22, batch7, load frames use: 3.74057602882s
step 1018/400000: train loss: 0.457674
epoch:22, batch8, load frames use: 3.61785197258s
step 1019/400000: train loss: 0.536878
epoch:22, batch9, load frames use: 3.69960308075s
step 1020/400000: train loss: 0.515846
epoch:22, batch10, load frames use: 4.12693405151s
step 1021/400000: train loss: 0.467620
epoch:22, batch11, load frames use: 3.75194811821s
step 1022/400000: train loss: 0.447911
epoch:22, batch12, load frames use: 3.80318284035s
step 1023/400000: train loss: 0.522718
epoch:22, batch13, load frames use: 3.49043512344s
step 1024/400000: train loss: 0.538939
epoch:22, batch14, load frames use: 3.99224495888s
step 1025/400000: train loss: 0.374585
epoch:22, batch15, load frames use: 4.04835605621s
step 1026/400000: train loss: 0.503833
epoch:22, batch16, load frames use: 3.86933708191s
step 1027/400000: train loss: 0.501143
epoch:22, batch17, load frames use: 3.68345999718s
step 1028/400000: train loss: 0.508067
epoch:22, batch18, load frames use: 4.29721307755s
step 1029/400000: train loss: 0.506871
epoch:22, batch19, load frames use: 4.27485203743s
step 1030/400000: train loss: 0.564111
epoch:22, batch20, load frames use: 3.56292390823s
step 1031/400000: train loss: 0.434783
epoch:22, batch21, load frames use: 3.88083195686s
step 1032/400000: train loss: 0.457326
epoch:22, batch22, load frames use: 3.88132500648s
step 1033/400000: train loss: 0.480899
epoch:22, batch23, load frames use: 4.98465108871s
step 1034/400000: train loss: 0.480415
epoch:22, batch24, load frames use: 4.49487900734s
step 1035/400000: train loss: 0.438189
epoch:22, batch25, load frames use: 3.90079689026s
step 1036/400000: train loss: 0.480076
epoch:22, batch26, load frames use: 3.92351198196s
step 1037/400000: train loss: 0.477080
epoch:22, batch27, load frames use: 3.83078718185s
step 1038/400000: train loss: 0.430335
epoch:22, batch28, load frames use: 4.02287912369s
step 1039/400000: train loss: 0.472433
epoch:22, batch29, load frames use: 4.43185114861s
step 1040/400000: train loss: 0.444687
epoch:22, batch30, load frames use: 3.66385412216s
step 1041/400000: train loss: 0.483778
epoch:22, batch31, load frames use: 3.66439199448s
step 1042/400000: train loss: 0.495852
epoch:22, batch32, load frames use: 3.98056483269s
step 1043/400000: train loss: 0.516582
epoch:22, batch33, load frames use: 3.68588399887s
step 1044/400000: train loss: 0.433026
epoch:22, batch34, load frames use: 4.04856705666s
step 1045/400000: train loss: 0.505927
epoch:22, batch35, load frames use: 3.65709114075s
step 1046/400000: train loss: 0.472253
epoch:22, batch36, load frames use: 3.76140618324s
step 1047/400000: train loss: 0.469299
epoch:22, batch37, load frames use: 3.72780394554s
step 1048/400000: train loss: 0.465887
epoch:22, batch38, load frames use: 4.61530184746s
step 1049/400000: train loss: 0.480189
epoch:22, batch39, load frames use: 4.05255293846s
step 1050/400000: train loss: 0.452858
epoch:22, batch40, load frames use: 3.84935688972s
step 1051/400000: train loss: 0.465010
epoch:22, batch41, load frames use: 3.92182683945s
step 1052/400000: train loss: 0.504304
epoch:22, batch42, load frames use: 4.0491130352s
step 1053/400000: train loss: 0.510753
epoch:22, batch43, load frames use: 4.90858697891s
step 1054/400000: train loss: 0.527635
epoch:22, batch44, load frames use: 4.453125s
step 1055/400000: train loss: 0.470157
epoch:22, batch45, load frames use: 4.28822302818s
step 1056/400000: train loss: 0.534557
epoch:22, batch46, load frames use: 3.54128193855s
step 1057/400000: train loss: 0.534011
epoch:23, batch1, load frames use: 3.8700799942s
step 1058/400000: train loss: 0.464884
epoch:23, batch2, load frames use: 3.75949811935s
step 1059/400000: train loss: 0.477192
epoch:23, batch3, load frames use: 4.63732814789s
step 1060/400000: train loss: 0.464286
epoch:23, batch4, load frames use: 4.7629430294s
step 1061/400000: train loss: 0.395315
epoch:23, batch5, load frames use: 4.40293097496s
step 1062/400000: train loss: 0.461740
epoch:23, batch6, load frames use: 4.52245402336s
step 1063/400000: train loss: 0.494901
epoch:23, batch7, load frames use: 3.59870791435s
step 1064/400000: train loss: 0.454196
epoch:23, batch8, load frames use: 3.7043671608s
step 1065/400000: train loss: 0.450940
epoch:23, batch9, load frames use: 3.72877883911s
step 1066/400000: train loss: 0.446297
epoch:23, batch10, load frames use: 3.98658490181s
step 1067/400000: train loss: 0.473223
epoch:23, batch11, load frames use: 4.65393710136s
step 1068/400000: train loss: 0.407857
epoch:23, batch12, load frames use: 3.47740197182s
step 1069/400000: train loss: 0.516638
epoch:23, batch13, load frames use: 3.59878396988s
step 1070/400000: train loss: 0.425616
epoch:23, batch14, load frames use: 3.62966394424s
step 1071/400000: train loss: 0.668998
epoch:23, batch15, load frames use: 3.96562695503s
step 1072/400000: train loss: 0.494165
epoch:23, batch16, load frames use: 4.30287885666s
step 1073/400000: train loss: 0.666649
epoch:23, batch17, load frames use: 3.71018099785s
step 1074/400000: train loss: 0.499738
epoch:23, batch18, load frames use: 3.36676216125s
step 1075/400000: train loss: 0.445378
epoch:23, batch19, load frames use: 4.04687213898s
step 1076/400000: train loss: 0.504101
epoch:23, batch20, load frames use: 4.01064705849s
step 1077/400000: train loss: 0.561874
epoch:23, batch21, load frames use: 4.55391001701s
step 1078/400000: train loss: 0.546289
epoch:23, batch22, load frames use: 4.34525799751s
step 1079/400000: train loss: 0.563234
epoch:23, batch23, load frames use: 3.52020096779s
step 1080/400000: train loss: 0.514066
epoch:23, batch24, load frames use: 4.01976084709s
step 1081/400000: train loss: 0.580631
epoch:23, batch25, load frames use: 3.66009902954s
step 1082/400000: train loss: 0.463283
epoch:23, batch26, load frames use: 3.99588799477s
step 1083/400000: train loss: 0.450195
epoch:23, batch27, load frames use: 4.61891198158s
step 1084/400000: train loss: 0.535014
epoch:23, batch28, load frames use: 3.53180289268s
step 1085/400000: train loss: 0.477846
epoch:23, batch29, load frames use: 3.48350095749s
step 1086/400000: train loss: 0.428128
epoch:23, batch30, load frames use: 3.95795488358s
step 1087/400000: train loss: 0.500186
epoch:23, batch31, load frames use: 3.90772891045s
step 1088/400000: train loss: 0.568968
epoch:23, batch32, load frames use: 5.02263879776s
step 1089/400000: train loss: 0.510059
epoch:23, batch33, load frames use: 4.86675715446s
step 1090/400000: train loss: 0.506306
epoch:23, batch34, load frames use: 3.6159529686s
step 1091/400000: train loss: 0.472396
epoch:23, batch35, load frames use: 4.46544289589s
step 1092/400000: train loss: 0.489440
epoch:23, batch36, load frames use: 3.74340891838s
step 1093/400000: train loss: 0.525160
epoch:23, batch37, load frames use: 3.56632709503s
step 1094/400000: train loss: 0.451751
epoch:23, batch38, load frames use: 4.19778203964s
step 1095/400000: train loss: 0.496594
epoch:23, batch39, load frames use: 3.58150482178s
step 1096/400000: train loss: 0.516543
epoch:23, batch40, load frames use: 3.56882095337s
step 1097/400000: train loss: 0.485338
epoch:23, batch41, load frames use: 3.58595180511s
step 1098/400000: train loss: 0.508011
epoch:23, batch42, load frames use: 4.14578795433s
step 1099/400000: train loss: 0.435885
epoch:23, batch43, load frames use: 3.62898087502s
step 1100/400000: train loss: 0.491834
step 1100/400000: validation loss: 0.464474
epoch:23, batch44, load frames use: 4.52906489372s
step 1101/400000: train loss: 0.451404
epoch:23, batch45, load frames use: 3.5818400383s
step 1102/400000: train loss: 0.514282
epoch:23, batch46, load frames use: 4.01632213593s
step 1103/400000: train loss: 0.500560
epoch:24, batch1, load frames use: 4.31627106667s
step 1104/400000: train loss: 0.427272
epoch:24, batch2, load frames use: 3.76278781891s
step 1105/400000: train loss: 0.536012
epoch:24, batch3, load frames use: 3.96424818039s
step 1106/400000: train loss: 0.448322
epoch:24, batch4, load frames use: 3.52205085754s
step 1107/400000: train loss: 0.508403
epoch:24, batch5, load frames use: 3.63860011101s
step 1108/400000: train loss: 0.470392
epoch:24, batch6, load frames use: 4.02932715416s
step 1109/400000: train loss: 0.523693
epoch:24, batch7, load frames use: 4.09323692322s
step 1110/400000: train loss: 0.485679
epoch:24, batch8, load frames use: 3.82977199554s
step 1111/400000: train loss: 0.468269
epoch:24, batch9, load frames use: 3.59581494331s
step 1112/400000: train loss: 0.447866
epoch:24, batch10, load frames use: 3.8611998558s
step 1113/400000: train loss: 0.591034
epoch:24, batch11, load frames use: 3.82716107368s
step 1114/400000: train loss: 0.425644
epoch:24, batch12, load frames use: 4.85849118233s
step 1115/400000: train loss: 0.455362
epoch:24, batch13, load frames use: 4.16460990906s
step 1116/400000: train loss: 0.507022
epoch:24, batch14, load frames use: 5.11588001251s
step 1117/400000: train loss: 0.450921
epoch:24, batch15, load frames use: 3.72062778473s
step 1118/400000: train loss: 0.488963
epoch:24, batch16, load frames use: 4.26722812653s
step 1119/400000: train loss: 0.385037
epoch:24, batch17, load frames use: 4.3136780262s
step 1120/400000: train loss: 0.500843
epoch:24, batch18, load frames use: 3.9959628582s
step 1121/400000: train loss: 0.436410
epoch:24, batch19, load frames use: 3.70067811012s
step 1122/400000: train loss: 0.563257
epoch:24, batch20, load frames use: 3.47334289551s
step 1123/400000: train loss: 0.568232
epoch:24, batch21, load frames use: 3.93001794815s
step 1124/400000: train loss: 0.572322
epoch:24, batch22, load frames use: 3.75459694862s
step 1125/400000: train loss: 0.532190
epoch:24, batch23, load frames use: 3.52502298355s
step 1126/400000: train loss: 0.447240
epoch:24, batch24, load frames use: 3.81076908112s
step 1127/400000: train loss: 0.472252
epoch:24, batch25, load frames use: 4.06347107887s
step 1128/400000: train loss: 0.456214
epoch:24, batch26, load frames use: 3.89482688904s
step 1129/400000: train loss: 0.463129
epoch:24, batch27, load frames use: 3.50581812859s
step 1130/400000: train loss: 0.456505
epoch:24, batch28, load frames use: 3.45010685921s
step 1131/400000: train loss: 0.438690
epoch:24, batch29, load frames use: 4.12006998062s
step 1132/400000: train loss: 0.451689
epoch:24, batch30, load frames use: 3.76230502129s
step 1133/400000: train loss: 0.389100
epoch:24, batch31, load frames use: 3.6761701107s
step 1134/400000: train loss: 0.548343
epoch:24, batch32, load frames use: 3.83050894737s
step 1135/400000: train loss: 0.479349
epoch:24, batch33, load frames use: 4.2430369854s
step 1136/400000: train loss: 0.498186
epoch:24, batch34, load frames use: 3.98586297035s
step 1137/400000: train loss: 0.517594
epoch:24, batch35, load frames use: 4.03855204582s
step 1138/400000: train loss: 0.434175
epoch:24, batch36, load frames use: 3.61168503761s
step 1139/400000: train loss: 0.440788
epoch:24, batch37, load frames use: 3.81088209152s
step 1140/400000: train loss: 0.499971
epoch:24, batch38, load frames use: 3.69135499001s
step 1141/400000: train loss: 0.473907
epoch:24, batch39, load frames use: 4.48067188263s
step 1142/400000: train loss: 0.477990
epoch:24, batch40, load frames use: 4.67980694771s
step 1143/400000: train loss: 0.491591
epoch:24, batch41, load frames use: 4.27704000473s
step 1144/400000: train loss: 0.485050
epoch:24, batch42, load frames use: 3.58300209045s
step 1145/400000: train loss: 0.453301
epoch:24, batch43, load frames use: 3.70922398567s
step 1146/400000: train loss: 0.520416
epoch:24, batch44, load frames use: 3.53617811203s
step 1147/400000: train loss: 0.504674
epoch:24, batch45, load frames use: 3.79439210892s
step 1148/400000: train loss: 0.459449
epoch:24, batch46, load frames use: 4.52615118027s
step 1149/400000: train loss: 0.438006
epoch:25, batch1, load frames use: 3.60460686684s
step 1150/400000: train loss: 0.500117
epoch:25, batch2, load frames use: 4.18129897118s
step 1151/400000: train loss: 0.396787
epoch:25, batch3, load frames use: 3.68902492523s
step 1152/400000: train loss: 0.488335
epoch:25, batch4, load frames use: 4.67687296867s
step 1153/400000: train loss: 0.484694
epoch:25, batch5, load frames use: 4.74125695229s
step 1154/400000: train loss: 0.477795
epoch:25, batch6, load frames use: 4.60740494728s
step 1155/400000: train loss: 0.371906
epoch:25, batch7, load frames use: 4.55856490135s
step 1156/400000: train loss: 0.452654
epoch:25, batch8, load frames use: 5.70230102539s
step 1157/400000: train loss: 0.581147
epoch:25, batch9, load frames use: 3.90374016762s
step 1158/400000: train loss: 0.468340
epoch:25, batch10, load frames use: 4.43515205383s
step 1159/400000: train loss: 0.473644
epoch:25, batch11, load frames use: 3.78800797462s
step 1160/400000: train loss: 0.484649
epoch:25, batch12, load frames use: 3.64651107788s
step 1161/400000: train loss: 0.462620
epoch:25, batch13, load frames use: 3.72561812401s
step 1162/400000: train loss: 0.371055
epoch:25, batch14, load frames use: 4.34339404106s
step 1163/400000: train loss: 0.424387
epoch:25, batch15, load frames use: 4.73369598389s
step 1164/400000: train loss: 0.457075
epoch:25, batch16, load frames use: 4.74682998657s
step 1165/400000: train loss: 0.550835
epoch:25, batch17, load frames use: 4.60692596436s
step 1166/400000: train loss: 0.448907
epoch:25, batch18, load frames use: 4.82028889656s
step 1167/400000: train loss: 0.422990
epoch:25, batch19, load frames use: 4.18307495117s
step 1168/400000: train loss: 0.442610
epoch:25, batch20, load frames use: 3.75960803032s
step 1169/400000: train loss: 0.428854
epoch:25, batch21, load frames use: 3.82977700233s
step 1170/400000: train loss: 0.499845
epoch:25, batch22, load frames use: 3.9975631237s
step 1171/400000: train loss: 0.453650
epoch:25, batch23, load frames use: 3.76065707207s
step 1172/400000: train loss: 0.449808
epoch:25, batch24, load frames use: 4.53322887421s
step 1173/400000: train loss: 0.502899
epoch:25, batch25, load frames use: 3.93157601357s
step 1174/400000: train loss: 0.433405
epoch:25, batch26, load frames use: 3.66175293922s
step 1175/400000: train loss: 0.470275
epoch:25, batch27, load frames use: 3.60473799706s
step 1176/400000: train loss: 0.453750
epoch:25, batch28, load frames use: 3.90652513504s
step 1177/400000: train loss: 0.415219
epoch:25, batch29, load frames use: 3.69470620155s
step 1178/400000: train loss: 0.414041
epoch:25, batch30, load frames use: 3.71999406815s
step 1179/400000: train loss: 0.389636
epoch:25, batch31, load frames use: 3.5210750103s
step 1180/400000: train loss: 0.438751
epoch:25, batch32, load frames use: 4.93298912048s
step 1181/400000: train loss: 0.460879
epoch:25, batch33, load frames use: 3.9910531044s
step 1182/400000: train loss: 0.473899
epoch:25, batch34, load frames use: 5.2798371315s
step 1183/400000: train loss: 0.445097
epoch:25, batch35, load frames use: 4.11593413353s
step 1184/400000: train loss: 0.504740
epoch:25, batch36, load frames use: 4.51128101349s
step 1185/400000: train loss: 0.516740
epoch:25, batch37, load frames use: 4.84566092491s
step 1186/400000: train loss: 0.498843
epoch:25, batch38, load frames use: 4.00820589066s
step 1187/400000: train loss: 0.453765
epoch:25, batch39, load frames use: 4.03165507317s
step 1188/400000: train loss: 0.475474
epoch:25, batch40, load frames use: 3.57698488235s
step 1189/400000: train loss: 0.461568
epoch:25, batch41, load frames use: 3.68536877632s
step 1190/400000: train loss: 0.491051
epoch:25, batch42, load frames use: 4.15189504623s
step 1191/400000: train loss: 0.433176
epoch:25, batch43, load frames use: 3.94002699852s
step 1192/400000: train loss: 0.450840
epoch:25, batch44, load frames use: 3.39962911606s
step 1193/400000: train loss: 0.416084
epoch:25, batch45, load frames use: 3.90156316757s
step 1194/400000: train loss: 0.484787
epoch:25, batch46, load frames use: 3.67670583725s
step 1195/400000: train loss: 0.537809
epoch:26, batch1, load frames use: 4.70336318016s
step 1196/400000: train loss: 0.503448
epoch:26, batch2, load frames use: 3.76578688622s
step 1197/400000: train loss: 0.425796
epoch:26, batch3, load frames use: 3.97759389877s
step 1198/400000: train loss: 0.416677
epoch:26, batch4, load frames use: 3.71486186981s
step 1199/400000: train loss: 0.406820
epoch:26, batch5, load frames use: 3.63597989082s
step 1200/400000: train loss: 0.406416
step 1200/400000: validation loss: 0.424382
epoch:26, batch6, load frames use: 4.34901595116s
step 1201/400000: train loss: 0.504468
epoch:26, batch7, load frames use: 4.04964804649s
step 1202/400000: train loss: 0.434716
epoch:26, batch8, load frames use: 4.4768948555s
step 1203/400000: train loss: 0.492919
epoch:26, batch9, load frames use: 4.15035676956s
step 1204/400000: train loss: 0.464898
epoch:26, batch10, load frames use: 3.74221014977s
step 1205/400000: train loss: 0.432807
epoch:26, batch11, load frames use: 3.76717996597s
step 1206/400000: train loss: 0.488560
epoch:26, batch12, load frames use: 3.85627007484s
step 1207/400000: train loss: 0.428888
epoch:26, batch13, load frames use: 4.00272202492s
step 1208/400000: train loss: 0.456533
epoch:26, batch14, load frames use: 4.86642408371s
step 1209/400000: train loss: 0.472665
epoch:26, batch15, load frames use: 4.36603403091s
step 1210/400000: train loss: 0.480671
epoch:26, batch16, load frames use: 3.54863405228s
step 1211/400000: train loss: 0.401672
epoch:26, batch17, load frames use: 3.54426598549s
step 1212/400000: train loss: 0.467014
epoch:26, batch18, load frames use: 3.63717699051s
step 1213/400000: train loss: 0.472572
epoch:26, batch19, load frames use: 4.60607099533s
step 1214/400000: train loss: 0.522997
epoch:26, batch20, load frames use: 4.2477478981s
step 1215/400000: train loss: 0.476546
epoch:26, batch21, load frames use: 3.98385095596s
step 1216/400000: train loss: 0.457604
epoch:26, batch22, load frames use: 3.74483299255s
step 1217/400000: train loss: 0.512680
epoch:26, batch23, load frames use: 3.83176493645s
step 1218/400000: train loss: 0.449584
epoch:26, batch24, load frames use: 3.87219691277s
step 1219/400000: train loss: 0.474509
epoch:26, batch25, load frames use: 4.85688114166s
step 1220/400000: train loss: 0.480640
epoch:26, batch26, load frames use: 3.56287884712s
step 1221/400000: train loss: 0.391472
epoch:26, batch27, load frames use: 3.61739706993s
step 1222/400000: train loss: 0.429239
epoch:26, batch28, load frames use: 3.62938499451s
step 1223/400000: train loss: 0.436156
epoch:26, batch29, load frames use: 3.85766196251s
step 1224/400000: train loss: 0.424531
epoch:26, batch30, load frames use: 5.21229982376s
step 1225/400000: train loss: 0.438057
epoch:26, batch31, load frames use: 3.5577018261s
step 1226/400000: train loss: 0.445439
epoch:26, batch32, load frames use: 3.92793107033s
step 1227/400000: train loss: 0.480754
epoch:26, batch33, load frames use: 3.63493704796s
step 1228/400000: train loss: 0.440492
epoch:26, batch34, load frames use: 3.92597103119s
step 1229/400000: train loss: 0.461409
epoch:26, batch35, load frames use: 4.02012896538s
step 1230/400000: train loss: 0.445222
epoch:26, batch36, load frames use: 4.28587794304s
step 1231/400000: train loss: 0.467960
epoch:26, batch37, load frames use: 4.46138191223s
step 1232/400000: train loss: 0.414640
epoch:26, batch38, load frames use: 3.69116210938s
step 1233/400000: train loss: 0.418742
epoch:26, batch39, load frames use: 3.81939387321s
step 1234/400000: train loss: 0.501622
epoch:26, batch40, load frames use: 3.9575779438s
step 1235/400000: train loss: 0.456638
epoch:26, batch41, load frames use: 3.96111798286s
step 1236/400000: train loss: 0.373159
epoch:26, batch42, load frames use: 4.56255197525s
step 1237/400000: train loss: 0.402623
epoch:26, batch43, load frames use: 5.64041590691s
step 1238/400000: train loss: 0.469116
epoch:26, batch44, load frames use: 3.9055929184s
step 1239/400000: train loss: 0.486729
epoch:26, batch45, load frames use: 3.58403205872s
step 1240/400000: train loss: 0.433271
epoch:26, batch46, load frames use: 4.04122710228s
step 1241/400000: train loss: 0.404868
epoch:27, batch1, load frames use: 3.78575205803s
step 1242/400000: train loss: 0.474605
epoch:27, batch2, load frames use: 3.91006088257s
step 1243/400000: train loss: 0.403709
epoch:27, batch3, load frames use: 4.25924801826s
step 1244/400000: train loss: 0.499836
epoch:27, batch4, load frames use: 3.56611609459s
step 1245/400000: train loss: 0.449221
epoch:27, batch5, load frames use: 3.66760396957s
step 1246/400000: train loss: 0.433910
epoch:27, batch6, load frames use: 3.59855008125s
step 1247/400000: train loss: 0.473830
epoch:27, batch7, load frames use: 4.39427018166s
step 1248/400000: train loss: 0.399997
epoch:27, batch8, load frames use: 4.45584893227s
step 1249/400000: train loss: 0.476367
epoch:27, batch9, load frames use: 4.5504181385s
step 1250/400000: train loss: 0.456197
epoch:27, batch10, load frames use: 4.23210477829s
step 1251/400000: train loss: 0.366779
epoch:27, batch11, load frames use: 3.77150392532s
step 1252/400000: train loss: 0.468053
epoch:27, batch12, load frames use: 4.11010503769s
step 1253/400000: train loss: 0.389772
epoch:27, batch13, load frames use: 3.57576084137s
step 1254/400000: train loss: 0.462206
epoch:27, batch14, load frames use: 4.15585494041s
step 1255/400000: train loss: 0.423059
epoch:27, batch15, load frames use: 3.81670498848s
step 1256/400000: train loss: 0.443094
epoch:27, batch16, load frames use: 3.60053491592s
step 1257/400000: train loss: 0.516456
epoch:27, batch17, load frames use: 4.00302481651s
step 1258/400000: train loss: 0.381497
epoch:27, batch18, load frames use: 4.34681391716s
step 1259/400000: train loss: 0.422582
epoch:27, batch19, load frames use: 5.30816793442s
step 1260/400000: train loss: 0.446046
epoch:27, batch20, load frames use: 4.19547605515s
step 1261/400000: train loss: 0.459017
epoch:27, batch21, load frames use: 4.62057709694s
step 1262/400000: train loss: 0.445412
epoch:27, batch22, load frames use: 3.92775702477s
step 1263/400000: train loss: 0.515777
epoch:27, batch23, load frames use: 3.46146988869s
step 1264/400000: train loss: 0.484722
epoch:27, batch24, load frames use: 3.58527302742s
step 1265/400000: train loss: 0.352140
epoch:27, batch25, load frames use: 3.50569605827s
step 1266/400000: train loss: 0.453803
epoch:27, batch26, load frames use: 4.61371397972s
step 1267/400000: train loss: 0.405776
epoch:27, batch27, load frames use: 4.53546690941s
step 1268/400000: train loss: 0.406418
epoch:27, batch28, load frames use: 3.58994102478s
step 1269/400000: train loss: 0.465669
epoch:27, batch29, load frames use: 3.71668481827s
step 1270/400000: train loss: 0.474654
epoch:27, batch30, load frames use: 3.88346791267s
step 1271/400000: train loss: 0.424750
epoch:27, batch31, load frames use: 3.64169096947s
step 1272/400000: train loss: 0.449578
epoch:27, batch32, load frames use: 4.13656306267s
step 1273/400000: train loss: 0.443689
epoch:27, batch33, load frames use: 3.77020907402s
step 1274/400000: train loss: 0.385560
epoch:27, batch34, load frames use: 3.70476388931s
step 1275/400000: train loss: 0.511426
epoch:27, batch35, load frames use: 3.83354616165s
step 1276/400000: train loss: 0.391837
epoch:27, batch36, load frames use: 4.14454507828s
step 1277/400000: train loss: 0.427553
epoch:27, batch37, load frames use: 3.37845897675s
step 1278/400000: train loss: 0.449859
epoch:27, batch38, load frames use: 3.89222598076s
step 1279/400000: train loss: 0.512026
epoch:27, batch39, load frames use: 3.5695579052s
step 1280/400000: train loss: 0.496842
epoch:27, batch40, load frames use: 4.61527204514s
step 1281/400000: train loss: 0.553662
epoch:27, batch41, load frames use: 4.38105916977s
step 1282/400000: train loss: 0.409753
epoch:27, batch42, load frames use: 3.76832199097s
step 1283/400000: train loss: 0.466237
epoch:27, batch43, load frames use: 3.64908981323s
step 1284/400000: train loss: 0.509041
epoch:27, batch44, load frames use: 3.51372098923s
step 1285/400000: train loss: 0.484683
epoch:27, batch45, load frames use: 4.35014796257s
step 1286/400000: train loss: 0.439830
epoch:27, batch46, load frames use: 4.56512117386s
step 1287/400000: train loss: 0.455061
epoch:28, batch1, load frames use: 4.46488118172s
step 1288/400000: train loss: 0.425780
epoch:28, batch2, load frames use: 4.02089309692s
step 1289/400000: train loss: 0.447495
epoch:28, batch3, load frames use: 3.77390384674s
step 1290/400000: train loss: 0.460445
epoch:28, batch4, load frames use: 3.96586704254s
step 1291/400000: train loss: 0.403200
epoch:28, batch5, load frames use: 3.54424881935s
step 1292/400000: train loss: 0.421513
epoch:28, batch6, load frames use: 4.05877113342s
step 1293/400000: train loss: 0.474511
epoch:28, batch7, load frames use: 4.22162604332s
step 1294/400000: train loss: 0.424181
epoch:28, batch8, load frames use: 3.67277789116s
step 1295/400000: train loss: 0.405827
epoch:28, batch9, load frames use: 4.29923081398s
step 1296/400000: train loss: 0.385985
epoch:28, batch10, load frames use: 4.53366279602s
step 1297/400000: train loss: 0.369405
epoch:28, batch11, load frames use: 4.45272803307s
step 1298/400000: train loss: 0.420004
epoch:28, batch12, load frames use: 3.5749669075s
step 1299/400000: train loss: 0.471670
epoch:28, batch13, load frames use: 3.96054792404s
step 1300/400000: train loss: 0.444448
step 1300/400000: validation loss: 0.443063
epoch:28, batch14, load frames use: 3.64440584183s
step 1301/400000: train loss: 0.403449
epoch:28, batch15, load frames use: 4.09192204475s
step 1302/400000: train loss: 0.400171
epoch:28, batch16, load frames use: 3.82410407066s
step 1303/400000: train loss: 0.458284
epoch:28, batch17, load frames use: 3.83268094063s
step 1304/400000: train loss: 0.423717
epoch:28, batch18, load frames use: 3.65165901184s
step 1305/400000: train loss: 0.392126
epoch:28, batch19, load frames use: 4.15788292885s
step 1306/400000: train loss: 0.428839
epoch:28, batch20, load frames use: 3.68546509743s
step 1307/400000: train loss: 0.507933
epoch:28, batch21, load frames use: 3.97293615341s
step 1308/400000: train loss: 0.448730
epoch:28, batch22, load frames use: 3.55135512352s
step 1309/400000: train loss: 0.452413
epoch:28, batch23, load frames use: 4.9990260601s
step 1310/400000: train loss: 0.381088
epoch:28, batch24, load frames use: 4.50732398033s
step 1311/400000: train loss: 0.397273
epoch:28, batch25, load frames use: 4.63659501076s
step 1312/400000: train loss: 0.371298
epoch:28, batch26, load frames use: 3.79790592194s
step 1313/400000: train loss: 0.488208
epoch:28, batch27, load frames use: 3.65437507629s
step 1314/400000: train loss: 0.362311
epoch:28, batch28, load frames use: 3.78991603851s
step 1315/400000: train loss: 0.341216
epoch:28, batch29, load frames use: 3.66661906242s
step 1316/400000: train loss: 0.402039
epoch:28, batch30, load frames use: 4.73494720459s
step 1317/400000: train loss: 0.463285
epoch:28, batch31, load frames use: 3.82740592957s
step 1318/400000: train loss: 0.444901
epoch:28, batch32, load frames use: 4.22967982292s
step 1319/400000: train loss: 0.480045
epoch:28, batch33, load frames use: 3.9928920269s
step 1320/400000: train loss: 0.453771
epoch:28, batch34, load frames use: 3.84280514717s
step 1321/400000: train loss: 0.402977
epoch:28, batch35, load frames use: 4.15978693962s
step 1322/400000: train loss: 0.468015
epoch:28, batch36, load frames use: 3.47707104683s
step 1323/400000: train loss: 0.450500
epoch:28, batch37, load frames use: 3.99352312088s
step 1324/400000: train loss: 0.448566
epoch:28, batch38, load frames use: 3.99863815308s
step 1325/400000: train loss: 0.395064
epoch:28, batch39, load frames use: 4.29863595963s
step 1326/400000: train loss: 0.427012
epoch:28, batch40, load frames use: 3.628292799s
step 1327/400000: train loss: 0.424379
epoch:28, batch41, load frames use: 3.77213215828s
step 1328/400000: train loss: 0.417962
epoch:28, batch42, load frames use: 3.84537911415s
step 1329/400000: train loss: 0.363926
epoch:28, batch43, load frames use: 4.58852100372s
step 1330/400000: train loss: 0.471103
epoch:28, batch44, load frames use: 4.33009910583s
step 1331/400000: train loss: 0.418986
epoch:28, batch45, load frames use: 3.93636989594s
step 1332/400000: train loss: 0.370514
epoch:28, batch46, load frames use: 3.78227901459s
step 1333/400000: train loss: 0.461264
epoch:29, batch1, load frames use: 3.84246611595s
step 1334/400000: train loss: 0.380266
epoch:29, batch2, load frames use: 4.12148094177s
step 1335/400000: train loss: 0.394660
epoch:29, batch3, load frames use: 3.85865306854s
step 1336/400000: train loss: 0.404446
epoch:29, batch4, load frames use: 4.42221713066s
step 1337/400000: train loss: 0.381407
epoch:29, batch5, load frames use: 4.01628398895s
step 1338/400000: train loss: 0.390368
epoch:29, batch6, load frames use: 3.83682608604s
step 1339/400000: train loss: 0.398586
epoch:29, batch7, load frames use: 3.73619604111s
step 1340/400000: train loss: 0.406658
epoch:29, batch8, load frames use: 3.92128801346s
step 1341/400000: train loss: 0.420056
epoch:29, batch9, load frames use: 4.69301891327s
step 1342/400000: train loss: 0.401788
epoch:29, batch10, load frames use: 4.45769000053s
step 1343/400000: train loss: 0.428792
epoch:29, batch11, load frames use: 4.70682907104s
step 1344/400000: train loss: 0.372885
epoch:29, batch12, load frames use: 4.66767287254s
step 1345/400000: train loss: 0.434833
epoch:29, batch13, load frames use: 4.48339915276s
step 1346/400000: train loss: 0.391873
epoch:29, batch14, load frames use: 4.46602702141s
step 1347/400000: train loss: 0.456050
epoch:29, batch15, load frames use: 3.46501898766s
step 1348/400000: train loss: 0.490969
epoch:29, batch16, load frames use: 3.97431206703s
step 1349/400000: train loss: 0.461448
epoch:29, batch17, load frames use: 3.76500296593s
step 1350/400000: train loss: 0.534067
epoch:29, batch18, load frames use: 5.2649679184s
step 1351/400000: train loss: 0.441294
epoch:29, batch19, load frames use: 4.12468194962s
step 1352/400000: train loss: 0.513160
epoch:29, batch20, load frames use: 4.7514359951s
step 1353/400000: train loss: 0.476554
epoch:29, batch21, load frames use: 4.9100060463s
step 1354/400000: train loss: 0.457342
epoch:29, batch22, load frames use: 4.64768505096s
step 1355/400000: train loss: 0.471111
epoch:29, batch23, load frames use: 4.6181910038s
step 1356/400000: train loss: 0.431220
epoch:29, batch24, load frames use: 4.63821697235s
step 1357/400000: train loss: 0.437115
epoch:29, batch25, load frames use: 4.32961511612s
step 1358/400000: train loss: 0.481658
epoch:29, batch26, load frames use: 4.83114600182s
step 1359/400000: train loss: 0.448736
epoch:29, batch27, load frames use: 4.66461706161s
step 1360/400000: train loss: 0.414117
epoch:29, batch28, load frames use: 4.46227502823s
step 1361/400000: train loss: 0.393296
epoch:29, batch29, load frames use: 3.93522000313s
step 1362/400000: train loss: 0.389514
epoch:29, batch30, load frames use: 3.8024148941s
step 1363/400000: train loss: 0.489136
epoch:29, batch31, load frames use: 3.99752402306s
step 1364/400000: train loss: 0.457571
epoch:29, batch32, load frames use: 4.15142416954s
step 1365/400000: train loss: 0.444014
epoch:29, batch33, load frames use: 3.83935809135s
step 1366/400000: train loss: 0.509901
epoch:29, batch34, load frames use: 4.72397589684s
step 1367/400000: train loss: 0.491816
epoch:29, batch35, load frames use: 3.79346084595s
step 1368/400000: train loss: 0.424488
epoch:29, batch36, load frames use: 3.77050089836s
step 1369/400000: train loss: 0.502774
epoch:29, batch37, load frames use: 3.78657102585s
step 1370/400000: train loss: 0.474733
epoch:29, batch38, load frames use: 3.71888399124s
step 1371/400000: train loss: 0.413279
epoch:29, batch39, load frames use: 4.41381406784s
step 1372/400000: train loss: 0.428133
epoch:29, batch40, load frames use: 3.90806293488s
step 1373/400000: train loss: 0.389929
epoch:29, batch41, load frames use: 3.92470693588s
step 1374/400000: train loss: 0.410206
epoch:29, batch42, load frames use: 4.5327000618s
step 1375/400000: train loss: 0.397350
epoch:29, batch43, load frames use: 3.63293099403s
step 1376/400000: train loss: 0.464034
epoch:29, batch44, load frames use: 4.04038596153s
step 1377/400000: train loss: 0.389755
epoch:29, batch45, load frames use: 4.0110771656s
step 1378/400000: train loss: 0.528884
epoch:29, batch46, load frames use: 3.69145917892s
step 1379/400000: train loss: 0.378884
epoch:30, batch1, load frames use: 3.81951904297s
step 1380/400000: train loss: 0.453500
epoch:30, batch2, load frames use: 4.40252184868s
step 1381/400000: train loss: 0.392268
epoch:30, batch3, load frames use: 3.46096992493s
step 1382/400000: train loss: 0.383139
epoch:30, batch4, load frames use: 4.32460594177s
step 1383/400000: train loss: 0.361604
epoch:30, batch5, load frames use: 4.08079385757s
step 1384/400000: train loss: 0.418090
epoch:30, batch6, load frames use: 3.92060208321s
step 1385/400000: train loss: 0.419129
epoch:30, batch7, load frames use: 4.87201499939s
step 1386/400000: train loss: 0.399197
epoch:30, batch8, load frames use: 4.67811107635s
step 1387/400000: train loss: 0.390170
epoch:30, batch9, load frames use: 4.53341984749s
step 1388/400000: train loss: 0.380920
epoch:30, batch10, load frames use: 3.68065500259s
step 1389/400000: train loss: 0.417136
epoch:30, batch11, load frames use: 3.55820703506s
step 1390/400000: train loss: 0.419957
epoch:30, batch12, load frames use: 3.7458999157s
step 1391/400000: train loss: 0.429796
epoch:30, batch13, load frames use: 6.89909791946s
step 1392/400000: train loss: 0.433826
epoch:30, batch14, load frames use: 5.68761897087s
step 1393/400000: train loss: 0.393506
epoch:30, batch15, load frames use: 7.47467112541s
step 1394/400000: train loss: 0.478231
epoch:30, batch16, load frames use: 6.23859000206s
step 1395/400000: train loss: 0.426270
epoch:30, batch17, load frames use: 6.50983500481s
step 1396/400000: train loss: 0.401270
epoch:30, batch18, load frames use: 7.04520988464s
step 1397/400000: train loss: 0.431397
epoch:30, batch19, load frames use: 6.27614402771s
step 1398/400000: train loss: 0.391682
epoch:30, batch20, load frames use: 6.03563594818s
step 1399/400000: train loss: 0.442064
epoch:30, batch21, load frames use: 6.98296499252s
step 1400/400000: train loss: 0.469444
step 1400/400000: validation loss: 0.436679
epoch:30, batch22, load frames use: 7.86476302147s
step 1401/400000: train loss: 0.431287
epoch:30, batch23, load frames use: 4.69507217407s
step 1402/400000: train loss: 0.419871
epoch:30, batch24, load frames use: 6.80160903931s
step 1403/400000: train loss: 0.337216
epoch:30, batch25, load frames use: 6.93604707718s
step 1404/400000: train loss: 0.439017
epoch:30, batch26, load frames use: 5.94800305367s
step 1405/400000: train loss: 0.430261
epoch:30, batch27, load frames use: 4.86821198463s
step 1406/400000: train loss: 0.452993
epoch:30, batch28, load frames use: 7.48819494247s
step 1407/400000: train loss: 0.392994
epoch:30, batch29, load frames use: 7.08772706985s
step 1408/400000: train loss: 0.482623
epoch:30, batch30, load frames use: 7.66930198669s
step 1409/400000: train loss: 0.431675
epoch:30, batch31, load frames use: 6.55457615852s
step 1410/400000: train loss: 0.406287
epoch:30, batch32, load frames use: 7.14904093742s
step 1411/400000: train loss: 0.409114
epoch:30, batch33, load frames use: 6.87598586082s
step 1412/400000: train loss: 0.425779
epoch:30, batch34, load frames use: 6.55291199684s
step 1413/400000: train loss: 0.420964
epoch:30, batch35, load frames use: 7.32973194122s
step 1414/400000: train loss: 0.499919
epoch:30, batch36, load frames use: 7.13908696175s
step 1415/400000: train loss: 0.437407
epoch:30, batch37, load frames use: 7.01827716827s
step 1416/400000: train loss: 0.401061
epoch:30, batch38, load frames use: 6.06596493721s
step 1417/400000: train loss: 0.405770
epoch:30, batch39, load frames use: 6.07498002052s
step 1418/400000: train loss: 0.474979
epoch:30, batch40, load frames use: 7.36389517784s
step 1419/400000: train loss: 0.412787
epoch:30, batch41, load frames use: 6.64145207405s
step 1420/400000: train loss: 0.411316
epoch:30, batch42, load frames use: 7.60033583641s
step 1421/400000: train loss: 0.405809
epoch:30, batch43, load frames use: 5.32851481438s
step 1422/400000: train loss: 0.402713
epoch:30, batch44, load frames use: 6.35933208466s
step 1423/400000: train loss: 0.445896
epoch:30, batch45, load frames use: 5.80255103111s
step 1424/400000: train loss: 0.427547
epoch:30, batch46, load frames use: 7.51060199738s
step 1425/400000: train loss: 0.425849
epoch:31, batch1, load frames use: 7.38873291016s
step 1426/400000: train loss: 0.353368
epoch:31, batch2, load frames use: 7.4641931057s
step 1427/400000: train loss: 0.401579
epoch:31, batch3, load frames use: 6.27150917053s
step 1428/400000: train loss: 0.388359
epoch:31, batch4, load frames use: 6.30109000206s
step 1429/400000: train loss: 0.398940
epoch:31, batch5, load frames use: 6.52641105652s
step 1430/400000: train loss: 0.441225
epoch:31, batch6, load frames use: 5.2410068512s
step 1431/400000: train loss: 0.408853
epoch:31, batch7, load frames use: 4.73608613014s
step 1432/400000: train loss: 0.419513
epoch:31, batch8, load frames use: 6.61345887184s
step 1433/400000: train loss: 0.443718
epoch:31, batch9, load frames use: 6.4248790741s
step 1434/400000: train loss: 0.419285
epoch:31, batch10, load frames use: 6.00273394585s
step 1435/400000: train loss: 0.427979
epoch:31, batch11, load frames use: 6.13331007957s
step 1436/400000: train loss: 0.416237
epoch:31, batch12, load frames use: 7.06470608711s
step 1437/400000: train loss: 0.449126
epoch:31, batch13, load frames use: 6.91639399529s
step 1438/400000: train loss: 0.382196
epoch:31, batch14, load frames use: 5.75510001183s
step 1439/400000: train loss: 0.440127
epoch:31, batch15, load frames use: 5.39577078819s
step 1440/400000: train loss: 0.432381
epoch:31, batch16, load frames use: 6.6349709034s
step 1441/400000: train loss: 0.459567
epoch:31, batch17, load frames use: 6.69320702553s
step 1442/400000: train loss: 0.427146
epoch:31, batch18, load frames use: 5.90048885345s
step 1443/400000: train loss: 0.413800
epoch:31, batch19, load frames use: 6.3183221817s
step 1444/400000: train loss: 0.395309
epoch:31, batch20, load frames use: 6.88868689537s
step 1445/400000: train loss: 0.375577
epoch:31, batch21, load frames use: 7.14308595657s
step 1446/400000: train loss: 0.453367
epoch:31, batch22, load frames use: 6.0523660183s
step 1447/400000: train loss: 0.457156
epoch:31, batch23, load frames use: 6.35465192795s
step 1448/400000: train loss: 0.460703
epoch:31, batch24, load frames use: 6.7576110363s
step 1449/400000: train loss: 0.496427
epoch:31, batch25, load frames use: 6.69595003128s
step 1450/400000: train loss: 0.405833
epoch:31, batch26, load frames use: 7.13129091263s
step 1451/400000: train loss: 0.445509
epoch:31, batch27, load frames use: 5.4300661087s
step 1452/400000: train loss: 0.458117
epoch:31, batch28, load frames use: 5.4966480732s
step 1453/400000: train loss: 0.429353
epoch:31, batch29, load frames use: 4.78443193436s
step 1454/400000: train loss: 0.443790
epoch:31, batch30, load frames use: 7.97170495987s
step 1455/400000: train loss: 0.434430
epoch:31, batch31, load frames use: 6.67378401756s
step 1456/400000: train loss: 0.394860
epoch:31, batch32, load frames use: 4.88378405571s
step 1457/400000: train loss: 0.445282
epoch:31, batch33, load frames use: 6.253688097s
step 1458/400000: train loss: 0.415547
epoch:31, batch34, load frames use: 6.73427796364s
step 1459/400000: train loss: 0.430354
epoch:31, batch35, load frames use: 6.77965092659s
step 1460/400000: train loss: 0.381763
epoch:31, batch36, load frames use: 6.22794604301s
step 1461/400000: train loss: 0.449301
epoch:31, batch37, load frames use: 6.7663090229s
step 1462/400000: train loss: 0.474550
epoch:31, batch38, load frames use: 7.11397099495s
step 1463/400000: train loss: 0.394929
epoch:31, batch39, load frames use: 5.93431901932s
step 1464/400000: train loss: 0.396913
epoch:31, batch40, load frames use: 4.62255811691s
step 1465/400000: train loss: 0.441205
epoch:31, batch41, load frames use: 6.4588561058s
step 1466/400000: train loss: 0.411987
epoch:31, batch42, load frames use: 6.52108001709s
step 1467/400000: train loss: 0.546860
epoch:31, batch43, load frames use: 6.44624495506s
step 1468/400000: train loss: 0.438012
epoch:31, batch44, load frames use: 6.18220686913s
step 1469/400000: train loss: 0.459725
epoch:31, batch45, load frames use: 5.45087409019s
step 1470/400000: train loss: 0.446002
epoch:31, batch46, load frames use: 7.96731805801s
step 1471/400000: train loss: 0.465865
epoch:32, batch1, load frames use: 6.08315777779s
step 1472/400000: train loss: 0.355548
epoch:32, batch2, load frames use: 7.56255698204s
step 1473/400000: train loss: 0.444502
epoch:32, batch3, load frames use: 6.21297216415s
step 1474/400000: train loss: 0.374726
epoch:32, batch4, load frames use: 6.17109107971s
step 1475/400000: train loss: 0.420196
epoch:32, batch5, load frames use: 6.73727178574s
step 1476/400000: train loss: 0.470368
epoch:32, batch6, load frames use: 5.97868609428s
step 1477/400000: train loss: 0.400899
epoch:32, batch7, load frames use: 6.13396000862s
step 1478/400000: train loss: 0.393357
epoch:32, batch8, load frames use: 5.24796199799s
step 1479/400000: train loss: 0.447300
epoch:32, batch9, load frames use: 6.12105822563s
step 1480/400000: train loss: 0.397754
epoch:32, batch10, load frames use: 7.61682891846s
step 1481/400000: train loss: 0.395118
epoch:32, batch11, load frames use: 6.39012002945s
step 1482/400000: train loss: 0.415706
epoch:32, batch12, load frames use: 6.22510814667s
step 1483/400000: train loss: 0.443042
epoch:32, batch13, load frames use: 5.98342609406s
step 1484/400000: train loss: 0.319306
epoch:32, batch14, load frames use: 7.30943918228s
step 1485/400000: train loss: 0.438088
epoch:32, batch15, load frames use: 7.65592217445s
step 1486/400000: train loss: 0.414104
epoch:32, batch16, load frames use: 5.90025401115s
step 1487/400000: train loss: 0.477685
epoch:32, batch17, load frames use: 7.08508205414s
step 1488/400000: train loss: 0.469597
epoch:32, batch18, load frames use: 6.66888999939s
step 1489/400000: train loss: 0.480836
epoch:32, batch19, load frames use: 7.79634594917s
step 1490/400000: train loss: 0.442950
epoch:32, batch20, load frames use: 6.79102897644s
step 1491/400000: train loss: 0.401339
epoch:32, batch21, load frames use: 7.22404408455s
step 1492/400000: train loss: 0.398606
epoch:32, batch22, load frames use: 4.87245798111s
step 1493/400000: train loss: 0.457774
epoch:32, batch23, load frames use: 6.53171396255s
step 1494/400000: train loss: 0.413987
epoch:32, batch24, load frames use: 7.82433605194s
step 1495/400000: train loss: 0.401728
epoch:32, batch25, load frames use: 6.2356698513s
step 1496/400000: train loss: 0.464609
epoch:32, batch26, load frames use: 6.52246212959s
step 1497/400000: train loss: 0.390406
epoch:32, batch27, load frames use: 7.30224609375s
step 1498/400000: train loss: 0.427194
epoch:32, batch28, load frames use: 6.27348804474s
step 1499/400000: train loss: 0.533990
epoch:32, batch29, load frames use: 9.44300413132s
step 1500/400000: train loss: 0.362272
step 1500/400000: validation loss: 0.358003
epoch:32, batch30, load frames use: 6.32039499283s
step 1501/400000: train loss: 0.424883
epoch:32, batch31, load frames use: 6.28092598915s
step 1502/400000: train loss: 0.409172
epoch:32, batch32, load frames use: 6.48040795326s
step 1503/400000: train loss: 0.392772
epoch:32, batch33, load frames use: 8.0228509903s
step 1504/400000: train loss: 0.396663
epoch:32, batch34, load frames use: 7.36347198486s
step 1505/400000: train loss: 0.425184
epoch:32, batch35, load frames use: 6.4117629528s
step 1506/400000: train loss: 0.418980
epoch:32, batch36, load frames use: 7.05952501297s
step 1507/400000: train loss: 0.382266
epoch:32, batch37, load frames use: 6.63850092888s
step 1508/400000: train loss: 0.472517
epoch:32, batch38, load frames use: 4.93228507042s
step 1509/400000: train loss: 0.358295
epoch:32, batch39, load frames use: 5.72995114326s
step 1510/400000: train loss: 0.449252
epoch:32, batch40, load frames use: 7.6307759285s
step 1511/400000: train loss: 0.429885
epoch:32, batch41, load frames use: 6.34060192108s
step 1512/400000: train loss: 0.441133
epoch:32, batch42, load frames use: 6.16237807274s
step 1513/400000: train loss: 0.411904
epoch:32, batch43, load frames use: 7.49732112885s
step 1514/400000: train loss: 0.421494
epoch:32, batch44, load frames use: 9.21948599815s
step 1515/400000: train loss: 0.436135
epoch:32, batch45, load frames use: 6.46114206314s
step 1516/400000: train loss: 0.395287
epoch:32, batch46, load frames use: 5.77124214172s
step 1517/400000: train loss: 0.360099
epoch:33, batch1, load frames use: 4.75038194656s
step 1518/400000: train loss: 0.459542
epoch:33, batch2, load frames use: 7.53487801552s
step 1519/400000: train loss: 0.431630
epoch:33, batch3, load frames use: 8.02767491341s
step 1520/400000: train loss: 0.380048
epoch:33, batch4, load frames use: 5.9211127758s
step 1521/400000: train loss: 0.426619
epoch:33, batch5, load frames use: 6.89813995361s
step 1522/400000: train loss: 0.361750
epoch:33, batch6, load frames use: 7.86801409721s
step 1523/400000: train loss: 0.404199
epoch:33, batch7, load frames use: 6.55797290802s
step 1524/400000: train loss: 0.390791
epoch:33, batch8, load frames use: 5.3156042099s
step 1525/400000: train loss: 0.357146
epoch:33, batch9, load frames use: 6.61225008965s
step 1526/400000: train loss: 0.460515
epoch:33, batch10, load frames use: 6.36859989166s
step 1527/400000: train loss: 0.411077
epoch:33, batch11, load frames use: 5.34403204918s
step 1528/400000: train loss: 0.417941
epoch:33, batch12, load frames use: 8.3828971386s
step 1529/400000: train loss: 0.400220
epoch:33, batch13, load frames use: 5.77054405212s
step 1530/400000: train loss: 0.441597
epoch:33, batch14, load frames use: 6.52183794975s
step 1531/400000: train loss: 0.342938
epoch:33, batch15, load frames use: 6.17415189743s
step 1532/400000: train loss: 0.396794
epoch:33, batch16, load frames use: 6.39313697815s
step 1533/400000: train loss: 0.432441
epoch:33, batch17, load frames use: 7.39506316185s
step 1534/400000: train loss: 0.446344
epoch:33, batch18, load frames use: 5.28465890884s
step 1535/400000: train loss: 0.409165
epoch:33, batch19, load frames use: 4.89336800575s
step 1536/400000: train loss: 0.420706
epoch:33, batch20, load frames use: 5.85459899902s
step 1537/400000: train loss: 0.445479
epoch:33, batch21, load frames use: 6.41089987755s
step 1538/400000: train loss: 0.484576
epoch:33, batch22, load frames use: 4.69821596146s
step 1539/400000: train loss: 0.441104
epoch:33, batch23, load frames use: 6.19139289856s
step 1540/400000: train loss: 0.402536
epoch:33, batch24, load frames use: 6.16267299652s
step 1541/400000: train loss: 0.392057
epoch:33, batch25, load frames use: 5.09021711349s
step 1542/400000: train loss: 0.383334
epoch:33, batch26, load frames use: 6.18534493446s
step 1543/400000: train loss: 0.443732
epoch:33, batch27, load frames use: 6.52170991898s
step 1544/400000: train loss: 0.401052
epoch:33, batch28, load frames use: 7.29558801651s
step 1545/400000: train loss: 0.378640
epoch:33, batch29, load frames use: 5.03967094421s
step 1546/400000: train loss: 0.473079
epoch:33, batch30, load frames use: 6.91885900497s
step 1547/400000: train loss: 0.396905
epoch:33, batch31, load frames use: 7.81866002083s
step 1548/400000: train loss: 0.424302
epoch:33, batch32, load frames use: 4.81863307953s
step 1549/400000: train loss: 0.440463
epoch:33, batch33, load frames use: 5.01250195503s
step 1550/400000: train loss: 0.373558
epoch:33, batch34, load frames use: 4.542979002s
step 1551/400000: train loss: 0.420118
epoch:33, batch35, load frames use: 5.80369901657s
step 1552/400000: train loss: 0.402999
epoch:33, batch36, load frames use: 5.10546112061s
step 1553/400000: train loss: 0.439979
epoch:33, batch37, load frames use: 4.996062994s
step 1554/400000: train loss: 0.373128
epoch:33, batch38, load frames use: 5.17810201645s
step 1555/400000: train loss: 0.447250
epoch:33, batch39, load frames use: 7.09605979919s
step 1556/400000: train loss: 0.485750
epoch:33, batch40, load frames use: 5.58295416832s
step 1557/400000: train loss: 0.415934
epoch:33, batch41, load frames use: 6.88398098946s
step 1558/400000: train loss: 0.428210
epoch:33, batch42, load frames use: 6.13558197021s
step 1559/400000: train loss: 0.429522
epoch:33, batch43, load frames use: 4.71568179131s
step 1560/400000: train loss: 0.411306
epoch:33, batch44, load frames use: 6.31830310822s
step 1561/400000: train loss: 0.371417
epoch:33, batch45, load frames use: 6.20189404488s
step 1562/400000: train loss: 0.374940
epoch:33, batch46, load frames use: 4.74812698364s
step 1563/400000: train loss: 0.432186
epoch:34, batch1, load frames use: 5.49996399879s
step 1564/400000: train loss: 0.395606
epoch:34, batch2, load frames use: 5.85465192795s
step 1565/400000: train loss: 0.422107
epoch:34, batch3, load frames use: 5.23541092873s
step 1566/400000: train loss: 0.357009
epoch:34, batch4, load frames use: 5.54733085632s
step 1567/400000: train loss: 0.405870
epoch:34, batch5, load frames use: 6.50554895401s
step 1568/400000: train loss: 0.415030
epoch:34, batch6, load frames use: 6.48946499825s
step 1569/400000: train loss: 0.459353
epoch:34, batch7, load frames use: 5.15039300919s
step 1570/400000: train loss: 0.461068
epoch:34, batch8, load frames use: 5.41570401192s
step 1571/400000: train loss: 0.394286
epoch:34, batch9, load frames use: 5.32547307014s
step 1572/400000: train loss: 0.403267
epoch:34, batch10, load frames use: 5.30494403839s
step 1573/400000: train loss: 0.418538
epoch:34, batch11, load frames use: 6.1003279686s
step 1574/400000: train loss: 0.413209
epoch:34, batch12, load frames use: 5.2598028183s
step 1575/400000: train loss: 0.393467
epoch:34, batch13, load frames use: 5.48156094551s
step 1576/400000: train loss: 0.313334
epoch:34, batch14, load frames use: 6.43911600113s
step 1577/400000: train loss: 0.421802
epoch:34, batch15, load frames use: 6.07329702377s
step 1578/400000: train loss: 0.377897
epoch:34, batch16, load frames use: 6.16949892044s
step 1579/400000: train loss: 0.385682
epoch:34, batch17, load frames use: 5.34023499489s
step 1580/400000: train loss: 0.370321
epoch:34, batch18, load frames use: 5.17783689499s
step 1581/400000: train loss: 0.386761
epoch:34, batch19, load frames use: 5.39052391052s
step 1582/400000: train loss: 0.362060
epoch:34, batch20, load frames use: 6.7125351429s
step 1583/400000: train loss: 0.367710
epoch:34, batch21, load frames use: 5.30430793762s
step 1584/400000: train loss: 0.355300
epoch:34, batch22, load frames use: 6.2504761219s
step 1585/400000: train loss: 0.466208
epoch:34, batch23, load frames use: 5.8830640316s
step 1586/400000: train loss: 0.443511
epoch:34, batch24, load frames use: 4.56935596466s
step 1587/400000: train loss: 0.353722
epoch:34, batch25, load frames use: 5.89067411423s
step 1588/400000: train loss: 0.404741
epoch:34, batch26, load frames use: 3.40536403656s
step 1589/400000: train loss: 0.371184
epoch:34, batch27, load frames use: 3.77059292793s
step 1590/400000: train loss: 0.518110
epoch:34, batch28, load frames use: 3.58639287949s
step 1591/400000: train loss: 0.408164
epoch:34, batch29, load frames use: 3.64673709869s
step 1592/400000: train loss: 0.409436
epoch:34, batch30, load frames use: 4.16923809052s
step 1593/400000: train loss: 0.403827
epoch:34, batch31, load frames use: 3.90192604065s
step 1594/400000: train loss: 0.392438
epoch:34, batch32, load frames use: 4.54716396332s
step 1595/400000: train loss: 0.383083
epoch:34, batch33, load frames use: 3.95497989655s
step 1596/400000: train loss: 0.375412
epoch:34, batch34, load frames use: 4.70393204689s
step 1597/400000: train loss: 0.460694
epoch:34, batch35, load frames use: 4.654971838s
step 1598/400000: train loss: 0.370451
epoch:34, batch36, load frames use: 4.24794387817s
step 1599/400000: train loss: 0.372846
epoch:34, batch37, load frames use: 3.86243510246s
step 1600/400000: train loss: 0.377962
step 1600/400000: validation loss: 0.392614
epoch:34, batch38, load frames use: 3.95798206329s
step 1601/400000: train loss: 0.406766
epoch:34, batch39, load frames use: 5.08042621613s
step 1602/400000: train loss: 0.387880
epoch:34, batch40, load frames use: 4.05651378632s
step 1603/400000: train loss: 0.380669
epoch:34, batch41, load frames use: 4.02933382988s
step 1604/400000: train loss: 0.423612
epoch:34, batch42, load frames use: 4.91661500931s
step 1605/400000: train loss: 0.411035
epoch:34, batch43, load frames use: 4.05589413643s
step 1606/400000: train loss: 0.347981
epoch:34, batch44, load frames use: 3.48056101799s
step 1607/400000: train loss: 0.416865
epoch:34, batch45, load frames use: 3.53908395767s
step 1608/400000: train loss: 0.343884
epoch:34, batch46, load frames use: 3.51673102379s
step 1609/400000: train loss: 0.397189
epoch:35, batch1, load frames use: 4.48161816597s
step 1610/400000: train loss: 0.394818
epoch:35, batch2, load frames use: 3.36001706123s
step 1611/400000: train loss: 0.398113
epoch:35, batch3, load frames use: 3.4383251667s
step 1612/400000: train loss: 0.451976
epoch:35, batch4, load frames use: 4.25051116943s
step 1613/400000: train loss: 0.338095
epoch:35, batch5, load frames use: 4.73163795471s
step 1614/400000: train loss: 0.402825
epoch:35, batch6, load frames use: 5.49665808678s
step 1615/400000: train loss: 0.385920
epoch:35, batch7, load frames use: 3.58512401581s
step 1616/400000: train loss: 0.425115
epoch:35, batch8, load frames use: 4.1627471447s
step 1617/400000: train loss: 0.365035
epoch:35, batch9, load frames use: 3.88536000252s
step 1618/400000: train loss: 0.409659
epoch:35, batch10, load frames use: 3.5844039917s
step 1619/400000: train loss: 0.409823
epoch:35, batch11, load frames use: 3.58181595802s
step 1620/400000: train loss: 0.371684
epoch:35, batch12, load frames use: 4.24416804314s
step 1621/400000: train loss: 0.347799
epoch:35, batch13, load frames use: 3.76177287102s
step 1622/400000: train loss: 0.436396
epoch:35, batch14, load frames use: 3.40563488007s
step 1623/400000: train loss: 0.358406
epoch:35, batch15, load frames use: 3.30787491798s
step 1624/400000: train loss: 0.341065
epoch:35, batch16, load frames use: 3.65322589874s
step 1625/400000: train loss: 0.335309
epoch:35, batch17, load frames use: 5.25066685677s
step 1626/400000: train loss: 0.349434
epoch:35, batch18, load frames use: 4.30273985863s
step 1627/400000: train loss: 0.387995
epoch:35, batch19, load frames use: 4.06580305099s
step 1628/400000: train loss: 0.411432
epoch:35, batch20, load frames use: 4.64500999451s
step 1629/400000: train loss: 0.398324
epoch:35, batch21, load frames use: 3.44051814079s
step 1630/400000: train loss: 0.439654
epoch:35, batch22, load frames use: 3.89917206764s
step 1631/400000: train loss: 0.352616
epoch:35, batch23, load frames use: 3.28219294548s
step 1632/400000: train loss: 0.348380
epoch:35, batch24, load frames use: 3.44252490997s
step 1633/400000: train loss: 0.341202
epoch:35, batch25, load frames use: 3.35399603844s
step 1634/400000: train loss: 0.349252
epoch:35, batch26, load frames use: 4.14639115334s
step 1635/400000: train loss: 0.380424
epoch:35, batch27, load frames use: 4.60444998741s
step 1636/400000: train loss: 0.421997
epoch:35, batch28, load frames use: 4.70116996765s
step 1637/400000: train loss: 0.353833
epoch:35, batch29, load frames use: 3.81866812706s
step 1638/400000: train loss: 0.358242
epoch:35, batch30, load frames use: 3.67288279533s
step 1639/400000: train loss: 0.382992
epoch:35, batch31, load frames use: 3.35784506798s
step 1640/400000: train loss: 0.378475
epoch:35, batch32, load frames use: 3.37631511688s
step 1641/400000: train loss: 0.391365
epoch:35, batch33, load frames use: 3.51187205315s
step 1642/400000: train loss: 0.452514
epoch:35, batch34, load frames use: 3.44227600098s
step 1643/400000: train loss: 0.445315
epoch:35, batch35, load frames use: 3.30744194984s
step 1644/400000: train loss: 0.464886
epoch:35, batch36, load frames use: 3.46389603615s
step 1645/400000: train loss: 0.421243
epoch:35, batch37, load frames use: 3.7579100132s
step 1646/400000: train loss: 0.372380
epoch:35, batch38, load frames use: 3.97121095657s
step 1647/400000: train loss: 0.394124
epoch:35, batch39, load frames use: 3.37722086906s
step 1648/400000: train loss: 0.380215
epoch:35, batch40, load frames use: 3.67611694336s
step 1649/400000: train loss: 0.436483
epoch:35, batch41, load frames use: 3.4946680069s
step 1650/400000: train loss: 0.381185
epoch:35, batch42, load frames use: 3.51012086868s
step 1651/400000: train loss: 0.448055
epoch:35, batch43, load frames use: 3.41208410263s
step 1652/400000: train loss: 0.334277
epoch:35, batch44, load frames use: 3.69128203392s
step 1653/400000: train loss: 0.382671
epoch:35, batch45, load frames use: 3.56130504608s
step 1654/400000: train loss: 0.363872
epoch:35, batch46, load frames use: 3.34664297104s
step 1655/400000: train loss: 0.365923
epoch:36, batch1, load frames use: 3.49424600601s
step 1656/400000: train loss: 0.425207
epoch:36, batch2, load frames use: 3.75726890564s
step 1657/400000: train loss: 0.407053
epoch:36, batch3, load frames use: 4.69003009796s
step 1658/400000: train loss: 0.404345
epoch:36, batch4, load frames use: 3.34605789185s
step 1659/400000: train loss: 0.379863
epoch:36, batch5, load frames use: 3.5985929966s
step 1660/400000: train loss: 0.340985
epoch:36, batch6, load frames use: 3.7462849617s
step 1661/400000: train loss: 0.397364
epoch:36, batch7, load frames use: 3.29293513298s
step 1662/400000: train loss: 0.433123
epoch:36, batch8, load frames use: 3.85407710075s
step 1663/400000: train loss: 0.407726
epoch:36, batch9, load frames use: 4.09323382378s
step 1664/400000: train loss: 0.405228
epoch:36, batch10, load frames use: 3.96768307686s
step 1665/400000: train loss: 0.350739
epoch:36, batch11, load frames use: 4.66186285019s
step 1666/400000: train loss: 0.378607
epoch:36, batch12, load frames use: 4.43922686577s
step 1667/400000: train loss: 0.386361
epoch:36, batch13, load frames use: 4.65155315399s
step 1668/400000: train loss: 0.380913
epoch:36, batch14, load frames use: 4.03470087051s
step 1669/400000: train loss: 0.392839
epoch:36, batch15, load frames use: 3.7904150486s
step 1670/400000: train loss: 0.372146
epoch:36, batch16, load frames use: 3.58784890175s
step 1671/400000: train loss: 0.355904
epoch:36, batch17, load frames use: 4.2908539772s
step 1672/400000: train loss: 0.405026
epoch:36, batch18, load frames use: 3.41909098625s
step 1673/400000: train loss: 0.444721
epoch:36, batch19, load frames use: 3.50811004639s
step 1674/400000: train loss: 0.373391
epoch:36, batch20, load frames use: 3.4808909893s
step 1675/400000: train loss: 0.377053
epoch:36, batch21, load frames use: 4.88381910324s
step 1676/400000: train loss: 0.419355
epoch:36, batch22, load frames use: 4.74072980881s
step 1677/400000: train loss: 0.393004
epoch:36, batch23, load frames use: 4.04873204231s
step 1678/400000: train loss: 0.382690
epoch:36, batch24, load frames use: 3.82337808609s
step 1679/400000: train loss: 0.398628
epoch:36, batch25, load frames use: 3.85818004608s
step 1680/400000: train loss: 0.428152
epoch:36, batch26, load frames use: 3.63445615768s
step 1681/400000: train loss: 0.397860
epoch:36, batch27, load frames use: 3.66841411591s
step 1682/400000: train loss: 0.399662
epoch:36, batch28, load frames use: 4.37261605263s
step 1683/400000: train loss: 0.452029
epoch:36, batch29, load frames use: 4.63989806175s
step 1684/400000: train loss: 0.389592
epoch:36, batch30, load frames use: 3.52740597725s
step 1685/400000: train loss: 0.400257
epoch:36, batch31, load frames use: 3.46905708313s
step 1686/400000: train loss: 0.358897
epoch:36, batch32, load frames use: 3.69773483276s
step 1687/400000: train loss: 0.378112
epoch:36, batch33, load frames use: 5.24234294891s
step 1688/400000: train loss: 0.406637
epoch:36, batch34, load frames use: 4.98123311996s
step 1689/400000: train loss: 0.374609
epoch:36, batch35, load frames use: 3.89681982994s
step 1690/400000: train loss: 0.368766
epoch:36, batch36, load frames use: 4.2716588974s
step 1691/400000: train loss: 0.381347
epoch:36, batch37, load frames use: 3.4113779068s
step 1692/400000: train loss: 0.365884
epoch:36, batch38, load frames use: 3.6880030632s
step 1693/400000: train loss: 0.376423
epoch:36, batch39, load frames use: 4.12428092957s
step 1694/400000: train loss: 0.410319
epoch:36, batch40, load frames use: 4.52216887474s
step 1695/400000: train loss: 0.329341
epoch:36, batch41, load frames use: 4.73594808578s
step 1696/400000: train loss: 0.441997
epoch:36, batch42, load frames use: 4.8683860302s
step 1697/400000: train loss: 0.364775
epoch:36, batch43, load frames use: 4.58611106873s
step 1698/400000: train loss: 0.356302
epoch:36, batch44, load frames use: 5.11265707016s
step 1699/400000: train loss: 0.362053
epoch:36, batch45, load frames use: 3.82408690453s
step 1700/400000: train loss: 0.323689
step 1700/400000: validation loss: 0.323327
epoch:36, batch46, load frames use: 4.21086907387s
step 1701/400000: train loss: 0.353482
epoch:37, batch1, load frames use: 3.43038082123s
step 1702/400000: train loss: 0.385915
epoch:37, batch2, load frames use: 3.7384250164s
step 1703/400000: train loss: 0.398901
epoch:37, batch3, load frames use: 3.91426301003s
step 1704/400000: train loss: 0.457971
epoch:37, batch4, load frames use: 5.40610909462s
step 1705/400000: train loss: 0.414463
epoch:37, batch5, load frames use: 3.8672349453s
step 1706/400000: train loss: 0.399320
epoch:37, batch6, load frames use: 4.23830699921s
step 1707/400000: train loss: 0.360003
epoch:37, batch7, load frames use: 3.54499197006s
step 1708/400000: train loss: 0.351321
epoch:37, batch8, load frames use: 3.88098692894s
step 1709/400000: train loss: 0.352145
epoch:37, batch9, load frames use: 3.76344704628s
step 1710/400000: train loss: 0.373067
epoch:37, batch10, load frames use: 4.56507396698s
step 1711/400000: train loss: 0.346437
epoch:37, batch11, load frames use: 4.99132394791s
step 1712/400000: train loss: 0.405172
epoch:37, batch12, load frames use: 4.53221392632s
step 1713/400000: train loss: 0.467755
epoch:37, batch13, load frames use: 4.27003383636s
step 1714/400000: train loss: 0.395444
epoch:37, batch14, load frames use: 4.38724517822s
step 1715/400000: train loss: 0.400182
epoch:37, batch15, load frames use: 3.51585006714s
step 1716/400000: train loss: 0.368785
epoch:37, batch16, load frames use: 3.507117033s
step 1717/400000: train loss: 0.298419
epoch:37, batch17, load frames use: 3.56410384178s
step 1718/400000: train loss: 0.402302
epoch:37, batch18, load frames use: 4.56084799767s
step 1719/400000: train loss: 0.375258
epoch:37, batch19, load frames use: 4.04733896255s
step 1720/400000: train loss: 0.340638
epoch:37, batch20, load frames use: 4.26343297958s
step 1721/400000: train loss: 0.397259
epoch:37, batch21, load frames use: 3.6012468338s
step 1722/400000: train loss: 0.374277
epoch:37, batch22, load frames use: 3.79098391533s
step 1723/400000: train loss: 0.362966
epoch:37, batch23, load frames use: 7.46890997887s
step 1724/400000: train loss: 0.433832
epoch:37, batch24, load frames use: 4.57127094269s
step 1725/400000: train loss: 0.338030
epoch:37, batch25, load frames use: 5.69123101234s
step 1726/400000: train loss: 0.369517
epoch:37, batch26, load frames use: 6.83519887924s
step 1727/400000: train loss: 0.363917
epoch:37, batch27, load frames use: 6.77762818336s
step 1728/400000: train loss: 0.411605
epoch:37, batch28, load frames use: 5.56169605255s
step 1729/400000: train loss: 0.381299
epoch:37, batch29, load frames use: 6.67240095139s
step 1730/400000: train loss: 0.349912
epoch:37, batch30, load frames use: 4.75254297256s
step 1731/400000: train loss: 0.328370
epoch:37, batch31, load frames use: 6.30170202255s
step 1732/400000: train loss: 0.421994
epoch:37, batch32, load frames use: 4.90242195129s
step 1733/400000: train loss: 0.366753
epoch:37, batch33, load frames use: 7.1594889164s
step 1734/400000: train loss: 0.323422
epoch:37, batch34, load frames use: 5.82223200798s
step 1735/400000: train loss: 0.356077
epoch:37, batch35, load frames use: 6.72441792488s
step 1736/400000: train loss: 0.403388
epoch:37, batch36, load frames use: 6.68131518364s
step 1737/400000: train loss: 0.393931
epoch:37, batch37, load frames use: 7.32656693459s
step 1738/400000: train loss: 0.411661
epoch:37, batch38, load frames use: 6.27923488617s
step 1739/400000: train loss: 0.396107
epoch:37, batch39, load frames use: 7.26122903824s
step 1740/400000: train loss: 0.403250
epoch:37, batch40, load frames use: 6.60463190079s
step 1741/400000: train loss: 0.430052
epoch:37, batch41, load frames use: 4.04183697701s
step 1742/400000: train loss: 0.430597
epoch:37, batch42, load frames use: 4.98758792877s
step 1743/400000: train loss: 0.478100
epoch:37, batch43, load frames use: 6.40325307846s
step 1744/400000: train loss: 0.423898
epoch:37, batch44, load frames use: 3.89178609848s
step 1745/400000: train loss: 0.405657
epoch:37, batch45, load frames use: 6.13099789619s
step 1746/400000: train loss: 0.353021
epoch:37, batch46, load frames use: 6.09343886375s
step 1747/400000: train loss: 0.384319
epoch:38, batch1, load frames use: 7.33115100861s
step 1748/400000: train loss: 0.372699
epoch:38, batch2, load frames use: 6.56215810776s
step 1749/400000: train loss: 0.370506
epoch:38, batch3, load frames use: 8.39272212982s
step 1750/400000: train loss: 0.381623
epoch:38, batch4, load frames use: 5.24776411057s
step 1751/400000: train loss: 0.335208
epoch:38, batch5, load frames use: 6.33830094337s
step 1752/400000: train loss: 0.400675
epoch:38, batch6, load frames use: 4.88307285309s
step 1753/400000: train loss: 0.338296
epoch:38, batch7, load frames use: 5.58582401276s
step 1754/400000: train loss: 0.372260
epoch:38, batch8, load frames use: 5.71958684921s
step 1755/400000: train loss: 0.398486
epoch:38, batch9, load frames use: 6.39991807938s
step 1756/400000: train loss: 0.376830
epoch:38, batch10, load frames use: 6.36797809601s
step 1757/400000: train loss: 0.381034
epoch:38, batch11, load frames use: 6.26637792587s
step 1758/400000: train loss: 0.352811
epoch:38, batch12, load frames use: 6.67469501495s
step 1759/400000: train loss: 0.378995
epoch:38, batch13, load frames use: 6.95429420471s
step 1760/400000: train loss: 0.404463
epoch:38, batch14, load frames use: 7.5320250988s
step 1761/400000: train loss: 0.323156
epoch:38, batch15, load frames use: 6.42515921593s
step 1762/400000: train loss: 0.405095
epoch:38, batch16, load frames use: 7.52746915817s
step 1763/400000: train loss: 0.366266
epoch:38, batch17, load frames use: 6.60559916496s
step 1764/400000: train loss: 0.354896
epoch:38, batch18, load frames use: 7.54869198799s
step 1765/400000: train loss: 0.315063
epoch:38, batch19, load frames use: 5.89830994606s
step 1766/400000: train loss: 0.346894
epoch:38, batch20, load frames use: 5.83559298515s
step 1767/400000: train loss: 0.396614
epoch:38, batch21, load frames use: 6.02506279945s
step 1768/400000: train loss: 0.345068
epoch:38, batch22, load frames use: 5.08662605286s
step 1769/400000: train loss: 0.343685
epoch:38, batch23, load frames use: 5.60531997681s
step 1770/400000: train loss: 0.350134
epoch:38, batch24, load frames use: 6.55344605446s
step 1771/400000: train loss: 0.373863
epoch:38, batch25, load frames use: 3.50140881538s
step 1772/400000: train loss: 0.328706
epoch:38, batch26, load frames use: 7.25625920296s
step 1773/400000: train loss: 0.411509
epoch:38, batch27, load frames use: 5.28653120995s
step 1774/400000: train loss: 0.365769
epoch:38, batch28, load frames use: 5.1373360157s
step 1775/400000: train loss: 0.363828
epoch:38, batch29, load frames use: 8.78856897354s
step 1776/400000: train loss: 0.387903
epoch:38, batch30, load frames use: 7.04704213142s
step 1777/400000: train loss: 0.383460
epoch:38, batch31, load frames use: 5.97283101082s
step 1778/400000: train loss: 0.426648
epoch:38, batch32, load frames use: 6.33653616905s
step 1779/400000: train loss: 0.420819
epoch:38, batch33, load frames use: 6.09341907501s
step 1780/400000: train loss: 0.353563
epoch:38, batch34, load frames use: 5.42780900002s
step 1781/400000: train loss: 0.347855
epoch:38, batch35, load frames use: 6.97404909134s
step 1782/400000: train loss: 0.365252
epoch:38, batch36, load frames use: 7.02301502228s
step 1783/400000: train loss: 0.366779
epoch:38, batch37, load frames use: 6.53019404411s
step 1784/400000: train loss: 0.392827
epoch:38, batch38, load frames use: 6.43311595917s
step 1785/400000: train loss: 0.384673
epoch:38, batch39, load frames use: 6.97267413139s
step 1786/400000: train loss: 0.403863
epoch:38, batch40, load frames use: 7.27614212036s
step 1787/400000: train loss: 0.449378
epoch:38, batch41, load frames use: 5.46917104721s
step 1788/400000: train loss: 0.415478
epoch:38, batch42, load frames use: 7.2592048645s
step 1789/400000: train loss: 0.436590
epoch:38, batch43, load frames use: 7.20970010757s
step 1790/400000: train loss: 0.344140
epoch:38, batch44, load frames use: 7.29854106903s
step 1791/400000: train loss: 0.373858
epoch:38, batch45, load frames use: 4.26051902771s
step 1792/400000: train loss: 0.467118
epoch:38, batch46, load frames use: 7.91770601273s
step 1793/400000: train loss: 0.420176
epoch:39, batch1, load frames use: 4.56311583519s
step 1794/400000: train loss: 0.360237
epoch:39, batch2, load frames use: 7.38825511932s
step 1795/400000: train loss: 0.451119
epoch:39, batch3, load frames use: 6.50747704506s
step 1796/400000: train loss: 0.376621
epoch:39, batch4, load frames use: 8.82372808456s
step 1797/400000: train loss: 0.351557
epoch:39, batch5, load frames use: 5.90086603165s
step 1798/400000: train loss: 0.447291
epoch:39, batch6, load frames use: 5.84517908096s
step 1799/400000: train loss: 0.366498
epoch:39, batch7, load frames use: 6.60723781586s
step 1800/400000: train loss: 0.400358
step 1800/400000: validation loss: 0.390873
epoch:39, batch8, load frames use: 6.81939601898s
step 1801/400000: train loss: 0.441215
epoch:39, batch9, load frames use: 4.60675501823s
step 1802/400000: train loss: 0.327368
epoch:39, batch10, load frames use: 7.60288715363s
step 1803/400000: train loss: 0.338416
epoch:39, batch11, load frames use: 5.57213711739s
step 1804/400000: train loss: 0.435418
epoch:39, batch12, load frames use: 8.92616415024s
step 1805/400000: train loss: 0.322960
epoch:39, batch13, load frames use: 7.13582992554s
step 1806/400000: train loss: 0.334189
epoch:39, batch14, load frames use: 9.74776411057s
step 1807/400000: train loss: 0.395932
epoch:39, batch15, load frames use: 7.80657815933s
step 1808/400000: train loss: 0.358029
epoch:39, batch16, load frames use: 8.07190799713s
step 1809/400000: train loss: 0.388183
epoch:39, batch17, load frames use: 6.11868977547s
step 1810/400000: train loss: 0.368537
epoch:39, batch18, load frames use: 6.87934803963s
step 1811/400000: train loss: 0.399015
epoch:39, batch19, load frames use: 7.00373005867s
step 1812/400000: train loss: 0.344069
epoch:39, batch20, load frames use: 6.80255198479s
step 1813/400000: train loss: 0.354205
epoch:39, batch21, load frames use: 4.30492281914s
step 1814/400000: train loss: 0.390628
epoch:39, batch22, load frames use: 6.55990314484s
step 1815/400000: train loss: 0.434691
epoch:39, batch23, load frames use: 6.34865307808s
step 1816/400000: train loss: 0.382438
epoch:39, batch24, load frames use: 5.59032607079s
step 1817/400000: train loss: 0.406545
epoch:39, batch25, load frames use: 5.90998601913s
step 1818/400000: train loss: 0.433325
epoch:39, batch26, load frames use: 7.47021389008s
step 1819/400000: train loss: 0.398868
epoch:39, batch27, load frames use: 5.7902109623s
step 1820/400000: train loss: 0.314724
epoch:39, batch28, load frames use: 6.20626711845s
step 1821/400000: train loss: 0.412421
epoch:39, batch29, load frames use: 5.34111905098s
step 1822/400000: train loss: 0.365650
epoch:39, batch30, load frames use: 6.406042099s
step 1823/400000: train loss: 0.365499
epoch:39, batch31, load frames use: 7.77628397942s
step 1824/400000: train loss: 0.387571
epoch:39, batch32, load frames use: 6.93069720268s
step 1825/400000: train loss: 0.410075
epoch:39, batch33, load frames use: 6.75146389008s
step 1826/400000: train loss: 0.352223
epoch:39, batch34, load frames use: 6.06365704536s
step 1827/400000: train loss: 0.357942
epoch:39, batch35, load frames use: 5.33933711052s
step 1828/400000: train loss: 0.363722
epoch:39, batch36, load frames use: 5.42034196854s
step 1829/400000: train loss: 0.426402
epoch:39, batch37, load frames use: 7.69475507736s
step 1830/400000: train loss: 0.389782
epoch:39, batch38, load frames use: 7.32524204254s
step 1831/400000: train loss: 0.344220
epoch:39, batch39, load frames use: 7.33179306984s
step 1832/400000: train loss: 0.318404
epoch:39, batch40, load frames use: 7.15309906006s
step 1833/400000: train loss: 0.329338
epoch:39, batch41, load frames use: 5.7138440609s
step 1834/400000: train loss: 0.390828
epoch:39, batch42, load frames use: 6.07952809334s
step 1835/400000: train loss: 0.306657
epoch:39, batch43, load frames use: 5.62797284126s
step 1836/400000: train loss: 0.508225
epoch:39, batch44, load frames use: 6.68729400635s
step 1837/400000: train loss: 0.389901
epoch:39, batch45, load frames use: 6.17420101166s
step 1838/400000: train loss: 0.410985
epoch:39, batch46, load frames use: 4.8064661026s
step 1839/400000: train loss: 0.390497
epoch:40, batch1, load frames use: 6.37488985062s
step 1840/400000: train loss: 0.357591
epoch:40, batch2, load frames use: 5.37475705147s
step 1841/400000: train loss: 0.346081
epoch:40, batch3, load frames use: 7.1013109684s
step 1842/400000: train loss: 0.383754
epoch:40, batch4, load frames use: 6.21884703636s
step 1843/400000: train loss: 0.356644
epoch:40, batch5, load frames use: 5.90385580063s
step 1844/400000: train loss: 0.363455
epoch:40, batch6, load frames use: 6.5317299366s
step 1845/400000: train loss: 0.402962
epoch:40, batch7, load frames use: 6.6486530304s
step 1846/400000: train loss: 0.360661
epoch:40, batch8, load frames use: 6.80859303474s
step 1847/400000: train loss: 0.362416
epoch:40, batch9, load frames use: 7.0264248848s
step 1848/400000: train loss: 0.349712
epoch:40, batch10, load frames use: 6.7805929184s
step 1849/400000: train loss: 0.334570
epoch:40, batch11, load frames use: 7.22327303886s
step 1850/400000: train loss: 0.334768
epoch:40, batch12, load frames use: 7.51151585579s
step 1851/400000: train loss: 0.387528
epoch:40, batch13, load frames use: 5.77990317345s
step 1852/400000: train loss: 0.414727
epoch:40, batch14, load frames use: 7.64177298546s
step 1853/400000: train loss: 0.301851
epoch:40, batch15, load frames use: 7.83547282219s
step 1854/400000: train loss: 0.387488
epoch:40, batch16, load frames use: 6.8663380146s
step 1855/400000: train loss: 0.428874
epoch:40, batch17, load frames use: 6.62707901001s
step 1856/400000: train loss: 0.324100
epoch:40, batch18, load frames use: 7.68313503265s
step 1857/400000: train loss: 0.369386
epoch:40, batch19, load frames use: 6.84826588631s
step 1858/400000: train loss: 0.359317
epoch:40, batch20, load frames use: 6.5525431633s
step 1859/400000: train loss: 0.387361
epoch:40, batch21, load frames use: 5.62431502342s
step 1860/400000: train loss: 0.393117
epoch:40, batch22, load frames use: 7.27663302422s
step 1861/400000: train loss: 0.375426
epoch:40, batch23, load frames use: 6.43227314949s
step 1862/400000: train loss: 0.369491
epoch:40, batch24, load frames use: 6.98758792877s
step 1863/400000: train loss: 0.399991
epoch:40, batch25, load frames use: 5.00951194763s
step 1864/400000: train loss: 0.356259
epoch:40, batch26, load frames use: 5.94361019135s
step 1865/400000: train loss: 0.343354
epoch:40, batch27, load frames use: 6.05752110481s
step 1866/400000: train loss: 0.303695
epoch:40, batch28, load frames use: 6.85843992233s
step 1867/400000: train loss: 0.334494
epoch:40, batch29, load frames use: 6.24440789223s
step 1868/400000: train loss: 0.410791
epoch:40, batch30, load frames use: 6.09934210777s
step 1869/400000: train loss: 0.366868
epoch:40, batch31, load frames use: 6.92973589897s
step 1870/400000: train loss: 0.384603
epoch:40, batch32, load frames use: 5.1194961071s
step 1871/400000: train loss: 0.352911
epoch:40, batch33, load frames use: 6.8958439827s
step 1872/400000: train loss: 0.336495
epoch:40, batch34, load frames use: 6.49773311615s
step 1873/400000: train loss: 0.415941
epoch:40, batch35, load frames use: 5.92791485786s
step 1874/400000: train loss: 0.383396
epoch:40, batch36, load frames use: 5.9542798996s
step 1875/400000: train loss: 0.382742
epoch:40, batch37, load frames use: 5.77216601372s
step 1876/400000: train loss: 0.361521
epoch:40, batch38, load frames use: 5.44022488594s
step 1877/400000: train loss: 0.315110
epoch:40, batch39, load frames use: 6.08545994759s
step 1878/400000: train loss: 0.394800
epoch:40, batch40, load frames use: 6.4143550396s
step 1879/400000: train loss: 0.307535
epoch:40, batch41, load frames use: 6.74173593521s
step 1880/400000: train loss: 0.400489
epoch:40, batch42, load frames use: 6.49674296379s
step 1881/400000: train loss: 0.392757
epoch:40, batch43, load frames use: 6.96129488945s
step 1882/400000: train loss: 0.376198
epoch:40, batch44, load frames use: 5.79252195358s
step 1883/400000: train loss: 0.343813
epoch:40, batch45, load frames use: 6.97343492508s
step 1884/400000: train loss: 0.383947
epoch:40, batch46, load frames use: 7.21009397507s
step 1885/400000: train loss: 0.362907
epoch:41, batch1, load frames use: 6.63600277901s
step 1886/400000: train loss: 0.375421
epoch:41, batch2, load frames use: 7.97857904434s
step 1887/400000: train loss: 0.359175
epoch:41, batch3, load frames use: 6.18646407127s
step 1888/400000: train loss: 0.363808
epoch:41, batch4, load frames use: 5.43491005898s
step 1889/400000: train loss: 0.354188
epoch:41, batch5, load frames use: 6.37409496307s
step 1890/400000: train loss: 0.358249
epoch:41, batch6, load frames use: 6.02286195755s
step 1891/400000: train loss: 0.352653
epoch:41, batch7, load frames use: 5.84529399872s
step 1892/400000: train loss: 0.310877
epoch:41, batch8, load frames use: 6.91168904305s
step 1893/400000: train loss: 0.307626
epoch:41, batch9, load frames use: 4.41126990318s
step 1894/400000: train loss: 0.376206
epoch:41, batch10, load frames use: 6.07298111916s
step 1895/400000: train loss: 0.377465
epoch:41, batch11, load frames use: 5.91971707344s
step 1896/400000: train loss: 0.367218
epoch:41, batch12, load frames use: 6.99107718468s
step 1897/400000: train loss: 0.325538
epoch:41, batch13, load frames use: 4.45040297508s
step 1898/400000: train loss: 0.356304
epoch:41, batch14, load frames use: 7.19496011734s
step 1899/400000: train loss: 0.363040
epoch:41, batch15, load frames use: 6.84662818909s
step 1900/400000: train loss: 0.325930
step 1900/400000: validation loss: 0.344325
epoch:41, batch16, load frames use: 6.44458794594s
step 1901/400000: train loss: 0.311132
epoch:41, batch17, load frames use: 7.28186297417s
step 1902/400000: train loss: 0.389897
epoch:41, batch18, load frames use: 6.50950288773s
step 1903/400000: train loss: 0.384117
epoch:41, batch19, load frames use: 7.37407398224s
step 1904/400000: train loss: 0.296266
epoch:41, batch20, load frames use: 6.87665891647s
step 1905/400000: train loss: 0.359913
epoch:41, batch21, load frames use: 7.33871388435s
step 1906/400000: train loss: 0.351985
epoch:41, batch22, load frames use: 6.05211901665s
step 1907/400000: train loss: 0.323615
epoch:41, batch23, load frames use: 3.61929702759s
step 1908/400000: train loss: 0.391034
epoch:41, batch24, load frames use: 6.32049107552s
step 1909/400000: train loss: 0.407020
epoch:41, batch25, load frames use: 7.08186101913s
step 1910/400000: train loss: 0.346231
epoch:41, batch26, load frames use: 5.5160908699s
step 1911/400000: train loss: 0.291373
epoch:41, batch27, load frames use: 6.22666692734s
step 1912/400000: train loss: 0.387783
epoch:41, batch28, load frames use: 6.1863219738s
step 1913/400000: train loss: 0.360059
epoch:41, batch29, load frames use: 6.33470988274s
step 1914/400000: train loss: 0.343702
epoch:41, batch30, load frames use: 7.2395670414s
step 1915/400000: train loss: 0.405441
epoch:41, batch31, load frames use: 6.98854589462s
step 1916/400000: train loss: 0.363279
epoch:41, batch32, load frames use: 10.7952599525s
step 1917/400000: train loss: 0.383003
epoch:41, batch33, load frames use: 6.63634896278s
step 1918/400000: train loss: 0.361264
epoch:41, batch34, load frames use: 7.22361707687s
step 1919/400000: train loss: 0.369137
epoch:41, batch35, load frames use: 6.53689408302s
step 1920/400000: train loss: 0.347430
epoch:41, batch36, load frames use: 6.60336709023s
step 1921/400000: train loss: 0.350291
epoch:41, batch37, load frames use: 5.571393013s
step 1922/400000: train loss: 0.333711
epoch:41, batch38, load frames use: 6.54591703415s
step 1923/400000: train loss: 0.336917
epoch:41, batch39, load frames use: 7.68367600441s
step 1924/400000: train loss: 0.361671
epoch:41, batch40, load frames use: 5.97256207466s
step 1925/400000: train loss: 0.311318
epoch:41, batch41, load frames use: 6.31919598579s
step 1926/400000: train loss: 0.348802
epoch:41, batch42, load frames use: 7.48508191109s
step 1927/400000: train loss: 0.319132
epoch:41, batch43, load frames use: 7.41842794418s
step 1928/400000: train loss: 0.324191
epoch:41, batch44, load frames use: 6.89845490456s
step 1929/400000: train loss: 0.383827
epoch:41, batch45, load frames use: 6.78667497635s
step 1930/400000: train loss: 0.367468
epoch:41, batch46, load frames use: 7.1408469677s
step 1931/400000: train loss: 0.328728
epoch:42, batch1, load frames use: 5.36722397804s
step 1932/400000: train loss: 0.430661
epoch:42, batch2, load frames use: 5.93758010864s
step 1933/400000: train loss: 0.355341
epoch:42, batch3, load frames use: 6.59621500969s
step 1934/400000: train loss: 0.325829
epoch:42, batch4, load frames use: 6.76138901711s
step 1935/400000: train loss: 0.357692
epoch:42, batch5, load frames use: 6.10365104675s
step 1936/400000: train loss: 0.355863
epoch:42, batch6, load frames use: 8.33462905884s
step 1937/400000: train loss: 0.303909
epoch:42, batch7, load frames use: 7.37613987923s
step 1938/400000: train loss: 0.377689
epoch:42, batch8, load frames use: 6.30855178833s
step 1939/400000: train loss: 0.348942
epoch:42, batch9, load frames use: 5.91557598114s
step 1940/400000: train loss: 0.310345
epoch:42, batch10, load frames use: 6.03860998154s
step 1941/400000: train loss: 0.311170
epoch:42, batch11, load frames use: 6.54173612595s
step 1942/400000: train loss: 0.344787
epoch:42, batch12, load frames use: 6.1539349556s
step 1943/400000: train loss: 0.302181
epoch:42, batch13, load frames use: 7.45707201958s
step 1944/400000: train loss: 0.347019
epoch:42, batch14, load frames use: 6.93533778191s
step 1945/400000: train loss: 0.360850
epoch:42, batch15, load frames use: 6.60572600365s
step 1946/400000: train loss: 0.351524
epoch:42, batch16, load frames use: 7.17590212822s
step 1947/400000: train loss: 0.359280
epoch:42, batch17, load frames use: 6.08034586906s
step 1948/400000: train loss: 0.349000
epoch:42, batch18, load frames use: 4.49484109879s
step 1949/400000: train loss: 0.333093
epoch:42, batch19, load frames use: 7.70129513741s
step 1950/400000: train loss: 0.299086
epoch:42, batch20, load frames use: 5.86491084099s
step 1951/400000: train loss: 0.326251
epoch:42, batch21, load frames use: 8.10178494453s
step 1952/400000: train loss: 0.310546
epoch:42, batch22, load frames use: 6.61341714859s
step 1953/400000: train loss: 0.315787
epoch:42, batch23, load frames use: 7.04257202148s
step 1954/400000: train loss: 0.413085
epoch:42, batch24, load frames use: 6.04920196533s
step 1955/400000: train loss: 0.315966
epoch:42, batch25, load frames use: 6.54723715782s
step 1956/400000: train loss: 0.359731
epoch:42, batch26, load frames use: 5.4292318821s
step 1957/400000: train loss: 0.401872
epoch:42, batch27, load frames use: 8.27218103409s
step 1958/400000: train loss: 0.335359
epoch:42, batch28, load frames use: 4.86687588692s
step 1959/400000: train loss: 0.379395
epoch:42, batch29, load frames use: 5.7653670311s
step 1960/400000: train loss: 0.326400
epoch:42, batch30, load frames use: 6.23346281052s
step 1961/400000: train loss: 0.344658
epoch:42, batch31, load frames use: 4.30979704857s
step 1962/400000: train loss: 0.345944
epoch:42, batch32, load frames use: 5.44869208336s
step 1963/400000: train loss: 0.366768
epoch:42, batch33, load frames use: 5.87590813637s
step 1964/400000: train loss: 0.305787
epoch:42, batch34, load frames use: 6.65372991562s
step 1965/400000: train loss: 0.339175
epoch:42, batch35, load frames use: 4.95818400383s
step 1966/400000: train loss: 0.316620
epoch:42, batch36, load frames use: 5.82129096985s
step 1967/400000: train loss: 0.338278
epoch:42, batch37, load frames use: 7.05339407921s
step 1968/400000: train loss: 0.354782
epoch:42, batch38, load frames use: 5.2857029438s
step 1969/400000: train loss: 0.332031
epoch:42, batch39, load frames use: 5.78880214691s
step 1970/400000: train loss: 0.326007
epoch:42, batch40, load frames use: 6.58009004593s
step 1971/400000: train loss: 0.352530
epoch:42, batch41, load frames use: 6.07071995735s
step 1972/400000: train loss: 0.345237
epoch:42, batch42, load frames use: 5.85720086098s
step 1973/400000: train loss: 0.322932
epoch:42, batch43, load frames use: 5.76398706436s
step 1974/400000: train loss: 0.370264
epoch:42, batch44, load frames use: 5.86686396599s
step 1975/400000: train loss: 0.351106
epoch:42, batch45, load frames use: 5.69990587234s
step 1976/400000: train loss: 0.343454
epoch:42, batch46, load frames use: 6.13312697411s
step 1977/400000: train loss: 0.345435
epoch:43, batch1, load frames use: 6.06990504265s
step 1978/400000: train loss: 0.378791
epoch:43, batch2, load frames use: 6.80372095108s
step 1979/400000: train loss: 0.333349
epoch:43, batch3, load frames use: 6.19307208061s
step 1980/400000: train loss: 0.307520
epoch:43, batch4, load frames use: 7.11693501472s
step 1981/400000: train loss: 0.318365
epoch:43, batch5, load frames use: 6.4852809906s
step 1982/400000: train loss: 0.323858
epoch:43, batch6, load frames use: 5.01423311234s
step 1983/400000: train loss: 0.367326
epoch:43, batch7, load frames use: 6.15623307228s
step 1984/400000: train loss: 0.300594
epoch:43, batch8, load frames use: 5.90559887886s
step 1985/400000: train loss: 0.343575
epoch:43, batch9, load frames use: 6.59334087372s
step 1986/400000: train loss: 0.343418
epoch:43, batch10, load frames use: 5.26837086678s
step 1987/400000: train loss: 0.322531
epoch:43, batch11, load frames use: 5.16428494453s
step 1988/400000: train loss: 0.327555
epoch:43, batch12, load frames use: 7.55446004868s
step 1989/400000: train loss: 0.363141
epoch:43, batch13, load frames use: 6.84632992744s
step 1990/400000: train loss: 0.322649
epoch:43, batch14, load frames use: 6.2196598053s
step 1991/400000: train loss: 0.322345
epoch:43, batch15, load frames use: 6.3821208477s
step 1992/400000: train loss: 0.383710
epoch:43, batch16, load frames use: 6.63098597527s
step 1993/400000: train loss: 0.332374
epoch:43, batch17, load frames use: 4.41982412338s
step 1994/400000: train loss: 0.338476
epoch:43, batch18, load frames use: 9.22273206711s
step 1995/400000: train loss: 0.383453
epoch:43, batch19, load frames use: 6.77261805534s
step 1996/400000: train loss: 0.302347
epoch:43, batch20, load frames use: 5.66960191727s
step 1997/400000: train loss: 0.326620
epoch:43, batch21, load frames use: 6.28998184204s
step 1998/400000: train loss: 0.319215
epoch:43, batch22, load frames use: 7.8201239109s
step 1999/400000: train loss: 0.349394
epoch:43, batch23, load frames use: 6.76550602913s
step 2000/400000: train loss: 0.359002
step 2000/400000: validation loss: 0.333359
epoch:43, batch24, load frames use: 6.13380002975s
step 2001/400000: train loss: 0.314986
epoch:43, batch25, load frames use: 4.19433021545s
step 2002/400000: train loss: 0.318613
epoch:43, batch26, load frames use: 4.89118099213s
step 2003/400000: train loss: 0.379950
epoch:43, batch27, load frames use: 7.17967987061s
step 2004/400000: train loss: 0.325261
epoch:43, batch28, load frames use: 7.46178412437s
step 2005/400000: train loss: 0.342024
epoch:43, batch29, load frames use: 6.6744389534s
step 2006/400000: train loss: 0.338239
epoch:43, batch30, load frames use: 7.18927097321s
step 2007/400000: train loss: 0.352626
epoch:43, batch31, load frames use: 7.05586004257s
step 2008/400000: train loss: 0.275208
epoch:43, batch32, load frames use: 7.52239489555s
step 2009/400000: train loss: 0.308767
epoch:43, batch33, load frames use: 6.53748011589s
step 2010/400000: train loss: 0.329912
epoch:43, batch34, load frames use: 4.71159005165s
step 2011/400000: train loss: 0.327331
epoch:43, batch35, load frames use: 6.53797793388s
step 2012/400000: train loss: 0.329809
epoch:43, batch36, load frames use: 5.33055400848s
step 2013/400000: train loss: 0.361765
epoch:43, batch37, load frames use: 6.12191200256s
step 2014/400000: train loss: 0.351962
epoch:43, batch38, load frames use: 6.6774148941s
step 2015/400000: train loss: 0.345151
epoch:43, batch39, load frames use: 5.82813000679s
step 2016/400000: train loss: 0.339013
epoch:43, batch40, load frames use: 5.7115240097s
step 2017/400000: train loss: 0.291124
epoch:43, batch41, load frames use: 6.62723302841s
step 2018/400000: train loss: 0.302565
epoch:43, batch42, load frames use: 6.70802092552s
step 2019/400000: train loss: 0.433561
epoch:43, batch43, load frames use: 6.16919803619s
step 2020/400000: train loss: 0.361459
epoch:43, batch44, load frames use: 6.11884593964s
step 2021/400000: train loss: 0.320385
epoch:43, batch45, load frames use: 4.38079595566s
step 2022/400000: train loss: 0.400584
epoch:43, batch46, load frames use: 5.92306900024s
step 2023/400000: train loss: 0.365728
epoch:44, batch1, load frames use: 4.13574814796s
step 2024/400000: train loss: 0.320650
epoch:44, batch2, load frames use: 6.63222980499s
step 2025/400000: train loss: 0.323540
epoch:44, batch3, load frames use: 5.97997903824s
step 2026/400000: train loss: 0.348574
epoch:44, batch4, load frames use: 6.94812297821s
step 2027/400000: train loss: 0.295245
epoch:44, batch5, load frames use: 6.3017821312s
step 2028/400000: train loss: 0.312525
epoch:44, batch6, load frames use: 5.48481321335s
step 2029/400000: train loss: 0.312660
epoch:44, batch7, load frames use: 3.97282385826s
step 2030/400000: train loss: 0.353088
epoch:44, batch8, load frames use: 6.42139101028s
step 2031/400000: train loss: 0.336490
epoch:44, batch9, load frames use: 7.76362395287s
step 2032/400000: train loss: 0.318643
epoch:44, batch10, load frames use: 7.84842586517s
step 2033/400000: train loss: 0.369899
epoch:44, batch11, load frames use: 7.04055118561s
step 2034/400000: train loss: 0.318379
epoch:44, batch12, load frames use: 7.22113704681s
step 2035/400000: train loss: 0.374448
epoch:44, batch13, load frames use: 5.8587231636s
step 2036/400000: train loss: 0.357357
epoch:44, batch14, load frames use: 5.25903201103s
step 2037/400000: train loss: 0.305493
epoch:44, batch15, load frames use: 7.46432995796s
step 2038/400000: train loss: 0.334976
epoch:44, batch16, load frames use: 7.19170689583s
step 2039/400000: train loss: 0.332775
epoch:44, batch17, load frames use: 7.50405597687s
step 2040/400000: train loss: 0.306183
epoch:44, batch18, load frames use: 6.25521588326s
step 2041/400000: train loss: 0.305669
epoch:44, batch19, load frames use: 6.88129210472s
step 2042/400000: train loss: 0.347084
epoch:44, batch20, load frames use: 6.16776609421s
step 2043/400000: train loss: 0.319505
epoch:44, batch21, load frames use: 8.85412502289s
step 2044/400000: train loss: 0.344805
epoch:44, batch22, load frames use: 7.09496593475s
step 2045/400000: train loss: 0.301206
epoch:44, batch23, load frames use: 6.34475898743s
step 2046/400000: train loss: 0.397329
epoch:44, batch24, load frames use: 6.92852687836s
step 2047/400000: train loss: 0.356551
epoch:44, batch25, load frames use: 7.93271780014s
step 2048/400000: train loss: 0.292038
epoch:44, batch26, load frames use: 6.88473200798s
step 2049/400000: train loss: 0.328846
epoch:44, batch27, load frames use: 6.55789494514s
step 2050/400000: train loss: 0.332808
epoch:44, batch28, load frames use: 6.74816989899s
step 2051/400000: train loss: 0.357055
epoch:44, batch29, load frames use: 5.75471019745s
step 2052/400000: train loss: 0.355155
epoch:44, batch30, load frames use: 6.33424091339s
step 2053/400000: train loss: 0.324140
epoch:44, batch31, load frames use: 7.1200799942s
step 2054/400000: train loss: 0.342392
epoch:44, batch32, load frames use: 6.28262805939s
step 2055/400000: train loss: 0.322881
epoch:44, batch33, load frames use: 6.76439785957s
step 2056/400000: train loss: 0.339127
epoch:44, batch34, load frames use: 7.3648648262s
step 2057/400000: train loss: 0.356930
epoch:44, batch35, load frames use: 5.18387889862s
step 2058/400000: train loss: 0.321736
epoch:44, batch36, load frames use: 8.28558301926s
step 2059/400000: train loss: 0.345348
epoch:44, batch37, load frames use: 6.41131901741s
step 2060/400000: train loss: 0.293184
epoch:44, batch38, load frames use: 6.44457888603s
step 2061/400000: train loss: 0.376334
epoch:44, batch39, load frames use: 4.96047496796s
step 2062/400000: train loss: 0.322100
epoch:44, batch40, load frames use: 7.42296004295s
step 2063/400000: train loss: 0.351283
epoch:44, batch41, load frames use: 7.33773493767s
step 2064/400000: train loss: 0.338868
epoch:44, batch42, load frames use: 6.42244791985s
step 2065/400000: train loss: 0.375509
epoch:44, batch43, load frames use: 4.08032798767s
step 2066/400000: train loss: 0.341040
epoch:44, batch44, load frames use: 6.78755402565s
step 2067/400000: train loss: 0.325090
epoch:44, batch45, load frames use: 7.54944586754s
step 2068/400000: train loss: 0.288842
epoch:44, batch46, load frames use: 6.84803104401s
step 2069/400000: train loss: 0.325983
epoch:45, batch1, load frames use: 7.27441310883s
step 2070/400000: train loss: 0.277116
epoch:45, batch2, load frames use: 6.08118414879s
step 2071/400000: train loss: 0.321838
epoch:45, batch3, load frames use: 5.99825716019s
step 2072/400000: train loss: 0.343836
epoch:45, batch4, load frames use: 6.25834393501s
step 2073/400000: train loss: 0.307246
epoch:45, batch5, load frames use: 6.56107902527s
step 2074/400000: train loss: 0.312966
epoch:45, batch6, load frames use: 6.01372289658s
step 2075/400000: train loss: 0.347302
epoch:45, batch7, load frames use: 6.66798901558s
step 2076/400000: train loss: 0.311807
epoch:45, batch8, load frames use: 6.54717493057s
step 2077/400000: train loss: 0.303397
epoch:45, batch9, load frames use: 7.15764379501s
step 2078/400000: train loss: 0.314892
epoch:45, batch10, load frames use: 6.06637597084s
step 2079/400000: train loss: 0.354879
epoch:45, batch11, load frames use: 6.56031799316s
step 2080/400000: train loss: 0.365544
epoch:45, batch12, load frames use: 6.14429092407s
step 2081/400000: train loss: 0.277247
epoch:45, batch13, load frames use: 5.35540795326s
step 2082/400000: train loss: 0.319151
epoch:45, batch14, load frames use: 8.90123701096s
step 2083/400000: train loss: 0.380443
epoch:45, batch15, load frames use: 6.44732999802s
step 2084/400000: train loss: 0.309815
epoch:45, batch16, load frames use: 6.63676691055s
step 2085/400000: train loss: 0.310653
epoch:45, batch17, load frames use: 4.55161905289s
step 2086/400000: train loss: 0.294028
epoch:45, batch18, load frames use: 5.11349701881s
step 2087/400000: train loss: 0.319587
epoch:45, batch19, load frames use: 5.87457203865s
step 2088/400000: train loss: 0.341763
epoch:45, batch20, load frames use: 4.49691390991s
step 2089/400000: train loss: 0.324573
epoch:45, batch21, load frames use: 6.17252993584s
step 2090/400000: train loss: 0.298342
epoch:45, batch22, load frames use: 6.16781306267s
step 2091/400000: train loss: 0.381341
epoch:45, batch23, load frames use: 4.66730093956s
step 2092/400000: train loss: 0.302471
epoch:45, batch24, load frames use: 6.64486193657s
step 2093/400000: train loss: 0.298839
epoch:45, batch25, load frames use: 6.72105097771s
step 2094/400000: train loss: 0.350162
epoch:45, batch26, load frames use: 5.48270010948s
step 2095/400000: train loss: 0.341861
epoch:45, batch27, load frames use: 5.34446716309s
step 2096/400000: train loss: 0.371824
epoch:45, batch28, load frames use: 6.62884616852s
step 2097/400000: train loss: 0.307945
epoch:45, batch29, load frames use: 5.60876202583s
step 2098/400000: train loss: 0.326649
epoch:45, batch30, load frames use: 5.95645999908s
step 2099/400000: train loss: 0.310859
epoch:45, batch31, load frames use: 6.86192798615s
step 2100/400000: train loss: 0.350548
step 2100/400000: validation loss: 0.323151
epoch:45, batch32, load frames use: 6.96963214874s
step 2101/400000: train loss: 0.302482
epoch:45, batch33, load frames use: 7.35576105118s
step 2102/400000: train loss: 0.349186
epoch:45, batch34, load frames use: 6.53294706345s
step 2103/400000: train loss: 0.331580
epoch:45, batch35, load frames use: 7.35323905945s
step 2104/400000: train loss: 0.305511
epoch:45, batch36, load frames use: 5.60086297989s
step 2105/400000: train loss: 0.287350
epoch:45, batch37, load frames use: 5.96875214577s
step 2106/400000: train loss: 0.313979
epoch:45, batch38, load frames use: 7.04807209969s
step 2107/400000: train loss: 0.325672
epoch:45, batch39, load frames use: 6.06035399437s
step 2108/400000: train loss: 0.300457
epoch:45, batch40, load frames use: 6.16618108749s
step 2109/400000: train loss: 0.371964
epoch:45, batch41, load frames use: 5.7824420929s
step 2110/400000: train loss: 0.293139
epoch:45, batch42, load frames use: 4.62670612335s
step 2111/400000: train loss: 0.325557
epoch:45, batch43, load frames use: 5.84720897675s
step 2112/400000: train loss: 0.326277
epoch:45, batch44, load frames use: 6.43422698975s
step 2113/400000: train loss: 0.335035
epoch:45, batch45, load frames use: 4.96054410934s
step 2114/400000: train loss: 0.330593
epoch:45, batch46, load frames use: 5.97214698792s
step 2115/400000: train loss: 0.408074
epoch:46, batch1, load frames use: 5.16105103493s
step 2116/400000: train loss: 0.290734
epoch:46, batch2, load frames use: 6.87698698044s
step 2117/400000: train loss: 0.289487
epoch:46, batch3, load frames use: 7.53230690956s
step 2118/400000: train loss: 0.314168
epoch:46, batch4, load frames use: 6.8507039547s
step 2119/400000: train loss: 0.327313
epoch:46, batch5, load frames use: 5.90631318092s
step 2120/400000: train loss: 0.279399
epoch:46, batch6, load frames use: 7.49326586723s
step 2121/400000: train loss: 0.304108
epoch:46, batch7, load frames use: 5.45271897316s
step 2122/400000: train loss: 0.304691
epoch:46, batch8, load frames use: 6.1886639595s
step 2123/400000: train loss: 0.294186
epoch:46, batch9, load frames use: 5.64176297188s
step 2124/400000: train loss: 0.303912
epoch:46, batch10, load frames use: 6.27303290367s
step 2125/400000: train loss: 0.294554
epoch:46, batch11, load frames use: 6.39837908745s
step 2126/400000: train loss: 0.354687
epoch:46, batch12, load frames use: 6.44048881531s
step 2127/400000: train loss: 0.307710
epoch:46, batch13, load frames use: 6.39499688148s
step 2128/400000: train loss: 0.325347
epoch:46, batch14, load frames use: 7.82817697525s
step 2129/400000: train loss: 0.352043
epoch:46, batch15, load frames use: 8.10027813911s
step 2130/400000: train loss: 0.314746
epoch:46, batch16, load frames use: 5.63336896896s
step 2131/400000: train loss: 0.341483
epoch:46, batch17, load frames use: 7.25968790054s
step 2132/400000: train loss: 0.318152
epoch:46, batch18, load frames use: 7.68601894379s
step 2133/400000: train loss: 0.326111
epoch:46, batch19, load frames use: 6.76182293892s
step 2134/400000: train loss: 0.319196
epoch:46, batch20, load frames use: 9.01253199577s
step 2135/400000: train loss: 0.373176
epoch:46, batch21, load frames use: 5.90987014771s
step 2136/400000: train loss: 0.349074
epoch:46, batch22, load frames use: 6.73066902161s
step 2137/400000: train loss: 0.332716
epoch:46, batch23, load frames use: 5.69325590134s
step 2138/400000: train loss: 0.367860
epoch:46, batch24, load frames use: 5.43924999237s
step 2139/400000: train loss: 0.321582
epoch:46, batch25, load frames use: 6.1911251545s
step 2140/400000: train loss: 0.361037
epoch:46, batch26, load frames use: 5.76162481308s
step 2141/400000: train loss: 0.444798
epoch:46, batch27, load frames use: 5.70235013962s
step 2142/400000: train loss: 0.339644
epoch:46, batch28, load frames use: 6.55248904228s
step 2143/400000: train loss: 0.304938
epoch:46, batch29, load frames use: 6.77940487862s
step 2144/400000: train loss: 0.336001
epoch:46, batch30, load frames use: 4.7609539032s
step 2145/400000: train loss: 0.343304
epoch:46, batch31, load frames use: 6.0234849453s
step 2146/400000: train loss: 0.342378
epoch:46, batch32, load frames use: 6.53269910812s
step 2147/400000: train loss: 0.314621
epoch:46, batch33, load frames use: 7.06856298447s
step 2148/400000: train loss: 0.391555
epoch:46, batch34, load frames use: 7.26779508591s
step 2149/400000: train loss: 0.313398
epoch:46, batch35, load frames use: 5.52223300934s
step 2150/400000: train loss: 0.320635
epoch:46, batch36, load frames use: 6.12549805641s
step 2151/400000: train loss: 0.333235
epoch:46, batch37, load frames use: 5.90946102142s
step 2152/400000: train loss: 0.358530
epoch:46, batch38, load frames use: 7.14740800858s
step 2153/400000: train loss: 0.318241
epoch:46, batch39, load frames use: 6.08049798012s
step 2154/400000: train loss: 0.351901
epoch:46, batch40, load frames use: 6.15782880783s
step 2155/400000: train loss: 0.336304
epoch:46, batch41, load frames use: 6.12850403786s
step 2156/400000: train loss: 0.355491
epoch:46, batch42, load frames use: 5.83166790009s
step 2157/400000: train loss: 0.355419
epoch:46, batch43, load frames use: 6.02913713455s
step 2158/400000: train loss: 0.336997
epoch:46, batch44, load frames use: 5.55855202675s
step 2159/400000: train loss: 0.305855
epoch:46, batch45, load frames use: 5.61119914055s
step 2160/400000: train loss: 0.311392
epoch:46, batch46, load frames use: 5.01395797729s
step 2161/400000: train loss: 0.324665
epoch:47, batch1, load frames use: 5.68961501122s
step 2162/400000: train loss: 0.345732
epoch:47, batch2, load frames use: 6.17413806915s
step 2163/400000: train loss: 0.287476
epoch:47, batch3, load frames use: 6.6784889698s
step 2164/400000: train loss: 0.291245
epoch:47, batch4, load frames use: 6.79122209549s
step 2165/400000: train loss: 0.353185
epoch:47, batch5, load frames use: 6.11418700218s
step 2166/400000: train loss: 0.328809
epoch:47, batch6, load frames use: 6.28042697906s
step 2167/400000: train loss: 0.283094
epoch:47, batch7, load frames use: 4.76903700829s
step 2168/400000: train loss: 0.267099
epoch:47, batch8, load frames use: 8.21479296684s
step 2169/400000: train loss: 0.329824
epoch:47, batch9, load frames use: 6.49877905846s
step 2170/400000: train loss: 0.303223
epoch:47, batch10, load frames use: 6.73124217987s
step 2171/400000: train loss: 0.315912
epoch:47, batch11, load frames use: 6.32670402527s
step 2172/400000: train loss: 0.299234
epoch:47, batch12, load frames use: 6.61752605438s
step 2173/400000: train loss: 0.369257
epoch:47, batch13, load frames use: 6.67500185966s
step 2174/400000: train loss: 0.297655
epoch:47, batch14, load frames use: 5.43849992752s
step 2175/400000: train loss: 0.277394
epoch:47, batch15, load frames use: 6.86697506905s
step 2176/400000: train loss: 0.284366
epoch:47, batch16, load frames use: 6.75913596153s
step 2177/400000: train loss: 0.351704
epoch:47, batch17, load frames use: 5.72483491898s
step 2178/400000: train loss: 0.355056
epoch:47, batch18, load frames use: 5.64683699608s
step 2179/400000: train loss: 0.315112
epoch:47, batch19, load frames use: 8.34425210953s
step 2180/400000: train loss: 0.301590
epoch:47, batch20, load frames use: 5.84649920464s
step 2181/400000: train loss: 0.305707
epoch:47, batch21, load frames use: 5.61001682281s
step 2182/400000: train loss: 0.293899
epoch:47, batch22, load frames use: 8.76278686523s
step 2183/400000: train loss: 0.309679
epoch:47, batch23, load frames use: 5.97706604004s
step 2184/400000: train loss: 0.342011
epoch:47, batch24, load frames use: 6.42637515068s
step 2185/400000: train loss: 0.293678
epoch:47, batch25, load frames use: 7.0880510807s
step 2186/400000: train loss: 0.366626
epoch:47, batch26, load frames use: 5.90942502022s
step 2187/400000: train loss: 0.308537
epoch:47, batch27, load frames use: 6.0875389576s
step 2188/400000: train loss: 0.335090
epoch:47, batch28, load frames use: 6.28709220886s
step 2189/400000: train loss: 0.297827
epoch:47, batch29, load frames use: 6.0220811367s
step 2190/400000: train loss: 0.356658
epoch:47, batch30, load frames use: 4.79461503029s
step 2191/400000: train loss: 0.330658
epoch:47, batch31, load frames use: 8.04508686066s
step 2192/400000: train loss: 0.328486
epoch:47, batch32, load frames use: 5.86906814575s
step 2193/400000: train loss: 0.350373
epoch:47, batch33, load frames use: 7.17750787735s
step 2194/400000: train loss: 0.289459
epoch:47, batch34, load frames use: 7.66818594933s
step 2195/400000: train loss: 0.273907
epoch:47, batch35, load frames use: 4.96879005432s
step 2196/400000: train loss: 0.302723
epoch:47, batch36, load frames use: 7.88461995125s
step 2197/400000: train loss: 0.337779
epoch:47, batch37, load frames use: 5.83133196831s
step 2198/400000: train loss: 0.336818
epoch:47, batch38, load frames use: 8.36315202713s
step 2199/400000: train loss: 0.312799
epoch:47, batch39, load frames use: 6.45248603821s
step 2200/400000: train loss: 0.331884
step 2200/400000: validation loss: 0.298794
epoch:47, batch40, load frames use: 7.58358120918s
step 2201/400000: train loss: 0.312344
epoch:47, batch41, load frames use: 7.12886095047s
step 2202/400000: train loss: 0.341656
epoch:47, batch42, load frames use: 7.37679982185s
step 2203/400000: train loss: 0.318785
epoch:47, batch43, load frames use: 8.3247551918s
step 2204/400000: train loss: 0.320706
epoch:47, batch44, load frames use: 5.90150094032s
step 2205/400000: train loss: 0.346483
epoch:47, batch45, load frames use: 6.44895195961s
step 2206/400000: train loss: 0.320893
epoch:47, batch46, load frames use: 6.66513395309s
step 2207/400000: train loss: 0.325500
epoch:48, batch1, load frames use: 6.49415206909s
step 2208/400000: train loss: 0.277229
epoch:48, batch2, load frames use: 6.28039216995s
step 2209/400000: train loss: 0.316779
epoch:48, batch3, load frames use: 7.14388203621s
step 2210/400000: train loss: 0.346195
epoch:48, batch4, load frames use: 8.82682704926s
step 2211/400000: train loss: 0.340205
epoch:48, batch5, load frames use: 6.60439181328s
step 2212/400000: train loss: 0.316210
epoch:48, batch6, load frames use: 4.64572095871s
step 2213/400000: train loss: 0.304718
epoch:48, batch7, load frames use: 8.00787878036s
step 2214/400000: train loss: 0.305889
epoch:48, batch8, load frames use: 7.37236189842s
step 2215/400000: train loss: 0.307530
epoch:48, batch9, load frames use: 8.71050906181s
step 2216/400000: train loss: 0.330728
epoch:48, batch10, load frames use: 6.22002410889s
step 2217/400000: train loss: 0.282281
epoch:48, batch11, load frames use: 6.80531001091s
step 2218/400000: train loss: 0.271672
epoch:48, batch12, load frames use: 6.44976711273s
step 2219/400000: train loss: 0.314846
epoch:48, batch13, load frames use: 7.91499185562s
step 2220/400000: train loss: 0.280057
epoch:48, batch14, load frames use: 7.28688907623s
step 2221/400000: train loss: 0.312891
epoch:48, batch15, load frames use: 6.46455287933s
step 2222/400000: train loss: 0.323311
epoch:48, batch16, load frames use: 6.44863605499s
step 2223/400000: train loss: 0.303850
epoch:48, batch17, load frames use: 6.61875009537s
step 2224/400000: train loss: 0.278826
epoch:48, batch18, load frames use: 6.44076490402s
step 2225/400000: train loss: 0.348565
epoch:48, batch19, load frames use: 6.20116186142s
step 2226/400000: train loss: 0.311990
epoch:48, batch20, load frames use: 6.20857596397s
step 2227/400000: train loss: 0.285481
epoch:48, batch21, load frames use: 6.91606783867s
step 2228/400000: train loss: 0.342449
epoch:48, batch22, load frames use: 7.60442495346s
step 2229/400000: train loss: 0.329999
epoch:48, batch23, load frames use: 5.27852916718s
step 2230/400000: train loss: 0.306366
epoch:48, batch24, load frames use: 6.84455990791s
step 2231/400000: train loss: 0.305563
epoch:48, batch25, load frames use: 8.06413602829s
step 2232/400000: train loss: 0.353907
epoch:48, batch26, load frames use: 6.68792390823s
step 2233/400000: train loss: 0.313976
epoch:48, batch27, load frames use: 5.94105696678s
step 2234/400000: train loss: 0.342173
epoch:48, batch28, load frames use: 7.71641588211s
step 2235/400000: train loss: 0.320940
epoch:48, batch29, load frames use: 5.00410890579s
step 2236/400000: train loss: 0.298504
epoch:48, batch30, load frames use: 7.39987611771s
step 2237/400000: train loss: 0.289548
epoch:48, batch31, load frames use: 8.06973981857s
step 2238/400000: train loss: 0.342014
epoch:48, batch32, load frames use: 7.91553592682s
step 2239/400000: train loss: 0.314786
epoch:48, batch33, load frames use: 6.52968406677s
step 2240/400000: train loss: 0.332834
epoch:48, batch34, load frames use: 6.4701051712s
step 2241/400000: train loss: 0.310736
epoch:48, batch35, load frames use: 8.49191904068s
step 2242/400000: train loss: 0.282667
epoch:48, batch36, load frames use: 3.97828888893s
step 2243/400000: train loss: 0.330274
epoch:48, batch37, load frames use: 5.23862791061s
step 2244/400000: train loss: 0.279710
epoch:48, batch38, load frames use: 6.23704195023s
step 2245/400000: train loss: 0.284292
epoch:48, batch39, load frames use: 6.69121289253s
step 2246/400000: train loss: 0.329718
epoch:48, batch40, load frames use: 6.76207995415s
step 2247/400000: train loss: 0.294339
epoch:48, batch41, load frames use: 5.4147131443s
step 2248/400000: train loss: 0.252282
epoch:48, batch42, load frames use: 7.92708778381s
step 2249/400000: train loss: 0.313407
epoch:48, batch43, load frames use: 5.74842596054s
step 2250/400000: train loss: 0.306764
epoch:48, batch44, load frames use: 5.46908402443s
step 2251/400000: train loss: 0.304439
epoch:48, batch45, load frames use: 5.62753009796s
step 2252/400000: train loss: 0.325641
epoch:48, batch46, load frames use: 5.99371910095s
step 2253/400000: train loss: 0.296203
epoch:49, batch1, load frames use: 8.15281414986s
step 2254/400000: train loss: 0.301758
epoch:49, batch2, load frames use: 7.49027204514s
step 2255/400000: train loss: 0.283821
epoch:49, batch3, load frames use: 6.56702303886s
step 2256/400000: train loss: 0.307095
epoch:49, batch4, load frames use: 5.92419290543s
step 2257/400000: train loss: 0.287212
epoch:49, batch5, load frames use: 6.22440814972s
step 2258/400000: train loss: 0.280627
epoch:49, batch6, load frames use: 6.39001584053s
step 2259/400000: train loss: 0.284821
epoch:49, batch7, load frames use: 6.55621600151s
step 2260/400000: train loss: 0.298350
epoch:49, batch8, load frames use: 6.41414904594s
step 2261/400000: train loss: 0.295618
epoch:49, batch9, load frames use: 6.23576903343s
step 2262/400000: train loss: 0.308938
epoch:49, batch10, load frames use: 6.79118895531s
step 2263/400000: train loss: 0.270342
epoch:49, batch11, load frames use: 6.42621588707s
step 2264/400000: train loss: 0.288714
epoch:49, batch12, load frames use: 5.27215909958s
step 2265/400000: train loss: 0.322452
epoch:49, batch13, load frames use: 7.73893618584s
step 2266/400000: train loss: 0.330864
epoch:49, batch14, load frames use: 5.98687505722s
step 2267/400000: train loss: 0.300969
epoch:49, batch15, load frames use: 5.77640914917s
step 2268/400000: train loss: 0.283889
epoch:49, batch16, load frames use: 5.96441698074s
step 2269/400000: train loss: 0.294573
epoch:49, batch17, load frames use: 6.48848104477s
step 2270/400000: train loss: 0.327921
epoch:49, batch18, load frames use: 5.82407402992s
step 2271/400000: train loss: 0.289421
epoch:49, batch19, load frames use: 7.0861890316s
step 2272/400000: train loss: 0.303196
epoch:49, batch20, load frames use: 7.03707790375s
step 2273/400000: train loss: 0.297205
epoch:49, batch21, load frames use: 6.38799381256s
step 2274/400000: train loss: 0.328368
epoch:49, batch22, load frames use: 7.09322404861s
step 2275/400000: train loss: 0.336944
epoch:49, batch23, load frames use: 7.10959291458s
step 2276/400000: train loss: 0.311742
epoch:49, batch24, load frames use: 5.42079210281s
step 2277/400000: train loss: 0.296086
epoch:49, batch25, load frames use: 9.78521800041s
step 2278/400000: train loss: 0.328366
epoch:49, batch26, load frames use: 6.00293302536s
step 2279/400000: train loss: 0.268090
epoch:49, batch27, load frames use: 5.39947485924s
step 2280/400000: train loss: 0.281754
epoch:49, batch28, load frames use: 6.86060500145s
step 2281/400000: train loss: 0.330933
epoch:49, batch29, load frames use: 7.58352422714s
step 2282/400000: train loss: 0.296434
epoch:49, batch30, load frames use: 5.33844304085s
step 2283/400000: train loss: 0.313323
epoch:49, batch31, load frames use: 5.50653100014s
step 2284/400000: train loss: 0.306524
epoch:49, batch32, load frames use: 8.48311686516s
step 2285/400000: train loss: 0.311933
epoch:49, batch33, load frames use: 6.90169501305s
step 2286/400000: train loss: 0.359138
epoch:49, batch34, load frames use: 5.99516415596s
step 2287/400000: train loss: 0.362629
epoch:49, batch35, load frames use: 5.78733611107s
step 2288/400000: train loss: 0.367385
epoch:49, batch36, load frames use: 5.71981096268s
step 2289/400000: train loss: 0.265858
epoch:49, batch37, load frames use: 6.30081605911s
step 2290/400000: train loss: 0.336032
epoch:49, batch38, load frames use: 7.3165872097s
step 2291/400000: train loss: 0.308555
epoch:49, batch39, load frames use: 6.17739486694s
step 2292/400000: train loss: 0.300901
epoch:49, batch40, load frames use: 5.63724303246s
step 2293/400000: train loss: 0.305256
epoch:49, batch41, load frames use: 5.99257707596s
step 2294/400000: train loss: 0.315081
epoch:49, batch42, load frames use: 6.58414292336s
step 2295/400000: train loss: 0.301430
epoch:49, batch43, load frames use: 6.3587770462s
step 2296/400000: train loss: 0.311124
epoch:49, batch44, load frames use: 6.78542399406s
step 2297/400000: train loss: 0.333898
epoch:49, batch45, load frames use: 6.91001486778s
step 2298/400000: train loss: 0.324542
epoch:49, batch46, load frames use: 5.84294104576s
step 2299/400000: train loss: 0.278363
epoch:50, batch1, load frames use: 5.75618314743s
step 2300/400000: train loss: 0.305576
step 2300/400000: validation loss: 0.303033
epoch:50, batch2, load frames use: 6.47055912018s
step 2301/400000: train loss: 0.315468
epoch:50, batch3, load frames use: 4.98755002022s
step 2302/400000: train loss: 0.282089
epoch:50, batch4, load frames use: 6.79723286629s
step 2303/400000: train loss: 0.321771
epoch:50, batch5, load frames use: 7.13454985619s
step 2304/400000: train loss: 0.253502
epoch:50, batch6, load frames use: 8.16819906235s
step 2305/400000: train loss: 0.281459
epoch:50, batch7, load frames use: 6.60679411888s
step 2306/400000: train loss: 0.315551
epoch:50, batch8, load frames use: 6.77477192879s
step 2307/400000: train loss: 0.294181
epoch:50, batch9, load frames use: 7.11486721039s
step 2308/400000: train loss: 0.303819
epoch:50, batch10, load frames use: 6.43510007858s
step 2309/400000: train loss: 0.286693
epoch:50, batch11, load frames use: 4.52076601982s
step 2310/400000: train loss: 0.292217
epoch:50, batch12, load frames use: 6.54698300362s
step 2311/400000: train loss: 0.307100
epoch:50, batch13, load frames use: 5.66464495659s
step 2312/400000: train loss: 0.326473
epoch:50, batch14, load frames use: 5.07233691216s
step 2313/400000: train loss: 0.302664
epoch:50, batch15, load frames use: 6.64209794998s
step 2314/400000: train loss: 0.278563
epoch:50, batch16, load frames use: 6.91858196259s
step 2315/400000: train loss: 0.309010
epoch:50, batch17, load frames use: 6.38019895554s
step 2316/400000: train loss: 0.320644
epoch:50, batch18, load frames use: 6.89045405388s
step 2317/400000: train loss: 0.270783
epoch:50, batch19, load frames use: 4.88064718246s
step 2318/400000: train loss: 0.307253
epoch:50, batch20, load frames use: 7.6263589859s
step 2319/400000: train loss: 0.303139
epoch:50, batch21, load frames use: 7.35462903976s
step 2320/400000: train loss: 0.334425
epoch:50, batch22, load frames use: 5.0585269928s
step 2321/400000: train loss: 0.334811
epoch:50, batch23, load frames use: 8.65007400513s
step 2322/400000: train loss: 0.352871
epoch:50, batch24, load frames use: 8.09692788124s
step 2323/400000: train loss: 0.293345
epoch:50, batch25, load frames use: 8.11596512794s
step 2324/400000: train loss: 0.275575
epoch:50, batch26, load frames use: 6.37941884995s
step 2325/400000: train loss: 0.346304
epoch:50, batch27, load frames use: 5.39213585854s
step 2326/400000: train loss: 0.315807
epoch:50, batch28, load frames use: 8.36380815506s
step 2327/400000: train loss: 0.290252
epoch:50, batch29, load frames use: 6.77224993706s
step 2328/400000: train loss: 0.348993
epoch:50, batch30, load frames use: 6.78478598595s
step 2329/400000: train loss: 0.314599
epoch:50, batch31, load frames use: 5.35022783279s
step 2330/400000: train loss: 0.302848
epoch:50, batch32, load frames use: 4.7638771534s
step 2331/400000: train loss: 0.369589
epoch:50, batch33, load frames use: 5.80574893951s
step 2332/400000: train loss: 0.362300
epoch:50, batch34, load frames use: 6.36514401436s
step 2333/400000: train loss: 0.304689
epoch:50, batch35, load frames use: 5.99059677124s
step 2334/400000: train loss: 0.342635
epoch:50, batch36, load frames use: 5.59270095825s
step 2335/400000: train loss: 0.289687
epoch:50, batch37, load frames use: 6.28806090355s
step 2336/400000: train loss: 0.281041
epoch:50, batch38, load frames use: 6.0327539444s
step 2337/400000: train loss: 0.348365
epoch:50, batch39, load frames use: 7.10738897324s
step 2338/400000: train loss: 0.272641
epoch:50, batch40, load frames use: 6.13292884827s
step 2339/400000: train loss: 0.333658
epoch:50, batch41, load frames use: 7.34030508995s
step 2340/400000: train loss: 0.346964
epoch:50, batch42, load frames use: 5.89990496635s
step 2341/400000: train loss: 0.355419
epoch:50, batch43, load frames use: 6.57883810997s
step 2342/400000: train loss: 0.339852
epoch:50, batch44, load frames use: 7.73176193237s
step 2343/400000: train loss: 0.306031
epoch:50, batch45, load frames use: 6.57965612411s
step 2344/400000: train loss: 0.282970
epoch:50, batch46, load frames use: 4.28931117058s
step 2345/400000: train loss: 0.300489
epoch:51, batch1, load frames use: 7.6962518692s
step 2346/400000: train loss: 0.266253
epoch:51, batch2, load frames use: 6.17201304436s
step 2347/400000: train loss: 0.333208
epoch:51, batch3, load frames use: 5.82854318619s
step 2348/400000: train loss: 0.301406
epoch:51, batch4, load frames use: 6.83996391296s
step 2349/400000: train loss: 0.267972
epoch:51, batch5, load frames use: 7.83863806725s
step 2350/400000: train loss: 0.331827
epoch:51, batch6, load frames use: 6.51677584648s
step 2351/400000: train loss: 0.324469
epoch:51, batch7, load frames use: 7.39727282524s
step 2352/400000: train loss: 0.302420
epoch:51, batch8, load frames use: 5.80576586723s
step 2353/400000: train loss: 0.286133
epoch:51, batch9, load frames use: 6.00583696365s
step 2354/400000: train loss: 0.275375
epoch:51, batch10, load frames use: 7.91494107246s
step 2355/400000: train loss: 0.315107
epoch:51, batch11, load frames use: 7.07965707779s
step 2356/400000: train loss: 0.287593
epoch:51, batch12, load frames use: 6.61015105247s
step 2357/400000: train loss: 0.314018
epoch:51, batch13, load frames use: 7.09938097s
step 2358/400000: train loss: 0.286945
epoch:51, batch14, load frames use: 6.6272611618s
step 2359/400000: train loss: 0.328522
epoch:51, batch15, load frames use: 6.63159918785s
step 2360/400000: train loss: 0.315256
epoch:51, batch16, load frames use: 7.25674295425s
step 2361/400000: train loss: 0.292377
epoch:51, batch17, load frames use: 6.66424584389s
step 2362/400000: train loss: 0.306467
epoch:51, batch18, load frames use: 5.22583413124s
step 2363/400000: train loss: 0.282726
epoch:51, batch19, load frames use: 5.94763994217s
step 2364/400000: train loss: 0.278738
epoch:51, batch20, load frames use: 5.1172990799s
step 2365/400000: train loss: 0.318075
epoch:51, batch21, load frames use: 4.94856286049s
step 2366/400000: train loss: 0.266204
epoch:51, batch22, load frames use: 6.49467301369s
step 2367/400000: train loss: 0.272100
epoch:51, batch23, load frames use: 6.14671397209s
step 2368/400000: train loss: 0.306315
epoch:51, batch24, load frames use: 6.91719484329s
step 2369/400000: train loss: 0.292635
epoch:51, batch25, load frames use: 6.74094510078s
step 2370/400000: train loss: 0.300193
epoch:51, batch26, load frames use: 5.20783686638s
step 2371/400000: train loss: 0.265381
epoch:51, batch27, load frames use: 6.82347488403s
step 2372/400000: train loss: 0.283301
epoch:51, batch28, load frames use: 7.688133955s
step 2373/400000: train loss: 0.343362
epoch:51, batch29, load frames use: 6.15760588646s
step 2374/400000: train loss: 0.320849
epoch:51, batch30, load frames use: 5.9810628891s
step 2375/400000: train loss: 0.305006
epoch:51, batch31, load frames use: 7.80878376961s
step 2376/400000: train loss: 0.287934
epoch:51, batch32, load frames use: 5.57065701485s
step 2377/400000: train loss: 0.322062
epoch:51, batch33, load frames use: 6.62388896942s
step 2378/400000: train loss: 0.299395
epoch:51, batch34, load frames use: 6.33100104332s
step 2379/400000: train loss: 0.293453
epoch:51, batch35, load frames use: 6.2339758873s
step 2380/400000: train loss: 0.297823
epoch:51, batch36, load frames use: 4.4999320507s
step 2381/400000: train loss: 0.334113
epoch:51, batch37, load frames use: 7.08148622513s
step 2382/400000: train loss: 0.286040
epoch:51, batch38, load frames use: 7.10481595993s
step 2383/400000: train loss: 0.304453
epoch:51, batch39, load frames use: 6.708370924s
step 2384/400000: train loss: 0.301066
epoch:51, batch40, load frames use: 6.91038894653s
step 2385/400000: train loss: 0.306260
epoch:51, batch41, load frames use: 7.50248003006s
step 2386/400000: train loss: 0.325319
epoch:51, batch42, load frames use: 4.47250795364s
step 2387/400000: train loss: 0.305447
epoch:51, batch43, load frames use: 5.83354496956s
step 2388/400000: train loss: 0.286533
epoch:51, batch44, load frames use: 6.0s
step 2389/400000: train loss: 0.308650
epoch:51, batch45, load frames use: 5.60818600655s
step 2390/400000: train loss: 0.308074
epoch:51, batch46, load frames use: 6.44956493378s
step 2391/400000: train loss: 0.320790
epoch:52, batch1, load frames use: 7.14916586876s
step 2392/400000: train loss: 0.260602
epoch:52, batch2, load frames use: 7.1268889904s
step 2393/400000: train loss: 0.265597
epoch:52, batch3, load frames use: 6.6190598011s
step 2394/400000: train loss: 0.259773
epoch:52, batch4, load frames use: 5.75311398506s
step 2395/400000: train loss: 0.299558
epoch:52, batch5, load frames use: 5.64638495445s
step 2396/400000: train loss: 0.275545
epoch:52, batch6, load frames use: 5.70299196243s
step 2397/400000: train loss: 0.293645
epoch:52, batch7, load frames use: 4.89005804062s
step 2398/400000: train loss: 0.282282
epoch:52, batch8, load frames use: 8.67418694496s
step 2399/400000: train loss: 0.298932
epoch:52, batch9, load frames use: 6.1096830368s
step 2400/400000: train loss: 0.315662
step 2400/400000: validation loss: 0.280564
epoch:52, batch10, load frames use: 7.27005004883s
step 2401/400000: train loss: 0.337162
epoch:52, batch11, load frames use: 5.78362393379s
step 2402/400000: train loss: 0.280214
epoch:52, batch12, load frames use: 4.50488400459s
step 2403/400000: train loss: 0.293547
epoch:52, batch13, load frames use: 6.88978791237s
step 2404/400000: train loss: 0.289568
epoch:52, batch14, load frames use: 6.15989804268s
step 2405/400000: train loss: 0.283348
epoch:52, batch15, load frames use: 8.36850595474s
step 2406/400000: train loss: 0.355178
epoch:52, batch16, load frames use: 4.46107792854s
step 2407/400000: train loss: 0.299323
epoch:52, batch17, load frames use: 5.86527299881s
step 2408/400000: train loss: 0.286717
epoch:52, batch18, load frames use: 7.3729569912s
step 2409/400000: train loss: 0.324000
epoch:52, batch19, load frames use: 7.24044394493s
step 2410/400000: train loss: 0.312738
epoch:52, batch20, load frames use: 8.19915485382s
step 2411/400000: train loss: 0.280122
epoch:52, batch21, load frames use: 5.49443817139s
step 2412/400000: train loss: 0.307321
epoch:52, batch22, load frames use: 6.94607901573s
step 2413/400000: train loss: 0.340569
epoch:52, batch23, load frames use: 5.19669699669s
step 2414/400000: train loss: 0.265385
epoch:52, batch24, load frames use: 5.82899689674s
step 2415/400000: train loss: 0.324890
epoch:52, batch25, load frames use: 6.47365498543s
step 2416/400000: train loss: 0.347434
epoch:52, batch26, load frames use: 7.13095712662s
step 2417/400000: train loss: 0.314403
epoch:52, batch27, load frames use: 5.29914116859s
step 2418/400000: train loss: 0.266189
epoch:52, batch28, load frames use: 5.47380614281s
step 2419/400000: train loss: 0.294238
epoch:52, batch29, load frames use: 7.20813894272s
step 2420/400000: train loss: 0.281181
epoch:52, batch30, load frames use: 6.20972013474s
step 2421/400000: train loss: 0.288486
epoch:52, batch31, load frames use: 5.61314082146s
step 2422/400000: train loss: 0.314191
epoch:52, batch32, load frames use: 4.73877120018s
step 2423/400000: train loss: 0.327076
epoch:52, batch33, load frames use: 6.72872900963s
step 2424/400000: train loss: 0.318980
epoch:52, batch34, load frames use: 3.89117288589s
step 2425/400000: train loss: 0.301926
epoch:52, batch35, load frames use: 3.89868497849s
step 2426/400000: train loss: 0.335655
epoch:52, batch36, load frames use: 6.39297413826s
step 2427/400000: train loss: 0.310913
epoch:52, batch37, load frames use: 3.47840690613s
step 2428/400000: train loss: 0.296359
epoch:52, batch38, load frames use: 6.70123100281s
step 2429/400000: train loss: 0.309225
epoch:52, batch39, load frames use: 7.65231013298s
step 2430/400000: train loss: 0.324364
epoch:52, batch40, load frames use: 4.43162298203s
step 2431/400000: train loss: 0.271704
epoch:52, batch41, load frames use: 4.73784518242s
step 2432/400000: train loss: 0.264846
epoch:52, batch42, load frames use: 5.68427205086s
step 2433/400000: train loss: 0.317994
epoch:52, batch43, load frames use: 4.95195412636s
step 2434/400000: train loss: 0.277216
epoch:52, batch44, load frames use: 3.6437458992s
step 2435/400000: train loss: 0.283664
epoch:52, batch45, load frames use: 4.68545007706s
step 2436/400000: train loss: 0.305554
epoch:52, batch46, load frames use: 7.06291103363s
step 2437/400000: train loss: 0.298163
epoch:53, batch1, load frames use: 5.0154440403s
step 2438/400000: train loss: 0.280267
epoch:53, batch2, load frames use: 3.96141600609s
step 2439/400000: train loss: 0.291895
epoch:53, batch3, load frames use: 3.66213297844s
step 2440/400000: train loss: 0.270189
epoch:53, batch4, load frames use: 6.11925196648s
step 2441/400000: train loss: 0.274378
epoch:53, batch5, load frames use: 5.27267003059s
step 2442/400000: train loss: 0.317525
epoch:53, batch6, load frames use: 6.2466840744s
step 2443/400000: train loss: 0.270246
epoch:53, batch7, load frames use: 5.20979404449s
step 2444/400000: train loss: 0.289317
epoch:53, batch8, load frames use: 5.63577985764s
step 2445/400000: train loss: 0.283503
epoch:53, batch9, load frames use: 3.7525780201s
step 2446/400000: train loss: 0.290543
epoch:53, batch10, load frames use: 7.2644329071s
step 2447/400000: train loss: 0.310092
epoch:53, batch11, load frames use: 6.26969981194s
step 2448/400000: train loss: 0.275582
epoch:53, batch12, load frames use: 5.79274201393s
step 2449/400000: train loss: 0.303621
epoch:53, batch13, load frames use: 3.77936291695s
step 2450/400000: train loss: 0.263987
epoch:53, batch14, load frames use: 6.47107505798s
step 2451/400000: train loss: 0.274696
epoch:53, batch15, load frames use: 7.55999517441s
step 2452/400000: train loss: 0.283838
epoch:53, batch16, load frames use: 4.58907818794s
step 2453/400000: train loss: 0.306301
epoch:53, batch17, load frames use: 5.3184158802s
step 2454/400000: train loss: 0.303914
epoch:53, batch18, load frames use: 5.41776800156s
step 2455/400000: train loss: 0.301063
epoch:53, batch19, load frames use: 4.65512895584s
step 2456/400000: train loss: 0.321640
epoch:53, batch20, load frames use: 5.01642584801s
step 2457/400000: train loss: 0.323535
epoch:53, batch21, load frames use: 5.75714206696s
step 2458/400000: train loss: 0.293583
epoch:53, batch22, load frames use: 4.76537799835s
step 2459/400000: train loss: 0.287584
epoch:53, batch23, load frames use: 7.87712287903s
step 2460/400000: train loss: 0.283243
epoch:53, batch24, load frames use: 8.78147888184s
step 2461/400000: train loss: 0.289854
epoch:53, batch25, load frames use: 5.52448487282s
step 2462/400000: train loss: 0.302569
epoch:53, batch26, load frames use: 4.0725979805s
step 2463/400000: train loss: 0.328308
epoch:53, batch27, load frames use: 5.37718582153s
step 2464/400000: train loss: 0.305235
epoch:53, batch28, load frames use: 6.06574106216s
step 2465/400000: train loss: 0.286304
epoch:53, batch29, load frames use: 5.22219586372s
step 2466/400000: train loss: 0.269538
epoch:53, batch30, load frames use: 7.43269014359s
step 2467/400000: train loss: 0.317050
epoch:53, batch31, load frames use: 4.50310993195s
step 2468/400000: train loss: 0.275877
epoch:53, batch32, load frames use: 4.92558789253s
step 2469/400000: train loss: 0.274999
epoch:53, batch33, load frames use: 4.26349210739s
step 2470/400000: train loss: 0.307037
epoch:53, batch34, load frames use: 3.87047386169s
step 2471/400000: train loss: 0.296587
epoch:53, batch35, load frames use: 6.61278510094s
step 2472/400000: train loss: 0.281233
epoch:53, batch36, load frames use: 3.57103705406s
step 2473/400000: train loss: 0.338983
epoch:53, batch37, load frames use: 4.59999394417s
step 2474/400000: train loss: 0.268778
epoch:53, batch38, load frames use: 8.46848917007s
step 2475/400000: train loss: 0.285704
epoch:53, batch39, load frames use: 3.97120594978s
step 2476/400000: train loss: 0.347775
epoch:53, batch40, load frames use: 6.07363605499s
step 2477/400000: train loss: 0.328401
epoch:53, batch41, load frames use: 3.85056090355s
step 2478/400000: train loss: 0.316141
epoch:53, batch42, load frames use: 6.40201401711s
step 2479/400000: train loss: 0.323356
epoch:53, batch43, load frames use: 5.86242198944s
step 2480/400000: train loss: 0.342666
epoch:53, batch44, load frames use: 4.50079011917s
step 2481/400000: train loss: 0.300087
epoch:53, batch45, load frames use: 4.44218802452s
step 2482/400000: train loss: 0.284680
epoch:53, batch46, load frames use: 5.2044608593s
step 2483/400000: train loss: 0.318959
epoch:54, batch1, load frames use: 4.53855490685s
step 2484/400000: train loss: 0.296503
epoch:54, batch2, load frames use: 4.40908408165s
step 2485/400000: train loss: 0.299399
epoch:54, batch3, load frames use: 5.21775698662s
step 2486/400000: train loss: 0.303777
epoch:54, batch4, load frames use: 4.75352096558s
step 2487/400000: train loss: 0.268072
epoch:54, batch5, load frames use: 5.63965678215s
step 2488/400000: train loss: 0.287687
epoch:54, batch6, load frames use: 3.48592305183s
step 2489/400000: train loss: 0.305698
epoch:54, batch7, load frames use: 5.90457701683s
step 2490/400000: train loss: 0.307392
epoch:54, batch8, load frames use: 3.74760293961s
step 2491/400000: train loss: 0.276291
epoch:54, batch9, load frames use: 3.9319050312s
step 2492/400000: train loss: 0.285096
epoch:54, batch10, load frames use: 7.626060009s
step 2493/400000: train loss: 0.266759
epoch:54, batch11, load frames use: 3.8271510601s
step 2494/400000: train loss: 0.302401
epoch:54, batch12, load frames use: 5.41875600815s
step 2495/400000: train loss: 0.308169
epoch:54, batch13, load frames use: 3.40229392052s
step 2496/400000: train loss: 0.262205
epoch:54, batch14, load frames use: 5.0815680027s
step 2497/400000: train loss: 0.329223
epoch:54, batch15, load frames use: 8.35876584053s
step 2498/400000: train loss: 0.331049
epoch:54, batch16, load frames use: 5.84501004219s
step 2499/400000: train loss: 0.303731
epoch:54, batch17, load frames use: 4.89856314659s
step 2500/400000: train loss: 0.289004
step 2500/400000: validation loss: 0.268529
epoch:54, batch18, load frames use: 5.25226020813s
step 2501/400000: train loss: 0.308253
epoch:54, batch19, load frames use: 3.10518097878s
step 2502/400000: train loss: 0.285034
epoch:54, batch20, load frames use: 3.80627202988s
step 2503/400000: train loss: 0.265376
epoch:54, batch21, load frames use: 5.49183106422s
step 2504/400000: train loss: 0.258725
epoch:54, batch22, load frames use: 5.01569509506s
step 2505/400000: train loss: 0.287280
epoch:54, batch23, load frames use: 4.86073303223s
step 2506/400000: train loss: 0.291866
epoch:54, batch24, load frames use: 5.39453697205s
step 2507/400000: train loss: 0.300168
epoch:54, batch25, load frames use: 6.07995009422s
step 2508/400000: train loss: 0.280153
epoch:54, batch26, load frames use: 7.67840194702s
step 2509/400000: train loss: 0.313066
epoch:54, batch27, load frames use: 6.78634500504s
step 2510/400000: train loss: 0.296656
epoch:54, batch28, load frames use: 5.54525399208s
step 2511/400000: train loss: 0.292627
epoch:54, batch29, load frames use: 6.32315707207s
step 2512/400000: train loss: 0.354065
epoch:54, batch30, load frames use: 7.70078206062s
step 2513/400000: train loss: 0.274879
epoch:54, batch31, load frames use: 3.12654995918s
step 2514/400000: train loss: 0.306928
epoch:54, batch32, load frames use: 6.29772686958s
step 2515/400000: train loss: 0.327984
epoch:54, batch33, load frames use: 7.04856991768s
step 2516/400000: train loss: 0.282878
epoch:54, batch34, load frames use: 3.59122180939s
step 2517/400000: train loss: 0.303381
epoch:54, batch35, load frames use: 7.27037215233s
step 2518/400000: train loss: 0.333567
epoch:54, batch36, load frames use: 4.09329891205s
step 2519/400000: train loss: 0.304158
epoch:54, batch37, load frames use: 6.55639386177s
step 2520/400000: train loss: 0.303180
epoch:54, batch38, load frames use: 4.36125206947s
step 2521/400000: train loss: 0.335361
epoch:54, batch39, load frames use: 3.75025701523s
step 2522/400000: train loss: 0.331219
epoch:54, batch40, load frames use: 5.59905815125s
step 2523/400000: train loss: 0.282624
epoch:54, batch41, load frames use: 6.56074500084s
step 2524/400000: train loss: 0.293090
epoch:54, batch42, load frames use: 4.76813697815s
step 2525/400000: train loss: 0.294245
epoch:54, batch43, load frames use: 3.67735910416s
step 2526/400000: train loss: 0.314370
epoch:54, batch44, load frames use: 3.34654307365s
step 2527/400000: train loss: 0.280823
epoch:54, batch45, load frames use: 3.69000196457s
step 2528/400000: train loss: 0.268564
epoch:54, batch46, load frames use: 3.57021903992s
step 2529/400000: train loss: 0.292828
epoch:55, batch1, load frames use: 3.62470793724s
step 2530/400000: train loss: 0.267460
epoch:55, batch2, load frames use: 4.95152997971s
step 2531/400000: train loss: 0.312598
epoch:55, batch3, load frames use: 5.80982685089s
step 2532/400000: train loss: 0.228015
epoch:55, batch4, load frames use: 5.69003009796s
step 2533/400000: train loss: 0.273756
epoch:55, batch5, load frames use: 5.82228088379s
step 2534/400000: train loss: 0.305819
epoch:55, batch6, load frames use: 5.87319207191s
step 2535/400000: train loss: 0.282527
epoch:55, batch7, load frames use: 5.77129006386s
step 2536/400000: train loss: 0.274852
epoch:55, batch8, load frames use: 7.72639703751s
step 2537/400000: train loss: 0.269682
epoch:55, batch9, load frames use: 5.94945001602s
step 2538/400000: train loss: 0.275511
epoch:55, batch10, load frames use: 4.6644411087s
step 2539/400000: train loss: 0.292012
epoch:55, batch11, load frames use: 3.02258515358s
step 2540/400000: train loss: 0.295140
epoch:55, batch12, load frames use: 6.6081609726s
step 2541/400000: train loss: 0.292563
epoch:55, batch13, load frames use: 3.06108999252s
step 2542/400000: train loss: 0.260784
epoch:55, batch14, load frames use: 7.19269299507s
step 2543/400000: train loss: 0.306430
epoch:55, batch15, load frames use: 3.77220702171s
step 2544/400000: train loss: 0.281154
epoch:55, batch16, load frames use: 6.6417620182s
step 2545/400000: train loss: 0.258892
epoch:55, batch17, load frames use: 3.93728613853s
step 2546/400000: train loss: 0.261849
epoch:55, batch18, load frames use: 6.32496595383s
step 2547/400000: train loss: 0.285345
epoch:55, batch19, load frames use: 5.53087806702s
step 2548/400000: train loss: 0.331061
epoch:55, batch20, load frames use: 6.21425199509s
step 2549/400000: train loss: 0.263430
epoch:55, batch21, load frames use: 4.69834113121s
step 2550/400000: train loss: 0.274495
epoch:55, batch22, load frames use: 6.87736988068s
step 2551/400000: train loss: 0.246818
epoch:55, batch23, load frames use: 3.18877387047s
step 2552/400000: train loss: 0.274082
epoch:55, batch24, load frames use: 5.23550200462s
step 2553/400000: train loss: 0.285096
epoch:55, batch25, load frames use: 4.23819088936s
step 2554/400000: train loss: 0.303307
epoch:55, batch26, load frames use: 3.91095805168s
step 2555/400000: train loss: 0.261215
epoch:55, batch27, load frames use: 6.45207691193s
step 2556/400000: train loss: 0.285820
epoch:55, batch28, load frames use: 7.67161798477s
step 2557/400000: train loss: 0.287291
epoch:55, batch29, load frames use: 5.81696295738s
step 2558/400000: train loss: 0.279728
epoch:55, batch30, load frames use: 6.47055506706s
step 2559/400000: train loss: 0.263650
epoch:55, batch31, load frames use: 6.01411986351s
step 2560/400000: train loss: 0.272395
epoch:55, batch32, load frames use: 6.79079198837s
step 2561/400000: train loss: 0.297985
epoch:55, batch33, load frames use: 7.56452584267s
step 2562/400000: train loss: 0.283928
epoch:55, batch34, load frames use: 6.35902500153s
step 2563/400000: train loss: 0.248530
epoch:55, batch35, load frames use: 3.30766701698s
step 2564/400000: train loss: 0.310590
epoch:55, batch36, load frames use: 6.44340705872s
step 2565/400000: train loss: 0.298539
epoch:55, batch37, load frames use: 3.67171096802s
step 2566/400000: train loss: 0.301346
epoch:55, batch38, load frames use: 6.96512317657s
step 2567/400000: train loss: 0.285050
epoch:55, batch39, load frames use: 5.99833106995s
step 2568/400000: train loss: 0.275742
epoch:55, batch40, load frames use: 5.66741085052s
step 2569/400000: train loss: 0.250980
epoch:55, batch41, load frames use: 5.63910698891s
step 2570/400000: train loss: 0.279600
epoch:55, batch42, load frames use: 5.68570303917s
step 2571/400000: train loss: 0.315549
epoch:55, batch43, load frames use: 3.67961502075s
step 2572/400000: train loss: 0.282415
epoch:55, batch44, load frames use: 6.39447402954s
step 2573/400000: train loss: 0.275358
epoch:55, batch45, load frames use: 6.22714591026s
step 2574/400000: train loss: 0.361731
epoch:55, batch46, load frames use: 3.71053886414s
step 2575/400000: train loss: 0.281528
epoch:56, batch1, load frames use: 3.33543920517s
step 2576/400000: train loss: 0.305807
epoch:56, batch2, load frames use: 5.71339511871s
step 2577/400000: train loss: 0.261471
epoch:56, batch3, load frames use: 3.99508690834s
step 2578/400000: train loss: 0.257551
epoch:56, batch4, load frames use: 7.70015311241s
step 2579/400000: train loss: 0.304128
epoch:56, batch5, load frames use: 5.87052607536s
step 2580/400000: train loss: 0.297925
epoch:56, batch6, load frames use: 5.55601596832s
step 2581/400000: train loss: 0.302677
epoch:56, batch7, load frames use: 4.63825297356s
step 2582/400000: train loss: 0.294522
epoch:56, batch8, load frames use: 6.38093805313s
step 2583/400000: train loss: 0.254287
epoch:56, batch9, load frames use: 4.36856818199s
step 2584/400000: train loss: 0.284649
epoch:56, batch10, load frames use: 6.22239995003s
step 2585/400000: train loss: 0.283323
epoch:56, batch11, load frames use: 5.56828308105s
step 2586/400000: train loss: 0.308935
epoch:56, batch12, load frames use: 5.55230402946s
step 2587/400000: train loss: 0.347466
epoch:56, batch13, load frames use: 3.9691131115s
step 2588/400000: train loss: 0.296068
epoch:56, batch14, load frames use: 7.70378112793s
step 2589/400000: train loss: 0.286791
epoch:56, batch15, load frames use: 2.98956012726s
step 2590/400000: train loss: 0.309694
epoch:56, batch16, load frames use: 7.52362799644s
step 2591/400000: train loss: 0.303175
epoch:56, batch17, load frames use: 4.30782318115s
step 2592/400000: train loss: 0.273027
epoch:56, batch18, load frames use: 6.64929199219s
step 2593/400000: train loss: 0.318399
epoch:56, batch19, load frames use: 6.44968390465s
step 2594/400000: train loss: 0.272592
epoch:56, batch20, load frames use: 5.44201993942s
step 2595/400000: train loss: 0.275680
epoch:56, batch21, load frames use: 3.81835889816s
step 2596/400000: train loss: 0.321144
epoch:56, batch22, load frames use: 8.93264985085s
step 2597/400000: train loss: 0.299798
epoch:56, batch23, load frames use: 3.53055787086s
step 2598/400000: train loss: 0.289839
epoch:56, batch24, load frames use: 7.25786995888s
step 2599/400000: train loss: 0.265705
epoch:56, batch25, load frames use: 7.06716299057s
step 2600/400000: train loss: 0.309815
step 2600/400000: validation loss: 0.282660
epoch:56, batch26, load frames use: 5.46750092506s
step 2601/400000: train loss: 0.274343
epoch:56, batch27, load frames use: 3.95673012733s
step 2602/400000: train loss: 0.296577
epoch:56, batch28, load frames use: 6.02033019066s
step 2603/400000: train loss: 0.312130
epoch:56, batch29, load frames use: 4.04107999802s
step 2604/400000: train loss: 0.303192
epoch:56, batch30, load frames use: 7.30711388588s
step 2605/400000: train loss: 0.286388
epoch:56, batch31, load frames use: 6.47958397865s
step 2606/400000: train loss: 0.291677
epoch:56, batch32, load frames use: 6.40787911415s
step 2607/400000: train loss: 0.264088
epoch:56, batch33, load frames use: 5.8506090641s
step 2608/400000: train loss: 0.301252
epoch:56, batch34, load frames use: 6.34868097305s
step 2609/400000: train loss: 0.256432
epoch:56, batch35, load frames use: 7.28544998169s
step 2610/400000: train loss: 0.295372
epoch:56, batch36, load frames use: 4.95520401001s
step 2611/400000: train loss: 0.287616
epoch:56, batch37, load frames use: 6.35962510109s
step 2612/400000: train loss: 0.247223
epoch:56, batch38, load frames use: 7.82181501389s
step 2613/400000: train loss: 0.263857
epoch:56, batch39, load frames use: 7.67847394943s
step 2614/400000: train loss: 0.282722
epoch:56, batch40, load frames use: 12.7275509834s
step 2615/400000: train loss: 0.277051
epoch:56, batch41, load frames use: 5.74049806595s
step 2616/400000: train loss: 0.282030
epoch:56, batch42, load frames use: 7.13611698151s
step 2617/400000: train loss: 0.267840
epoch:56, batch43, load frames use: 12.4416279793s
step 2618/400000: train loss: 0.271478
epoch:56, batch44, load frames use: 4.60809993744s
step 2619/400000: train loss: 0.259821
epoch:56, batch45, load frames use: 9.6693110466s
step 2620/400000: train loss: 0.315375
epoch:56, batch46, load frames use: 13.4648520947s
step 2621/400000: train loss: 0.257295
epoch:57, batch1, load frames use: 7.78834295273s
step 2622/400000: train loss: 0.290082
epoch:57, batch2, load frames use: 10.9384200573s
step 2623/400000: train loss: 0.274330
epoch:57, batch3, load frames use: 10.8402740955s
step 2624/400000: train loss: 0.249288
epoch:57, batch4, load frames use: 5.11635088921s
step 2625/400000: train loss: 0.252790
epoch:57, batch5, load frames use: 9.12704110146s
step 2626/400000: train loss: 0.273215
epoch:57, batch6, load frames use: 7.81180286407s
step 2627/400000: train loss: 0.301368
epoch:57, batch7, load frames use: 9.92676401138s
step 2628/400000: train loss: 0.284478
epoch:57, batch8, load frames use: 10.6141400337s
step 2629/400000: train loss: 0.292365
epoch:57, batch9, load frames use: 4.48785591125s
step 2630/400000: train loss: 0.294616
epoch:57, batch10, load frames use: 5.39021515846s
step 2631/400000: train loss: 0.295645
epoch:57, batch11, load frames use: 10.0631129742s
step 2632/400000: train loss: 0.290473
epoch:57, batch12, load frames use: 10.0796749592s
step 2633/400000: train loss: 0.292284
epoch:57, batch13, load frames use: 8.4214220047s
step 2634/400000: train loss: 0.316840
epoch:57, batch14, load frames use: 11.9907619953s
step 2635/400000: train loss: 0.227602
epoch:57, batch15, load frames use: 10.1286668777s
step 2636/400000: train loss: 0.275370
epoch:57, batch16, load frames use: 5.97842407227s
step 2637/400000: train loss: 0.280293
epoch:57, batch17, load frames use: 9.53828191757s
step 2638/400000: train loss: 0.249645
epoch:57, batch18, load frames use: 13.2225060463s
step 2639/400000: train loss: 0.255551
epoch:57, batch19, load frames use: 12.221280098s
step 2640/400000: train loss: 0.249568
epoch:57, batch20, load frames use: 15.6811561584s
step 2641/400000: train loss: 0.283579
epoch:57, batch21, load frames use: 10.7965149879s
step 2642/400000: train loss: 0.252961
epoch:57, batch22, load frames use: 10.8949680328s
step 2643/400000: train loss: 0.289478
epoch:57, batch23, load frames use: 7.11305117607s
step 2644/400000: train loss: 0.277748
epoch:57, batch24, load frames use: 9.33723402023s
step 2645/400000: train loss: 0.293020
epoch:57, batch25, load frames use: 11.6836209297s
step 2646/400000: train loss: 0.259787
epoch:57, batch26, load frames use: 5.10267996788s
step 2647/400000: train loss: 0.271007
epoch:57, batch27, load frames use: 3.79737186432s
step 2648/400000: train loss: 0.346617
epoch:57, batch28, load frames use: 9.7160410881s
step 2649/400000: train loss: 0.283500
epoch:57, batch29, load frames use: 10.9343819618s
step 2650/400000: train loss: 0.280012
epoch:57, batch30, load frames use: 9.13236689568s
step 2651/400000: train loss: 0.296073
epoch:57, batch31, load frames use: 10.5063438416s
step 2652/400000: train loss: 0.319105
epoch:57, batch32, load frames use: 12.6244668961s
step 2653/400000: train loss: 0.265663
epoch:57, batch33, load frames use: 4.87327694893s
step 2654/400000: train loss: 0.326373
epoch:57, batch34, load frames use: 3.81949090958s
step 2655/400000: train loss: 0.275872
epoch:57, batch35, load frames use: 8.37638115883s
step 2656/400000: train loss: 0.294697
epoch:57, batch36, load frames use: 9.26178002357s
step 2657/400000: train loss: 0.297036
epoch:57, batch37, load frames use: 8.28063583374s
step 2658/400000: train loss: 0.255050
epoch:57, batch38, load frames use: 8.02134919167s
step 2659/400000: train loss: 0.249808
epoch:57, batch39, load frames use: 9.47434592247s
step 2660/400000: train loss: 0.274599
epoch:57, batch40, load frames use: 7.93556404114s
step 2661/400000: train loss: 0.286507
epoch:57, batch41, load frames use: 4.11800694466s
step 2662/400000: train loss: 0.296042
epoch:57, batch42, load frames use: 3.78871011734s
step 2663/400000: train loss: 0.249341
epoch:57, batch43, load frames use: 8.46390104294s
step 2664/400000: train loss: 0.310411
epoch:57, batch44, load frames use: 9.16377401352s
step 2665/400000: train loss: 0.276847
epoch:57, batch45, load frames use: 10.8473649025s
step 2666/400000: train loss: 0.281084
epoch:57, batch46, load frames use: 11.477960825s
step 2667/400000: train loss: 0.323968
epoch:58, batch1, load frames use: 10.9138338566s
step 2668/400000: train loss: 0.270916
epoch:58, batch2, load frames use: 11.8966200352s
step 2669/400000: train loss: 0.252042
epoch:58, batch3, load frames use: 10.3423171043s
step 2670/400000: train loss: 0.261588
epoch:58, batch4, load frames use: 4.17248296738s
step 2671/400000: train loss: 0.271457
epoch:58, batch5, load frames use: 8.16770195961s
step 2672/400000: train loss: 0.271436
epoch:58, batch6, load frames use: 7.35164189339s
step 2673/400000: train loss: 0.296698
epoch:58, batch7, load frames use: 8.89318490028s
step 2674/400000: train loss: 0.265409
epoch:58, batch8, load frames use: 10.945141077s
step 2675/400000: train loss: 0.284980
epoch:58, batch9, load frames use: 10.6932749748s
step 2676/400000: train loss: 0.280613
epoch:58, batch10, load frames use: 8.0696041584s
step 2677/400000: train loss: 0.249668
epoch:58, batch11, load frames use: 4.58757305145s
step 2678/400000: train loss: 0.258389
epoch:58, batch12, load frames use: 9.0448319912s
step 2679/400000: train loss: 0.288037
epoch:58, batch13, load frames use: 8.04169106483s
step 2680/400000: train loss: 0.292428
epoch:58, batch14, load frames use: 5.42361593246s
step 2681/400000: train loss: 0.290360
epoch:58, batch15, load frames use: 7.33921694756s
step 2682/400000: train loss: 0.289887
epoch:58, batch16, load frames use: 3.72760391235s
step 2683/400000: train loss: 0.277269
epoch:58, batch17, load frames use: 8.67191100121s
step 2684/400000: train loss: 0.307919
epoch:58, batch18, load frames use: 3.59842991829s
step 2685/400000: train loss: 0.284243
epoch:58, batch19, load frames use: 7.92969489098s
step 2686/400000: train loss: 0.273975
epoch:58, batch20, load frames use: 8.40824508667s
step 2687/400000: train loss: 0.298707
epoch:58, batch21, load frames use: 8.12510919571s
step 2688/400000: train loss: 0.272256
epoch:58, batch22, load frames use: 3.57662510872s
step 2689/400000: train loss: 0.248474
epoch:58, batch23, load frames use: 8.97842502594s
step 2690/400000: train loss: 0.281173
epoch:58, batch24, load frames use: 6.81813812256s
step 2691/400000: train loss: 0.255287
epoch:58, batch25, load frames use: 6.49850392342s
step 2692/400000: train loss: 0.280498
epoch:58, batch26, load frames use: 8.68025493622s
step 2693/400000: train loss: 0.282525
epoch:58, batch27, load frames use: 7.73182415962s
step 2694/400000: train loss: 0.263512
epoch:58, batch28, load frames use: 6.4213449955s
step 2695/400000: train loss: 0.281019
epoch:58, batch29, load frames use: 5.63647699356s
step 2696/400000: train loss: 0.200047
epoch:58, batch30, load frames use: 8.45355081558s
step 2697/400000: train loss: 0.281768
epoch:58, batch31, load frames use: 9.40792798996s
step 2698/400000: train loss: 0.274869
epoch:58, batch32, load frames use: 7.19070696831s
step 2699/400000: train loss: 0.230034
epoch:58, batch33, load frames use: 5.98210096359s
step 2700/400000: train loss: 0.291190
step 2700/400000: validation loss: 0.245691
epoch:58, batch34, load frames use: 4.25196099281s
step 2701/400000: train loss: 0.299149
epoch:58, batch35, load frames use: 6.50895404816s
step 2702/400000: train loss: 0.305683
epoch:58, batch36, load frames use: 8.15681910515s
step 2703/400000: train loss: 0.277823
epoch:58, batch37, load frames use: 6.21133494377s
step 2704/400000: train loss: 0.289164
epoch:58, batch38, load frames use: 6.92008399963s
step 2705/400000: train loss: 0.261917
epoch:58, batch39, load frames use: 5.73574399948s
step 2706/400000: train loss: 0.257032
epoch:58, batch40, load frames use: 5.61276292801s
step 2707/400000: train loss: 0.248621
epoch:58, batch41, load frames use: 6.63636803627s
step 2708/400000: train loss: 0.258307
epoch:58, batch42, load frames use: 6.46484780312s
step 2709/400000: train loss: 0.305468
epoch:58, batch43, load frames use: 7.32804584503s
step 2710/400000: train loss: 0.266141
epoch:58, batch44, load frames use: 4.93320393562s
step 2711/400000: train loss: 0.285256
epoch:58, batch45, load frames use: 5.20057606697s
step 2712/400000: train loss: 0.265787
epoch:58, batch46, load frames use: 5.33921194077s
step 2713/400000: train loss: 0.282333
epoch:59, batch1, load frames use: 5.99911308289s
step 2714/400000: train loss: 0.256172
epoch:59, batch2, load frames use: 6.23249316216s
step 2715/400000: train loss: 0.261409
epoch:59, batch3, load frames use: 5.18513703346s
step 2716/400000: train loss: 0.299512
epoch:59, batch4, load frames use: 5.55118298531s
step 2717/400000: train loss: 0.266439
epoch:59, batch5, load frames use: 5.39605402946s
step 2718/400000: train loss: 0.270686
epoch:59, batch6, load frames use: 6.05586600304s
step 2719/400000: train loss: 0.287262
epoch:59, batch7, load frames use: 5.68910598755s
step 2720/400000: train loss: 0.282608
epoch:59, batch8, load frames use: 3.16467881203s
step 2721/400000: train loss: 0.287543
epoch:59, batch9, load frames use: 5.33533716202s
step 2722/400000: train loss: 0.265691
epoch:59, batch10, load frames use: 4.46535921097s
step 2723/400000: train loss: 0.267487
epoch:59, batch11, load frames use: 5.21345496178s
step 2724/400000: train loss: 0.270127
epoch:59, batch12, load frames use: 5.04995012283s
step 2725/400000: train loss: 0.315378
epoch:59, batch13, load frames use: 5.23377585411s
step 2726/400000: train loss: 0.229697
epoch:59, batch14, load frames use: 4.83180499077s
step 2727/400000: train loss: 0.262773
epoch:59, batch15, load frames use: 7.67211794853s
step 2728/400000: train loss: 0.278060
epoch:59, batch16, load frames use: 3.66235899925s
step 2729/400000: train loss: 0.270620
epoch:59, batch17, load frames use: 4.59257388115s
step 2730/400000: train loss: 0.256401
epoch:59, batch18, load frames use: 4.5727159977s
step 2731/400000: train loss: 0.263795
epoch:59, batch19, load frames use: 4.96053695679s
step 2732/400000: train loss: 0.257835
epoch:59, batch20, load frames use: 5.7440829277s
step 2733/400000: train loss: 0.283858
epoch:59, batch21, load frames use: 3.50712585449s
step 2734/400000: train loss: 0.288021
epoch:59, batch22, load frames use: 5.24060511589s
step 2735/400000: train loss: 0.244296
epoch:59, batch23, load frames use: 3.93014121056s
step 2736/400000: train loss: 0.286213
epoch:59, batch24, load frames use: 4.72855305672s
step 2737/400000: train loss: 0.248168
epoch:59, batch25, load frames use: 4.78432202339s
step 2738/400000: train loss: 0.265613
epoch:59, batch26, load frames use: 4.00969910622s
step 2739/400000: train loss: 0.277653
epoch:59, batch27, load frames use: 3.4039170742s
step 2740/400000: train loss: 0.275693
epoch:59, batch28, load frames use: 3.98886108398s
step 2741/400000: train loss: 0.248464
epoch:59, batch29, load frames use: 3.60660600662s
step 2742/400000: train loss: 0.300732
epoch:59, batch30, load frames use: 3.47046899796s
step 2743/400000: train loss: 0.271565
epoch:59, batch31, load frames use: 3.8839328289s
step 2744/400000: train loss: 0.278153
epoch:59, batch32, load frames use: 3.5990228653s
step 2745/400000: train loss: 0.287415
epoch:59, batch33, load frames use: 4.41298103333s
step 2746/400000: train loss: 0.263354
epoch:59, batch34, load frames use: 5.95273089409s
step 2747/400000: train loss: 0.246821
epoch:59, batch35, load frames use: 4.02092885971s
step 2748/400000: train loss: 0.284797
epoch:59, batch36, load frames use: 6.01558995247s
step 2749/400000: train loss: 0.329327
epoch:59, batch37, load frames use: 4.44399189949s
step 2750/400000: train loss: 0.293287
epoch:59, batch38, load frames use: 5.09747219086s
step 2751/400000: train loss: 0.247960
epoch:59, batch39, load frames use: 3.56225991249s
step 2752/400000: train loss: 0.259698
epoch:59, batch40, load frames use: 4.69230699539s
step 2753/400000: train loss: 0.288847
epoch:59, batch41, load frames use: 4.72568297386s
step 2754/400000: train loss: 0.274360
epoch:59, batch42, load frames use: 4.40731811523s
step 2755/400000: train loss: 0.258367
epoch:59, batch43, load frames use: 4.79723310471s
step 2756/400000: train loss: 0.272468
epoch:59, batch44, load frames use: 4.81405591965s
step 2757/400000: train loss: 0.289675
epoch:59, batch45, load frames use: 4.36268901825s
step 2758/400000: train loss: 0.280836
epoch:59, batch46, load frames use: 3.97396397591s
step 2759/400000: train loss: 0.289642
epoch:60, batch1, load frames use: 4.70820999146s
step 2760/400000: train loss: 0.267773
epoch:60, batch2, load frames use: 4.11554408073s
step 2761/400000: train loss: 0.280446
epoch:60, batch3, load frames use: 4.79924106598s
step 2762/400000: train loss: 0.231481
epoch:60, batch4, load frames use: 5.73110699654s
step 2763/400000: train loss: 0.247497
epoch:60, batch5, load frames use: 3.84809494019s
step 2764/400000: train loss: 0.242457
epoch:60, batch6, load frames use: 3.29165410995s
step 2765/400000: train loss: 0.232376
epoch:60, batch7, load frames use: 3.34669184685s
step 2766/400000: train loss: 0.284187
epoch:60, batch8, load frames use: 3.45611715317s
step 2767/400000: train loss: 0.275798
epoch:60, batch9, load frames use: 4.48935389519s
step 2768/400000: train loss: 0.239411
epoch:60, batch10, load frames use: 4.07509112358s
step 2769/400000: train loss: 0.274194
epoch:60, batch11, load frames use: 3.67062592506s
step 2770/400000: train loss: 0.244481
epoch:60, batch12, load frames use: 4.0667321682s
step 2771/400000: train loss: 0.273579
epoch:60, batch13, load frames use: 6.15090298653s
step 2772/400000: train loss: 0.233481
epoch:60, batch14, load frames use: 5.34658098221s
step 2773/400000: train loss: 0.268674
epoch:60, batch15, load frames use: 3.99166798592s
step 2774/400000: train loss: 0.246224
epoch:60, batch16, load frames use: 4.40939283371s
step 2775/400000: train loss: 0.227374
epoch:60, batch17, load frames use: 4.95825695992s
step 2776/400000: train loss: 0.285416
epoch:60, batch18, load frames use: 4.13771414757s
step 2777/400000: train loss: 0.236995
epoch:60, batch19, load frames use: 5.00991082191s
step 2778/400000: train loss: 0.274191
epoch:60, batch20, load frames use: 6.17506814003s
step 2779/400000: train loss: 0.256807
epoch:60, batch21, load frames use: 6.1104581356s
step 2780/400000: train loss: 0.238930
epoch:60, batch22, load frames use: 6.02590799332s
step 2781/400000: train loss: 0.265087
epoch:60, batch23, load frames use: 3.40882611275s
step 2782/400000: train loss: 0.282355
epoch:60, batch24, load frames use: 4.17092680931s
step 2783/400000: train loss: 0.269783
epoch:60, batch25, load frames use: 4.12673807144s
step 2784/400000: train loss: 0.297842
epoch:60, batch26, load frames use: 5.2908039093s
step 2785/400000: train loss: 0.232454
epoch:60, batch27, load frames use: 5.96647191048s
step 2786/400000: train loss: 0.277080
epoch:60, batch28, load frames use: 3.90319299698s
step 2787/400000: train loss: 0.269748
epoch:60, batch29, load frames use: 4.88632702827s
step 2788/400000: train loss: 0.242757
epoch:60, batch30, load frames use: 4.36801099777s
step 2789/400000: train loss: 0.260151
epoch:60, batch31, load frames use: 4.94253706932s
step 2790/400000: train loss: 0.289300
epoch:60, batch32, load frames use: 5.63388085365s
step 2791/400000: train loss: 0.261001
epoch:60, batch33, load frames use: 3.56189703941s
step 2792/400000: train loss: 0.286819
epoch:60, batch34, load frames use: 3.21698617935s
step 2793/400000: train loss: 0.264030
epoch:60, batch35, load frames use: 4.25291490555s
step 2794/400000: train loss: 0.248815
epoch:60, batch36, load frames use: 4.41772508621s
step 2795/400000: train loss: 0.257801
epoch:60, batch37, load frames use: 3.40569615364s
step 2796/400000: train loss: 0.266725
epoch:60, batch38, load frames use: 4.6376209259s
step 2797/400000: train loss: 0.262688
epoch:60, batch39, load frames use: 6.81591296196s
step 2798/400000: train loss: 0.259239
epoch:60, batch40, load frames use: 5.180341959s
step 2799/400000: train loss: 0.232545
epoch:60, batch41, load frames use: 4.16013813019s
step 2800/400000: train loss: 0.275280
step 2800/400000: validation loss: 0.256798
epoch:60, batch42, load frames use: 3.58334088326s
step 2801/400000: train loss: 0.267008
epoch:60, batch43, load frames use: 4.87760591507s
step 2802/400000: train loss: 0.285690
epoch:60, batch44, load frames use: 5.37693405151s
step 2803/400000: train loss: 0.278950
epoch:60, batch45, load frames use: 3.59953093529s
step 2804/400000: train loss: 0.268148
epoch:60, batch46, load frames use: 3.94914507866s
step 2805/400000: train loss: 0.275972
epoch:61, batch1, load frames use: 4.73554611206s
step 2806/400000: train loss: 0.252299
epoch:61, batch2, load frames use: 3.22440695763s
step 2807/400000: train loss: 0.271037
epoch:61, batch3, load frames use: 5.40962791443s
step 2808/400000: train loss: 0.296577
epoch:61, batch4, load frames use: 4.74285101891s
step 2809/400000: train loss: 0.241125
epoch:61, batch5, load frames use: 4.49231004715s
step 2810/400000: train loss: 0.242211
epoch:61, batch6, load frames use: 5.06178808212s
step 2811/400000: train loss: 0.268151
epoch:61, batch7, load frames use: 6.07508015633s
step 2812/400000: train loss: 0.273893
epoch:61, batch8, load frames use: 6.77191996574s
step 2813/400000: train loss: 0.262188
epoch:61, batch9, load frames use: 3.96106791496s
step 2814/400000: train loss: 0.277261
epoch:61, batch10, load frames use: 7.02325105667s
step 2815/400000: train loss: 0.268647
epoch:61, batch11, load frames use: 3.39578008652s
step 2816/400000: train loss: 0.291470
epoch:61, batch12, load frames use: 6.88400506973s
step 2817/400000: train loss: 0.282010
epoch:61, batch13, load frames use: 3.83155989647s
step 2818/400000: train loss: 0.250088
epoch:61, batch14, load frames use: 3.66920495033s
step 2819/400000: train loss: 0.249140
epoch:61, batch15, load frames use: 4.2257540226s
step 2820/400000: train loss: 0.278507
epoch:61, batch16, load frames use: 4.90493083s
step 2821/400000: train loss: 0.289070
epoch:61, batch17, load frames use: 3.60579109192s
step 2822/400000: train loss: 0.255779
epoch:61, batch18, load frames use: 7.80328416824s
step 2823/400000: train loss: 0.265848
epoch:61, batch19, load frames use: 3.36071109772s
step 2824/400000: train loss: 0.263060
epoch:61, batch20, load frames use: 3.64031386375s
step 2825/400000: train loss: 0.285828
epoch:61, batch21, load frames use: 6.8215970993s
step 2826/400000: train loss: 0.247738
epoch:61, batch22, load frames use: 5.14505004883s
step 2827/400000: train loss: 0.275313
epoch:61, batch23, load frames use: 7.00004696846s
step 2828/400000: train loss: 0.261607
epoch:61, batch24, load frames use: 5.02035498619s
step 2829/400000: train loss: 0.279924
epoch:61, batch25, load frames use: 4.88359999657s
step 2830/400000: train loss: 0.278237
epoch:61, batch26, load frames use: 4.54264116287s
step 2831/400000: train loss: 0.245975
epoch:61, batch27, load frames use: 3.72546505928s
step 2832/400000: train loss: 0.264540
epoch:61, batch28, load frames use: 3.72714185715s
step 2833/400000: train loss: 0.251705
epoch:61, batch29, load frames use: 4.25402212143s
step 2834/400000: train loss: 0.247478
epoch:61, batch30, load frames use: 3.38393497467s
step 2835/400000: train loss: 0.299593
epoch:61, batch31, load frames use: 3.39271998405s
step 2836/400000: train loss: 0.270011
epoch:61, batch32, load frames use: 4.04663395882s
step 2837/400000: train loss: 0.304858
epoch:61, batch33, load frames use: 4.20664215088s
step 2838/400000: train loss: 0.269687
epoch:61, batch34, load frames use: 5.19250202179s
step 2839/400000: train loss: 0.258097
epoch:61, batch35, load frames use: 7.22781586647s
step 2840/400000: train loss: 0.262083
epoch:61, batch36, load frames use: 3.8101580143s
step 2841/400000: train loss: 0.272294
epoch:61, batch37, load frames use: 4.49233198166s
step 2842/400000: train loss: 0.282682
epoch:61, batch38, load frames use: 4.73041391373s
step 2843/400000: train loss: 0.260163
epoch:61, batch39, load frames use: 6.48344397545s
step 2844/400000: train loss: 0.268769
epoch:61, batch40, load frames use: 6.83381390572s
step 2845/400000: train loss: 0.255547
epoch:61, batch41, load frames use: 3.172757864s
step 2846/400000: train loss: 0.273032
epoch:61, batch42, load frames use: 4.63748598099s
step 2847/400000: train loss: 0.317075
epoch:61, batch43, load frames use: 3.76731610298s
step 2848/400000: train loss: 0.316382
epoch:61, batch44, load frames use: 4.49569797516s
step 2849/400000: train loss: 0.238188
epoch:61, batch45, load frames use: 4.13480520248s
step 2850/400000: train loss: 0.234088
epoch:61, batch46, load frames use: 3.78556895256s
step 2851/400000: train loss: 0.305364
epoch:62, batch1, load frames use: 5.24728393555s
step 2852/400000: train loss: 0.249449
epoch:62, batch2, load frames use: 4.63649892807s
step 2853/400000: train loss: 0.239747
epoch:62, batch3, load frames use: 4.8818898201s
step 2854/400000: train loss: 0.311102
epoch:62, batch4, load frames use: 3.72804307938s
step 2855/400000: train loss: 0.271903
epoch:62, batch5, load frames use: 3.36923694611s
step 2856/400000: train loss: 0.251297
epoch:62, batch6, load frames use: 4.04980611801s
step 2857/400000: train loss: 0.310575
epoch:62, batch7, load frames use: 6.87885403633s
step 2858/400000: train loss: 0.267112
epoch:62, batch8, load frames use: 6.75438094139s
step 2859/400000: train loss: 0.242859
epoch:62, batch9, load frames use: 3.46360015869s
step 2860/400000: train loss: 0.265614
epoch:62, batch10, load frames use: 5.18647313118s
step 2861/400000: train loss: 0.265506
epoch:62, batch11, load frames use: 3.63351607323s
step 2862/400000: train loss: 0.293496
epoch:62, batch12, load frames use: 3.02640795708s
step 2863/400000: train loss: 0.300581
epoch:62, batch13, load frames use: 3.9581720829s
step 2864/400000: train loss: 0.320366
epoch:62, batch14, load frames use: 6.38319993019s
step 2865/400000: train loss: 0.264969
epoch:62, batch15, load frames use: 4.4269361496s
step 2866/400000: train loss: 0.263229
epoch:62, batch16, load frames use: 3.39590096474s
step 2867/400000: train loss: 0.249045
epoch:62, batch17, load frames use: 4.69946908951s
step 2868/400000: train loss: 0.257141
epoch:62, batch18, load frames use: 4.81691098213s
step 2869/400000: train loss: 0.312031
epoch:62, batch19, load frames use: 4.20568299294s
step 2870/400000: train loss: 0.252256
epoch:62, batch20, load frames use: 5.06228399277s
step 2871/400000: train loss: 0.274343
epoch:62, batch21, load frames use: 5.42967796326s
step 2872/400000: train loss: 0.282669
epoch:62, batch22, load frames use: 4.63284611702s
step 2873/400000: train loss: 0.267206
epoch:62, batch23, load frames use: 4.80944895744s
step 2874/400000: train loss: 0.243784
epoch:62, batch24, load frames use: 4.60300111771s
step 2875/400000: train loss: 0.249534
epoch:62, batch25, load frames use: 4.29365801811s
step 2876/400000: train loss: 0.295795
epoch:62, batch26, load frames use: 4.33540391922s
step 2877/400000: train loss: 0.285509
epoch:62, batch27, load frames use: 4.62138295174s
step 2878/400000: train loss: 0.316044
epoch:62, batch28, load frames use: 5.34440898895s
step 2879/400000: train loss: 0.289803
epoch:62, batch29, load frames use: 4.35385608673s
step 2880/400000: train loss: 0.265465
epoch:62, batch30, load frames use: 4.44169616699s
step 2881/400000: train loss: 0.265641
epoch:62, batch31, load frames use: 5.11747002602s
step 2882/400000: train loss: 0.270028
epoch:62, batch32, load frames use: 5.10965585709s
step 2883/400000: train loss: 0.269788
epoch:62, batch33, load frames use: 5.65824794769s
step 2884/400000: train loss: 0.305996
epoch:62, batch34, load frames use: 4.36378908157s
step 2885/400000: train loss: 0.236547
epoch:62, batch35, load frames use: 4.14561486244s
step 2886/400000: train loss: 0.253642
epoch:62, batch36, load frames use: 5.63588786125s
step 2887/400000: train loss: 0.243531
epoch:62, batch37, load frames use: 3.98583102226s
step 2888/400000: train loss: 0.255106
epoch:62, batch38, load frames use: 3.92829799652s
step 2889/400000: train loss: 0.276906
epoch:62, batch39, load frames use: 4.86427307129s
step 2890/400000: train loss: 0.260897
epoch:62, batch40, load frames use: 3.39628410339s
step 2891/400000: train loss: 0.253378
epoch:62, batch41, load frames use: 3.2067220211s
step 2892/400000: train loss: 0.286152
epoch:62, batch42, load frames use: 3.43151402473s
step 2893/400000: train loss: 0.223707
epoch:62, batch43, load frames use: 4.94978094101s
step 2894/400000: train loss: 0.285938
epoch:62, batch44, load frames use: 4.56959795952s
step 2895/400000: train loss: 0.277991
epoch:62, batch45, load frames use: 6.7407450676s
step 2896/400000: train loss: 0.278132
epoch:62, batch46, load frames use: 4.70461893082s
step 2897/400000: train loss: 0.261855
epoch:63, batch1, load frames use: 3.51655912399s
step 2898/400000: train loss: 0.258556
epoch:63, batch2, load frames use: 3.57821393013s
step 2899/400000: train loss: 0.222742
epoch:63, batch3, load frames use: 3.33688521385s
step 2900/400000: train loss: 0.249823
step 2900/400000: validation loss: 0.250329
epoch:63, batch4, load frames use: 4.42170882225s
step 2901/400000: train loss: 0.249464
epoch:63, batch5, load frames use: 3.22627401352s
step 2902/400000: train loss: 0.264397
epoch:63, batch6, load frames use: 4.69625401497s
step 2903/400000: train loss: 0.231982
epoch:63, batch7, load frames use: 5.95074677467s
step 2904/400000: train loss: 0.272857
epoch:63, batch8, load frames use: 3.50579810143s
step 2905/400000: train loss: 0.245561
epoch:63, batch9, load frames use: 3.62710690498s
step 2906/400000: train loss: 0.249838
epoch:63, batch10, load frames use: 3.37520694733s
step 2907/400000: train loss: 0.260323
epoch:63, batch11, load frames use: 4.88578081131s
step 2908/400000: train loss: 0.244345
epoch:63, batch12, load frames use: 5.18677902222s
step 2909/400000: train loss: 0.277796
epoch:63, batch13, load frames use: 6.88275289536s
step 2910/400000: train loss: 0.271846
epoch:63, batch14, load frames use: 3.25855612755s
step 2911/400000: train loss: 0.244101
epoch:63, batch15, load frames use: 4.70529508591s
step 2912/400000: train loss: 0.261634
epoch:63, batch16, load frames use: 3.83210515976s
step 2913/400000: train loss: 0.239218
epoch:63, batch17, load frames use: 6.20373606682s
step 2914/400000: train loss: 0.240513
epoch:63, batch18, load frames use: 4.22887706757s
step 2915/400000: train loss: 0.263669
epoch:63, batch19, load frames use: 4.05684614182s
step 2916/400000: train loss: 0.277494
epoch:63, batch20, load frames use: 3.6975440979s
step 2917/400000: train loss: 0.248734
epoch:63, batch21, load frames use: 3.52588582039s
step 2918/400000: train loss: 0.264207
epoch:63, batch22, load frames use: 3.1901679039s
step 2919/400000: train loss: 0.242495
epoch:63, batch23, load frames use: 3.21772289276s
step 2920/400000: train loss: 0.306670
epoch:63, batch24, load frames use: 4.52470779419s
step 2921/400000: train loss: 0.261353
epoch:63, batch25, load frames use: 5.00273108482s
step 2922/400000: train loss: 0.288871
epoch:63, batch26, load frames use: 3.43066596985s
step 2923/400000: train loss: 0.284569
epoch:63, batch27, load frames use: 3.85064816475s
step 2924/400000: train loss: 0.293259
epoch:63, batch28, load frames use: 5.27489089966s
step 2925/400000: train loss: 0.315552
epoch:63, batch29, load frames use: 4.08215093613s
step 2926/400000: train loss: 0.255587
epoch:63, batch30, load frames use: 4.12823200226s
step 2927/400000: train loss: 0.243651
epoch:63, batch31, load frames use: 3.47658991814s
step 2928/400000: train loss: 0.247846
epoch:63, batch32, load frames use: 4.61401200294s
step 2929/400000: train loss: 0.284971
epoch:63, batch33, load frames use: 5.21009802818s
step 2930/400000: train loss: 0.250958
epoch:63, batch34, load frames use: 4.68637704849s
step 2931/400000: train loss: 0.274222
epoch:63, batch35, load frames use: 4.10537314415s
step 2932/400000: train loss: 0.262058
epoch:63, batch36, load frames use: 4.24323415756s
step 2933/400000: train loss: 0.278976
epoch:63, batch37, load frames use: 4.04414701462s
step 2934/400000: train loss: 0.288768
epoch:63, batch38, load frames use: 3.32578897476s
step 2935/400000: train loss: 0.230457
epoch:63, batch39, load frames use: 3.32543897629s
step 2936/400000: train loss: 0.278322
epoch:63, batch40, load frames use: 4.42280697823s
step 2937/400000: train loss: 0.257979
epoch:63, batch41, load frames use: 4.75034403801s
step 2938/400000: train loss: 0.253766
epoch:63, batch42, load frames use: 4.75749397278s
step 2939/400000: train loss: 0.270863
epoch:63, batch43, load frames use: 3.88317584991s
step 2940/400000: train loss: 0.299753
epoch:63, batch44, load frames use: 3.59010386467s
step 2941/400000: train loss: 0.292761
epoch:63, batch45, load frames use: 7.31579208374s
step 2942/400000: train loss: 0.262376
epoch:63, batch46, load frames use: 4.14417386055s
step 2943/400000: train loss: 0.267312
epoch:64, batch1, load frames use: 3.64012098312s
step 2944/400000: train loss: 0.238864
epoch:64, batch2, load frames use: 3.37129807472s
step 2945/400000: train loss: 0.254848
epoch:64, batch3, load frames use: 3.50310897827s
step 2946/400000: train loss: 0.256801
epoch:64, batch4, load frames use: 3.8273870945s
step 2947/400000: train loss: 0.253047
epoch:64, batch5, load frames use: 3.53928804398s
step 2948/400000: train loss: 0.231106
epoch:64, batch6, load frames use: 4.06410479546s
step 2949/400000: train loss: 0.282159
epoch:64, batch7, load frames use: 3.52016210556s
step 2950/400000: train loss: 0.223548
epoch:64, batch8, load frames use: 4.3325688839s
step 2951/400000: train loss: 0.268490
epoch:64, batch9, load frames use: 3.90247321129s
step 2952/400000: train loss: 0.290844
epoch:64, batch10, load frames use: 3.95599317551s
step 2953/400000: train loss: 0.251422
epoch:64, batch11, load frames use: 4.46006703377s
step 2954/400000: train loss: 0.255745
epoch:64, batch12, load frames use: 3.5749630928s
step 2955/400000: train loss: 0.221202
epoch:64, batch13, load frames use: 3.09563994408s
step 2956/400000: train loss: 0.253373
epoch:64, batch14, load frames use: 5.12058615685s
step 2957/400000: train loss: 0.240866
epoch:64, batch15, load frames use: 3.31622886658s
step 2958/400000: train loss: 0.294187
epoch:64, batch16, load frames use: 4.74770498276s
step 2959/400000: train loss: 0.253082
epoch:64, batch17, load frames use: 4.42033600807s
step 2960/400000: train loss: 0.295811
epoch:64, batch18, load frames use: 4.93775081635s
step 2961/400000: train loss: 0.242785
epoch:64, batch19, load frames use: 4.15984511375s
step 2962/400000: train loss: 0.259772
epoch:64, batch20, load frames use: 6.17860007286s
step 2963/400000: train loss: 0.279718
epoch:64, batch21, load frames use: 3.44608712196s
step 2964/400000: train loss: 0.234747
epoch:64, batch22, load frames use: 3.83046197891s
step 2965/400000: train loss: 0.255995
epoch:64, batch23, load frames use: 3.34624004364s
step 2966/400000: train loss: 0.277090
epoch:64, batch24, load frames use: 3.21624708176s
step 2967/400000: train loss: 0.234810
epoch:64, batch25, load frames use: 3.66578507423s
step 2968/400000: train loss: 0.257731
epoch:64, batch26, load frames use: 3.2802131176s
step 2969/400000: train loss: 0.250595
epoch:64, batch27, load frames use: 3.71790003777s
step 2970/400000: train loss: 0.234228
epoch:64, batch28, load frames use: 3.8226621151s
step 2971/400000: train loss: 0.280034
epoch:64, batch29, load frames use: 7.64440894127s
step 2972/400000: train loss: 0.246526
epoch:64, batch30, load frames use: 4.04912996292s
step 2973/400000: train loss: 0.235072
epoch:64, batch31, load frames use: 4.83656215668s
step 2974/400000: train loss: 0.300109
epoch:64, batch32, load frames use: 4.3873269558s
step 2975/400000: train loss: 0.259249
epoch:64, batch33, load frames use: 4.8437359333s
step 2976/400000: train loss: 0.248558
epoch:64, batch34, load frames use: 6.6527929306s
step 2977/400000: train loss: 0.266367
epoch:64, batch35, load frames use: 4.26446199417s
step 2978/400000: train loss: 0.226344
epoch:64, batch36, load frames use: 3.43746399879s
step 2979/400000: train loss: 0.243719
epoch:64, batch37, load frames use: 3.61094093323s
step 2980/400000: train loss: 0.301719
epoch:64, batch38, load frames use: 3.2977771759s
step 2981/400000: train loss: 0.266150
epoch:64, batch39, load frames use: 4.6071600914s
step 2982/400000: train loss: 0.249109
epoch:64, batch40, load frames use: 4.63873100281s
step 2983/400000: train loss: 0.304033
epoch:64, batch41, load frames use: 4.71108007431s
step 2984/400000: train loss: 0.264485
epoch:64, batch42, load frames use: 3.50927686691s
step 2985/400000: train loss: 0.267416
epoch:64, batch43, load frames use: 5.06125402451s
step 2986/400000: train loss: 0.258844
epoch:64, batch44, load frames use: 4.28024601936s
step 2987/400000: train loss: 0.259052
epoch:64, batch45, load frames use: 3.52612900734s
step 2988/400000: train loss: 0.286166
epoch:64, batch46, load frames use: 3.72716498375s
step 2989/400000: train loss: 0.280581
epoch:65, batch1, load frames use: 4.27780485153s
step 2990/400000: train loss: 0.237640
epoch:65, batch2, load frames use: 6.05263185501s
step 2991/400000: train loss: 0.246186
epoch:65, batch3, load frames use: 4.65569901466s
step 2992/400000: train loss: 0.249114
epoch:65, batch4, load frames use: 3.73221182823s
step 2993/400000: train loss: 0.250074
epoch:65, batch5, load frames use: 3.53133177757s
step 2994/400000: train loss: 0.235545
epoch:65, batch6, load frames use: 3.35340094566s
step 2995/400000: train loss: 0.231828
epoch:65, batch7, load frames use: 3.63570380211s
step 2996/400000: train loss: 0.251356
epoch:65, batch8, load frames use: 3.88677191734s
step 2997/400000: train loss: 0.261720
epoch:65, batch9, load frames use: 4.93832612038s
step 2998/400000: train loss: 0.241276
epoch:65, batch10, load frames use: 4.89714002609s
step 2999/400000: train loss: 0.245512
epoch:65, batch11, load frames use: 4.89959096909s
step 3000/400000: train loss: 0.242862
step 3000/400000: validation loss: 0.240692
epoch:65, batch12, load frames use: 4.2559440136s
step 3001/400000: train loss: 0.252080
epoch:65, batch13, load frames use: 4.56306004524s
step 3002/400000: train loss: 0.288889
epoch:65, batch14, load frames use: 3.55347108841s
step 3003/400000: train loss: 0.244990
epoch:65, batch15, load frames use: 3.59306502342s
step 3004/400000: train loss: 0.258283
epoch:65, batch16, load frames use: 3.14071297646s
step 3005/400000: train loss: 0.246720
epoch:65, batch17, load frames use: 4.21956801414s
step 3006/400000: train loss: 0.268615
epoch:65, batch18, load frames use: 4.06215286255s
step 3007/400000: train loss: 0.237346
epoch:65, batch19, load frames use: 3.83387112617s
step 3008/400000: train loss: 0.229476
epoch:65, batch20, load frames use: 3.52129793167s
step 3009/400000: train loss: 0.263744
epoch:65, batch21, load frames use: 3.61803483963s
step 3010/400000: train loss: 0.237678
epoch:65, batch22, load frames use: 3.66959786415s
step 3011/400000: train loss: 0.240414
epoch:65, batch23, load frames use: 3.81226301193s
step 3012/400000: train loss: 0.256193
epoch:65, batch24, load frames use: 3.37266302109s
step 3013/400000: train loss: 0.227849
epoch:65, batch25, load frames use: 3.69114804268s
step 3014/400000: train loss: 0.237819
epoch:65, batch26, load frames use: 3.21732902527s
step 3015/400000: train loss: 0.250393
epoch:65, batch27, load frames use: 4.07031822205s
step 3016/400000: train loss: 0.243889
epoch:65, batch28, load frames use: 3.54234790802s
step 3017/400000: train loss: 0.254254
epoch:65, batch29, load frames use: 3.543364048s
step 3018/400000: train loss: 0.259986
epoch:65, batch30, load frames use: 3.10566782951s
step 3019/400000: train loss: 0.257005
epoch:65, batch31, load frames use: 3.78930616379s
step 3020/400000: train loss: 0.252729
epoch:65, batch32, load frames use: 3.79013705254s
step 3021/400000: train loss: 0.261147
epoch:65, batch33, load frames use: 3.91215801239s
step 3022/400000: train loss: 0.246244
epoch:65, batch34, load frames use: 3.43389105797s
step 3023/400000: train loss: 0.263382
epoch:65, batch35, load frames use: 3.68275499344s
step 3024/400000: train loss: 0.313520
epoch:65, batch36, load frames use: 3.63090896606s
step 3025/400000: train loss: 0.281088
epoch:65, batch37, load frames use: 3.38258981705s
step 3026/400000: train loss: 0.253671
epoch:65, batch38, load frames use: 3.30013513565s
step 3027/400000: train loss: 0.285072
epoch:65, batch39, load frames use: 3.7155380249s
step 3028/400000: train loss: 0.247409
epoch:65, batch40, load frames use: 3.28255414963s
step 3029/400000: train loss: 0.265101
epoch:65, batch41, load frames use: 3.95719194412s
step 3030/400000: train loss: 0.242457
epoch:65, batch42, load frames use: 3.27304887772s
step 3031/400000: train loss: 0.253453
epoch:65, batch43, load frames use: 3.4942381382s
step 3032/400000: train loss: 0.290840
epoch:65, batch44, load frames use: 3.63124704361s
step 3033/400000: train loss: 0.272426
epoch:65, batch45, load frames use: 3.61419391632s
step 3034/400000: train loss: 0.246300
epoch:65, batch46, load frames use: 3.389950037s
step 3035/400000: train loss: 0.253188
epoch:66, batch1, load frames use: 3.49679803848s
step 3036/400000: train loss: 0.266728
epoch:66, batch2, load frames use: 3.9683110714s
step 3037/400000: train loss: 0.242704
epoch:66, batch3, load frames use: 3.33825182915s
step 3038/400000: train loss: 0.276510
epoch:66, batch4, load frames use: 3.29534697533s
step 3039/400000: train loss: 0.224450
epoch:66, batch5, load frames use: 3.70291304588s
step 3040/400000: train loss: 0.235349
epoch:66, batch6, load frames use: 3.72542691231s
step 3041/400000: train loss: 0.245049
epoch:66, batch7, load frames use: 3.4878590107s
step 3042/400000: train loss: 0.235145
epoch:66, batch8, load frames use: 3.76955318451s
step 3043/400000: train loss: 0.249136
epoch:66, batch9, load frames use: 3.34553098679s
step 3044/400000: train loss: 0.259670
epoch:66, batch10, load frames use: 3.46290493011s
step 3045/400000: train loss: 0.247632
epoch:66, batch11, load frames use: 3.39311885834s
step 3046/400000: train loss: 0.251338
epoch:66, batch12, load frames use: 3.57302618027s
step 3047/400000: train loss: 0.270898
epoch:66, batch13, load frames use: 5.4985461235s
step 3048/400000: train loss: 0.290039
epoch:66, batch14, load frames use: 6.43199491501s
step 3049/400000: train loss: 0.239242
epoch:66, batch15, load frames use: 4.21082901955s
step 3050/400000: train loss: 0.239087
epoch:66, batch16, load frames use: 3.46172904968s
step 3051/400000: train loss: 0.258324
epoch:66, batch17, load frames use: 4.49197697639s
step 3052/400000: train loss: 0.244813
epoch:66, batch18, load frames use: 6.39467692375s
step 3053/400000: train loss: 0.229195
epoch:66, batch19, load frames use: 3.86427092552s
step 3054/400000: train loss: 0.223882
epoch:66, batch20, load frames use: 4.95638298988s
step 3055/400000: train loss: 0.269613
epoch:66, batch21, load frames use: 4.20873308182s
step 3056/400000: train loss: 0.284090
epoch:66, batch22, load frames use: 4.23453688622s
step 3057/400000: train loss: 0.244113
epoch:66, batch23, load frames use: 4.37006282806s
step 3058/400000: train loss: 0.265818
epoch:66, batch24, load frames use: 3.41240310669s
step 3059/400000: train loss: 0.258753
epoch:66, batch25, load frames use: 3.13759994507s
step 3060/400000: train loss: 0.284495
epoch:66, batch26, load frames use: 3.9501721859s
step 3061/400000: train loss: 0.230805
epoch:66, batch27, load frames use: 3.64222598076s
step 3062/400000: train loss: 0.276134
epoch:66, batch28, load frames use: 3.54910683632s
step 3063/400000: train loss: 0.281357
epoch:66, batch29, load frames use: 3.71591591835s
step 3064/400000: train loss: 0.255736
epoch:66, batch30, load frames use: 3.40653014183s
step 3065/400000: train loss: 0.233993
epoch:66, batch31, load frames use: 3.83405590057s
step 3066/400000: train loss: 0.259985
epoch:66, batch32, load frames use: 3.48480510712s
step 3067/400000: train loss: 0.250393
epoch:66, batch33, load frames use: 3.84911608696s
step 3068/400000: train loss: 0.272057
epoch:66, batch34, load frames use: 3.69419813156s
step 3069/400000: train loss: 0.286849
epoch:66, batch35, load frames use: 4.47922611237s
step 3070/400000: train loss: 0.304053
epoch:66, batch36, load frames use: 3.54927706718s
step 3071/400000: train loss: 0.303091
epoch:66, batch37, load frames use: 3.51912093163s
step 3072/400000: train loss: 0.226562
epoch:66, batch38, load frames use: 3.6905400753s
step 3073/400000: train loss: 0.301498
epoch:66, batch39, load frames use: 4.23917102814s
step 3074/400000: train loss: 0.275555
epoch:66, batch40, load frames use: 3.77081203461s
step 3075/400000: train loss: 0.248877
epoch:66, batch41, load frames use: 3.39163899422s
step 3076/400000: train loss: 0.286586
epoch:66, batch42, load frames use: 3.95955896378s
step 3077/400000: train loss: 0.257227
epoch:66, batch43, load frames use: 4.72361516953s
step 3078/400000: train loss: 0.247044
epoch:66, batch44, load frames use: 3.68920397758s
step 3079/400000: train loss: 0.246347
epoch:66, batch45, load frames use: 3.33945894241s
step 3080/400000: train loss: 0.274145
epoch:66, batch46, load frames use: 3.87479901314s
step 3081/400000: train loss: 0.280196
epoch:67, batch1, load frames use: 4.13971090317s
step 3082/400000: train loss: 0.252283
epoch:67, batch2, load frames use: 4.13969492912s
step 3083/400000: train loss: 0.235132
epoch:67, batch3, load frames use: 3.84017300606s
step 3084/400000: train loss: 0.277371
epoch:67, batch4, load frames use: 3.46251320839s
step 3085/400000: train loss: 0.303883
epoch:67, batch5, load frames use: 3.802257061s
step 3086/400000: train loss: 0.247988
epoch:67, batch6, load frames use: 3.82359695435s
step 3087/400000: train loss: 0.269063
epoch:67, batch7, load frames use: 3.33615899086s
step 3088/400000: train loss: 0.255228
epoch:67, batch8, load frames use: 3.44261598587s
step 3089/400000: train loss: 0.268405
epoch:67, batch9, load frames use: 3.17285203934s
step 3090/400000: train loss: 0.251908
epoch:67, batch10, load frames use: 3.80433082581s
step 3091/400000: train loss: 0.262909
epoch:67, batch11, load frames use: 3.30235815048s
step 3092/400000: train loss: 0.251531
epoch:67, batch12, load frames use: 4.13766908646s
step 3093/400000: train loss: 0.234629
epoch:67, batch13, load frames use: 3.71116805077s
step 3094/400000: train loss: 0.251438
epoch:67, batch14, load frames use: 3.38874793053s
step 3095/400000: train loss: 0.250640
epoch:67, batch15, load frames use: 3.42179703712s
step 3096/400000: train loss: 0.247160
epoch:67, batch16, load frames use: 3.84304904938s
step 3097/400000: train loss: 0.216394
epoch:67, batch17, load frames use: 3.00950288773s
step 3098/400000: train loss: 0.280901
epoch:67, batch18, load frames use: 5.59374880791s
step 3099/400000: train loss: 0.260866
epoch:67, batch19, load frames use: 3.26209783554s
step 3100/400000: train loss: 0.249115
step 3100/400000: validation loss: 0.252380
epoch:67, batch20, load frames use: 3.85553908348s
step 3101/400000: train loss: 0.255724
epoch:67, batch21, load frames use: 3.62743997574s
step 3102/400000: train loss: 0.270574
epoch:67, batch22, load frames use: 3.65878891945s
step 3103/400000: train loss: 0.246056
epoch:67, batch23, load frames use: 4.43873286247s
step 3104/400000: train loss: 0.268222
epoch:67, batch24, load frames use: 5.47892785072s
step 3105/400000: train loss: 0.243952
epoch:67, batch25, load frames use: 2.86179113388s
step 3106/400000: train loss: 0.224252
epoch:67, batch26, load frames use: 4.10481905937s
step 3107/400000: train loss: 0.252470
epoch:67, batch27, load frames use: 3.99103403091s
step 3108/400000: train loss: 0.231914
epoch:67, batch28, load frames use: 3.20264410973s
step 3109/400000: train loss: 0.225467
epoch:67, batch29, load frames use: 2.82922506332s
step 3110/400000: train loss: 0.236248
epoch:67, batch30, load frames use: 4.80607414246s
step 3111/400000: train loss: 0.279183
epoch:67, batch31, load frames use: 4.65643215179s
step 3112/400000: train loss: 0.242458
epoch:67, batch32, load frames use: 3.47507786751s
step 3113/400000: train loss: 0.232871
epoch:67, batch33, load frames use: 3.76407814026s
step 3114/400000: train loss: 0.232428
epoch:67, batch34, load frames use: 3.22318696976s
step 3115/400000: train loss: 0.234156
epoch:67, batch35, load frames use: 2.96535801888s
step 3116/400000: train loss: 0.245533
epoch:67, batch36, load frames use: 4.06982111931s
step 3117/400000: train loss: 0.243750
epoch:67, batch37, load frames use: 3.33151817322s
step 3118/400000: train loss: 0.280037
epoch:67, batch38, load frames use: 2.85109996796s
step 3119/400000: train loss: 0.265904
epoch:67, batch39, load frames use: 4.09125208855s
step 3120/400000: train loss: 0.285710
epoch:67, batch40, load frames use: 2.97236800194s
step 3121/400000: train loss: 0.272584
epoch:67, batch41, load frames use: 3.3722410202s
step 3122/400000: train loss: 0.267267
epoch:67, batch42, load frames use: 3.04143309593s
step 3123/400000: train loss: 0.287104
epoch:67, batch43, load frames use: 4.37044095993s
step 3124/400000: train loss: 0.272028
epoch:67, batch44, load frames use: 3.83736491203s
step 3125/400000: train loss: 0.251561
epoch:67, batch45, load frames use: 3.00242996216s
step 3126/400000: train loss: 0.293197
epoch:67, batch46, load frames use: 3.41727900505s
step 3127/400000: train loss: 0.314532
epoch:68, batch1, load frames use: 2.86070895195s
step 3128/400000: train loss: 0.222039
epoch:68, batch2, load frames use: 3.48003387451s
step 3129/400000: train loss: 0.242013
epoch:68, batch3, load frames use: 3.04037308693s
step 3130/400000: train loss: 0.233225
epoch:68, batch4, load frames use: 3.45591807365s
step 3131/400000: train loss: 0.251141
epoch:68, batch5, load frames use: 2.89000296593s
step 3132/400000: train loss: 0.245205
epoch:68, batch6, load frames use: 3.59393191338s
step 3133/400000: train loss: 0.250733
epoch:68, batch7, load frames use: 3.18836808205s
step 3134/400000: train loss: 0.254972
epoch:68, batch8, load frames use: 3.32921004295s
step 3135/400000: train loss: 0.235965
epoch:68, batch9, load frames use: 3.12554192543s
step 3136/400000: train loss: 0.246605
epoch:68, batch10, load frames use: 3.73025107384s
step 3137/400000: train loss: 0.247988
epoch:68, batch11, load frames use: 2.53144598007s
step 3138/400000: train loss: 0.246816
epoch:68, batch12, load frames use: 3.29683804512s
step 3139/400000: train loss: 0.267869
epoch:68, batch13, load frames use: 3.179459095s
step 3140/400000: train loss: 0.255992
epoch:68, batch14, load frames use: 3.01096105576s
step 3141/400000: train loss: 0.253381
epoch:68, batch15, load frames use: 4.06582808495s
step 3142/400000: train loss: 0.256392
epoch:68, batch16, load frames use: 3.52961587906s
step 3143/400000: train loss: 0.224309
epoch:68, batch17, load frames use: 3.77437615395s
step 3144/400000: train loss: 0.278599
epoch:68, batch18, load frames use: 2.96668601036s
step 3145/400000: train loss: 0.293190
epoch:68, batch19, load frames use: 4.60135197639s
step 3146/400000: train loss: 0.248458
epoch:68, batch20, load frames use: 3.93816113472s
step 3147/400000: train loss: 0.243602
epoch:68, batch21, load frames use: 4.05329012871s
step 3148/400000: train loss: 0.275773
epoch:68, batch22, load frames use: 3.31188488007s
step 3149/400000: train loss: 0.255565
epoch:68, batch23, load frames use: 2.67785096169s
step 3150/400000: train loss: 0.288761
epoch:68, batch24, load frames use: 5.35447907448s
step 3151/400000: train loss: 0.267547
epoch:68, batch25, load frames use: 5.44134092331s
step 3152/400000: train loss: 0.232559
epoch:68, batch26, load frames use: 4.28920292854s
step 3153/400000: train loss: 0.251580
epoch:68, batch27, load frames use: 6.55636787415s
step 3154/400000: train loss: 0.309164
epoch:68, batch28, load frames use: 4.3848760128s
step 3155/400000: train loss: 0.266403
epoch:68, batch29, load frames use: 4.04617118835s
step 3156/400000: train loss: 0.272445
epoch:68, batch30, load frames use: 3.87598705292s
step 3157/400000: train loss: 0.243037
epoch:68, batch31, load frames use: 9.55909204483s
step 3158/400000: train loss: 0.245064
epoch:68, batch32, load frames use: 4.12811088562s
step 3159/400000: train loss: 0.272904
epoch:68, batch33, load frames use: 3.91553997993s
step 3160/400000: train loss: 0.255370
epoch:68, batch34, load frames use: 5.30317497253s
step 3161/400000: train loss: 0.234828
epoch:68, batch35, load frames use: 4.12746882439s
step 3162/400000: train loss: 0.272534
epoch:68, batch36, load frames use: 5.78137612343s
step 3163/400000: train loss: 0.259834
epoch:68, batch37, load frames use: 4.3444750309s
step 3164/400000: train loss: 0.236088
epoch:68, batch38, load frames use: 4.49339699745s
step 3165/400000: train loss: 0.242639
epoch:68, batch39, load frames use: 4.35374379158s
step 3166/400000: train loss: 0.263839
epoch:68, batch40, load frames use: 3.50242996216s
step 3167/400000: train loss: 0.241149
epoch:68, batch41, load frames use: 3.60839700699s
step 3168/400000: train loss: 0.252754
epoch:68, batch42, load frames use: 3.05798101425s
step 3169/400000: train loss: 0.253045
epoch:68, batch43, load frames use: 4.75731897354s
step 3170/400000: train loss: 0.268025
epoch:68, batch44, load frames use: 4.11648201942s
step 3171/400000: train loss: 0.246221
epoch:68, batch45, load frames use: 3.24280905724s
step 3172/400000: train loss: 0.270638
epoch:68, batch46, load frames use: 3.15927886963s
step 3173/400000: train loss: 0.268497
epoch:69, batch1, load frames use: 4.0984017849s
step 3174/400000: train loss: 0.229930
epoch:69, batch2, load frames use: 4.27951312065s
step 3175/400000: train loss: 0.247942
epoch:69, batch3, load frames use: 3.82470107079s
step 3176/400000: train loss: 0.241085
epoch:69, batch4, load frames use: 3.10654783249s
step 3177/400000: train loss: 0.219476
epoch:69, batch5, load frames use: 3.15968704224s
step 3178/400000: train loss: 0.260674
epoch:69, batch6, load frames use: 4.57349419594s
step 3179/400000: train loss: 0.221191
epoch:69, batch7, load frames use: 4.4593539238s
step 3180/400000: train loss: 0.275091
epoch:69, batch8, load frames use: 3.8121278286s
step 3181/400000: train loss: 0.241810
epoch:69, batch9, load frames use: 3.1205470562s
step 3182/400000: train loss: 0.247532
epoch:69, batch10, load frames use: 2.96223783493s
step 3183/400000: train loss: 0.293057
epoch:69, batch11, load frames use: 4.40233492851s
step 3184/400000: train loss: 0.241427
epoch:69, batch12, load frames use: 4.04433202744s
step 3185/400000: train loss: 0.225446
epoch:69, batch13, load frames use: 2.79811501503s
step 3186/400000: train loss: 0.284369
epoch:69, batch14, load frames use: 3.14609122276s
step 3187/400000: train loss: 0.237032
epoch:69, batch15, load frames use: 3.79767894745s
step 3188/400000: train loss: 0.250134
epoch:69, batch16, load frames use: 4.27721595764s
step 3189/400000: train loss: 0.243630
epoch:69, batch17, load frames use: 3.25064992905s
step 3190/400000: train loss: 0.279952
epoch:69, batch18, load frames use: 3.19095396996s
step 3191/400000: train loss: 0.255623
epoch:69, batch19, load frames use: 3.60281991959s
step 3192/400000: train loss: 0.247838
epoch:69, batch20, load frames use: 4.65302705765s
step 3193/400000: train loss: 0.277160
epoch:69, batch21, load frames use: 5.41122913361s
step 3194/400000: train loss: 0.237356
epoch:69, batch22, load frames use: 3.03103494644s
step 3195/400000: train loss: 0.238724
epoch:69, batch23, load frames use: 3.59049987793s
step 3196/400000: train loss: 0.234309
epoch:69, batch24, load frames use: 2.79766011238s
step 3197/400000: train loss: 0.224576
epoch:69, batch25, load frames use: 4.39337611198s
step 3198/400000: train loss: 0.237422
epoch:69, batch26, load frames use: 4.56727409363s
step 3199/400000: train loss: 0.245366
epoch:69, batch27, load frames use: 3.14312005043s
step 3200/400000: train loss: 0.248707
step 3200/400000: validation loss: 0.245710
epoch:69, batch28, load frames use: 3.40636515617s
step 3201/400000: train loss: 0.259078
epoch:69, batch29, load frames use: 3.37372207642s
step 3202/400000: train loss: 0.253654
epoch:69, batch30, load frames use: 3.98117208481s
step 3203/400000: train loss: 0.254639
epoch:69, batch31, load frames use: 4.28238892555s
step 3204/400000: train loss: 0.302896
epoch:69, batch32, load frames use: 3.54076099396s
step 3205/400000: train loss: 0.252990
epoch:69, batch33, load frames use: 3.1242108345s
step 3206/400000: train loss: 0.252475
epoch:69, batch34, load frames use: 2.96769213676s
step 3207/400000: train loss: 0.286518
epoch:69, batch35, load frames use: 3.59550285339s
step 3208/400000: train loss: 0.276792
epoch:69, batch36, load frames use: 2.82725811005s
step 3209/400000: train loss: 0.249479
epoch:69, batch37, load frames use: 4.26884794235s
step 3210/400000: train loss: 0.257387
epoch:69, batch38, load frames use: 3.78440499306s
step 3211/400000: train loss: 0.317259
epoch:69, batch39, load frames use: 2.92262005806s
step 3212/400000: train loss: 0.239077
epoch:69, batch40, load frames use: 3.20192813873s
step 3213/400000: train loss: 0.256434
epoch:69, batch41, load frames use: 3.75430893898s
step 3214/400000: train loss: 0.253244
epoch:69, batch42, load frames use: 2.81733894348s
step 3215/400000: train loss: 0.244761
epoch:69, batch43, load frames use: 3.09118795395s
step 3216/400000: train loss: 0.275964
epoch:69, batch44, load frames use: 3.79717803001s
step 3217/400000: train loss: 0.322143
epoch:69, batch45, load frames use: 4.54867601395s
step 3218/400000: train loss: 0.292983
epoch:69, batch46, load frames use: 4.09558200836s
step 3219/400000: train loss: 0.271361
epoch:70, batch1, load frames use: 3.60027909279s
step 3220/400000: train loss: 0.207932
epoch:70, batch2, load frames use: 2.91844201088s
step 3221/400000: train loss: 0.237215
epoch:70, batch3, load frames use: 4.03741312027s
step 3222/400000: train loss: 0.240048
epoch:70, batch4, load frames use: 3.08784794807s
step 3223/400000: train loss: 0.219375
epoch:70, batch5, load frames use: 3.35693717003s
step 3224/400000: train loss: 0.256183
epoch:70, batch6, load frames use: 3.03257203102s
step 3225/400000: train loss: 0.261950
epoch:70, batch7, load frames use: 3.342689991s
step 3226/400000: train loss: 0.248769
epoch:70, batch8, load frames use: 2.97521018982s
step 3227/400000: train loss: 0.234757
epoch:70, batch9, load frames use: 4.27375197411s
step 3228/400000: train loss: 0.234536
epoch:70, batch10, load frames use: 3.62911605835s
step 3229/400000: train loss: 0.263372
epoch:70, batch11, load frames use: 3.1838619709s
step 3230/400000: train loss: 0.240433
epoch:70, batch12, load frames use: 3.42526817322s
step 3231/400000: train loss: 0.274539
epoch:70, batch13, load frames use: 3.78216600418s
step 3232/400000: train loss: 0.246787
epoch:70, batch14, load frames use: 3.22627210617s
step 3233/400000: train loss: 0.213193
epoch:70, batch15, load frames use: 2.9813170433s
step 3234/400000: train loss: 0.289656
epoch:70, batch16, load frames use: 2.7612490654s
step 3235/400000: train loss: 0.226416
epoch:70, batch17, load frames use: 2.98406505585s
step 3236/400000: train loss: 0.234062
epoch:70, batch18, load frames use: 3.1960940361s
step 3237/400000: train loss: 0.300524
epoch:70, batch19, load frames use: 2.92946791649s
step 3238/400000: train loss: 0.242059
epoch:70, batch20, load frames use: 3.01945018768s
step 3239/400000: train loss: 0.249426
epoch:70, batch21, load frames use: 3.65361785889s
step 3240/400000: train loss: 0.233801
epoch:70, batch22, load frames use: 3.15712499619s
step 3241/400000: train loss: 0.247916
epoch:70, batch23, load frames use: 3.21020317078s
step 3242/400000: train loss: 0.244802
epoch:70, batch24, load frames use: 3.35003900528s
step 3243/400000: train loss: 0.269920
epoch:70, batch25, load frames use: 3.40700507164s
step 3244/400000: train loss: 0.248807
epoch:70, batch26, load frames use: 2.86689114571s
step 3245/400000: train loss: 0.248894
epoch:70, batch27, load frames use: 4.02828097343s
step 3246/400000: train loss: 0.276788
epoch:70, batch28, load frames use: 4.07603812218s
step 3247/400000: train loss: 0.248285
epoch:70, batch29, load frames use: 3.65870809555s
step 3248/400000: train loss: 0.262134
epoch:70, batch30, load frames use: 2.73119592667s
step 3249/400000: train loss: 0.261501
epoch:70, batch31, load frames use: 4.17187595367s
step 3250/400000: train loss: 0.271456
epoch:70, batch32, load frames use: 4.55819010735s
step 3251/400000: train loss: 0.248649
epoch:70, batch33, load frames use: 3.43252205849s
step 3252/400000: train loss: 0.251256
epoch:70, batch34, load frames use: 3.34415602684s
step 3253/400000: train loss: 0.242852
epoch:70, batch35, load frames use: 2.7266368866s
step 3254/400000: train loss: 0.263355
epoch:70, batch36, load frames use: 3.8098449707s
step 3255/400000: train loss: 0.256902
epoch:70, batch37, load frames use: 3.09679079056s
step 3256/400000: train loss: 0.262525
epoch:70, batch38, load frames use: 3.66957402229s
step 3257/400000: train loss: 0.259744
epoch:70, batch39, load frames use: 2.84273481369s
step 3258/400000: train loss: 0.282166
epoch:70, batch40, load frames use: 3.81336116791s
step 3259/400000: train loss: 0.245159
epoch:70, batch41, load frames use: 3.01536488533s
step 3260/400000: train loss: 0.226461
epoch:70, batch42, load frames use: 3.82130384445s
step 3261/400000: train loss: 0.243549
epoch:70, batch43, load frames use: 3.15449500084s
step 3262/400000: train loss: 0.247797
epoch:70, batch44, load frames use: 3.11557197571s
step 3263/400000: train loss: 0.239650
epoch:70, batch45, load frames use: 3.34029698372s
step 3264/400000: train loss: 0.262520
epoch:70, batch46, load frames use: 3.56119012833s
step 3265/400000: train loss: 0.252094
epoch:71, batch1, load frames use: 2.91815805435s
step 3266/400000: train loss: 0.261410
epoch:71, batch2, load frames use: 3.97184085846s
step 3267/400000: train loss: 0.239990
epoch:71, batch3, load frames use: 3.83998894691s
step 3268/400000: train loss: 0.228099
epoch:71, batch4, load frames use: 3.69920992851s
step 3269/400000: train loss: 0.224069
epoch:71, batch5, load frames use: 3.12333512306s
step 3270/400000: train loss: 0.245377
epoch:71, batch6, load frames use: 3.06269216537s
step 3271/400000: train loss: 0.257136
epoch:71, batch7, load frames use: 3.30991697311s
step 3272/400000: train loss: 0.278609
epoch:71, batch8, load frames use: 3.83473181725s
step 3273/400000: train loss: 0.217709
epoch:71, batch9, load frames use: 7.36842107773s
step 3274/400000: train loss: 0.224970
epoch:71, batch10, load frames use: 3.63494205475s
step 3275/400000: train loss: 0.248241
epoch:71, batch11, load frames use: 4.292979002s
step 3276/400000: train loss: 0.237770
epoch:71, batch12, load frames use: 4.10270905495s
step 3277/400000: train loss: 0.230317
epoch:71, batch13, load frames use: 4.37801909447s
step 3278/400000: train loss: 0.247631
epoch:71, batch14, load frames use: 3.29275202751s
step 3279/400000: train loss: 0.249702
epoch:71, batch15, load frames use: 2.99928617477s
step 3280/400000: train loss: 0.233454
epoch:71, batch16, load frames use: 3.18349695206s
step 3281/400000: train loss: 0.262509
epoch:71, batch17, load frames use: 3.42036604881s
step 3282/400000: train loss: 0.266727
epoch:71, batch18, load frames use: 3.08387804031s
step 3283/400000: train loss: 0.223975
epoch:71, batch19, load frames use: 4.33915901184s
step 3284/400000: train loss: 0.266040
epoch:71, batch20, load frames use: 4.40688085556s
step 3285/400000: train loss: 0.204783
epoch:71, batch21, load frames use: 3.08002090454s
step 3286/400000: train loss: 0.243080
epoch:71, batch22, load frames use: 3.73869514465s
step 3287/400000: train loss: 0.250060
epoch:71, batch23, load frames use: 2.85107517242s
step 3288/400000: train loss: 0.230371
epoch:71, batch24, load frames use: 4.54077005386s
step 3289/400000: train loss: 0.239378
epoch:71, batch25, load frames use: 4.10703396797s
step 3290/400000: train loss: 0.272088
epoch:71, batch26, load frames use: 3.08204698563s
step 3291/400000: train loss: 0.216927
epoch:71, batch27, load frames use: 3.05695986748s
step 3292/400000: train loss: 0.255395
epoch:71, batch28, load frames use: 3.40762686729s
step 3293/400000: train loss: 0.267660
epoch:71, batch29, load frames use: 3.57262516022s
step 3294/400000: train loss: 0.229606
epoch:71, batch30, load frames use: 3.25081515312s
step 3295/400000: train loss: 0.212732
epoch:71, batch31, load frames use: 3.65729093552s
step 3296/400000: train loss: 0.221802
epoch:71, batch32, load frames use: 2.89480280876s
step 3297/400000: train loss: 0.249573
epoch:71, batch33, load frames use: 2.91220903397s
step 3298/400000: train loss: 0.218638
epoch:71, batch34, load frames use: 3.63681387901s
step 3299/400000: train loss: 0.240946
epoch:71, batch35, load frames use: 2.94592499733s
step 3300/400000: train loss: 0.270511
step 3300/400000: validation loss: 0.253002
epoch:71, batch36, load frames use: 3.5972738266s
step 3301/400000: train loss: 0.258915
epoch:71, batch37, load frames use: 3.54098010063s
step 3302/400000: train loss: 0.220757
epoch:71, batch38, load frames use: 3.59089112282s
step 3303/400000: train loss: 0.235961
epoch:71, batch39, load frames use: 3.44572305679s
step 3304/400000: train loss: 0.233468
epoch:71, batch40, load frames use: 2.99451088905s
step 3305/400000: train loss: 0.276955
epoch:71, batch41, load frames use: 4.48671102524s
step 3306/400000: train loss: 0.264532
epoch:71, batch42, load frames use: 3.62050199509s
step 3307/400000: train loss: 0.256162
epoch:71, batch43, load frames use: 2.93068313599s
step 3308/400000: train loss: 0.222717
epoch:71, batch44, load frames use: 2.84612083435s
step 3309/400000: train loss: 0.237099
epoch:71, batch45, load frames use: 3.93869185448s
step 3310/400000: train loss: 0.247877
epoch:71, batch46, load frames use: 3.00010085106s
step 3311/400000: train loss: 0.247422
epoch:72, batch1, load frames use: 2.89953780174s
step 3312/400000: train loss: 0.255450
epoch:72, batch2, load frames use: 2.78872489929s
step 3313/400000: train loss: 0.248395
epoch:72, batch3, load frames use: 2.90920305252s
step 3314/400000: train loss: 0.217381
epoch:72, batch4, load frames use: 2.55951094627s
step 3315/400000: train loss: 0.238638
epoch:72, batch5, load frames use: 2.71382689476s
step 3316/400000: train loss: 0.268844
epoch:72, batch6, load frames use: 2.77329683304s
step 3317/400000: train loss: 0.246449
epoch:72, batch7, load frames use: 4.22853684425s
step 3318/400000: train loss: 0.232871
epoch:72, batch8, load frames use: 3.08027696609s
step 3319/400000: train loss: 0.252034
epoch:72, batch9, load frames use: 3.48657011986s
step 3320/400000: train loss: 0.226954
epoch:72, batch10, load frames use: 3.0625770092s
step 3321/400000: train loss: 0.255300
epoch:72, batch11, load frames use: 3.96768403053s
step 3322/400000: train loss: 0.242846
epoch:72, batch12, load frames use: 2.89021205902s
step 3323/400000: train loss: 0.240319
epoch:72, batch13, load frames use: 4.39215111732s
step 3324/400000: train loss: 0.254708
epoch:72, batch14, load frames use: 4.06764006615s
step 3325/400000: train loss: 0.251496
epoch:72, batch15, load frames use: 4.11220097542s
step 3326/400000: train loss: 0.203837
epoch:72, batch16, load frames use: 3.68918895721s
step 3327/400000: train loss: 0.211216
epoch:72, batch17, load frames use: 5.4587020874s
step 3328/400000: train loss: 0.225735
epoch:72, batch18, load frames use: 6.32767891884s
step 3329/400000: train loss: 0.249227
epoch:72, batch19, load frames use: 4.32378005981s
step 3330/400000: train loss: 0.240077
epoch:72, batch20, load frames use: 4.25120592117s
step 3331/400000: train loss: 0.252406
epoch:72, batch21, load frames use: 5.64738583565s
step 3332/400000: train loss: 0.243315
epoch:72, batch22, load frames use: 3.28084111214s
step 3333/400000: train loss: 0.239651
epoch:72, batch23, load frames use: 3.74685692787s
step 3334/400000: train loss: 0.219883
epoch:72, batch24, load frames use: 3.71311593056s
step 3335/400000: train loss: 0.265864
epoch:72, batch25, load frames use: 3.74354887009s
step 3336/400000: train loss: 0.227271
epoch:72, batch26, load frames use: 3.5564930439s
step 3337/400000: train loss: 0.254271
epoch:72, batch27, load frames use: 3.85076403618s
step 3338/400000: train loss: 0.260902
epoch:72, batch28, load frames use: 3.40100312233s
step 3339/400000: train loss: 0.229160
epoch:72, batch29, load frames use: 3.0176179409s
step 3340/400000: train loss: 0.256442
epoch:72, batch30, load frames use: 3.31180286407s
step 3341/400000: train loss: 0.233419
epoch:72, batch31, load frames use: 3.46683716774s
step 3342/400000: train loss: 0.252710
epoch:72, batch32, load frames use: 3.04621005058s
step 3343/400000: train loss: 0.244488
epoch:72, batch33, load frames use: 4.25359106064s
step 3344/400000: train loss: 0.231535
epoch:72, batch34, load frames use: 4.16028022766s
step 3345/400000: train loss: 0.246210
epoch:72, batch35, load frames use: 3.28241109848s
step 3346/400000: train loss: 0.259423
epoch:72, batch36, load frames use: 3.48796796799s
step 3347/400000: train loss: 0.239256
epoch:72, batch37, load frames use: 3.45610499382s
step 3348/400000: train loss: 0.241840
epoch:72, batch38, load frames use: 3.47394013405s
step 3349/400000: train loss: 0.240045
epoch:72, batch39, load frames use: 3.49002504349s
step 3350/400000: train loss: 0.212399
epoch:72, batch40, load frames use: 4.2028670311s
step 3351/400000: train loss: 0.224448
epoch:72, batch41, load frames use: 3.30359506607s
step 3352/400000: train loss: 0.271936
epoch:72, batch42, load frames use: 2.92884516716s
step 3353/400000: train loss: 0.270844
epoch:72, batch43, load frames use: 2.99615001678s
step 3354/400000: train loss: 0.220601
epoch:72, batch44, load frames use: 3.67550587654s
step 3355/400000: train loss: 0.250392
epoch:72, batch45, load frames use: 3.40225410461s
step 3356/400000: train loss: 0.219674
epoch:72, batch46, load frames use: 2.95524096489s
step 3357/400000: train loss: 0.278036
epoch:73, batch1, load frames use: 3.69987201691s
step 3358/400000: train loss: 0.256248
epoch:73, batch2, load frames use: 3.04083108902s
step 3359/400000: train loss: 0.230186
epoch:73, batch3, load frames use: 4.62411499023s
step 3360/400000: train loss: 0.225084
epoch:73, batch4, load frames use: 4.36952590942s
step 3361/400000: train loss: 0.240615
epoch:73, batch5, load frames use: 3.40637993813s
step 3362/400000: train loss: 0.226882
epoch:73, batch6, load frames use: 3.50181293488s
step 3363/400000: train loss: 0.198704
epoch:73, batch7, load frames use: 3.70073390007s
step 3364/400000: train loss: 0.236339
epoch:73, batch8, load frames use: 3.86038398743s
step 3365/400000: train loss: 0.223621
epoch:73, batch9, load frames use: 4.26884794235s
step 3366/400000: train loss: 0.216256
epoch:73, batch10, load frames use: 3.27148890495s
step 3367/400000: train loss: 0.238193
epoch:73, batch11, load frames use: 3.03997802734s
step 3368/400000: train loss: 0.245164
epoch:73, batch12, load frames use: 4.13361597061s
step 3369/400000: train loss: 0.250875
epoch:73, batch13, load frames use: 4.26157808304s
step 3370/400000: train loss: 0.248782
epoch:73, batch14, load frames use: 3.25964999199s
step 3371/400000: train loss: 0.238734
epoch:73, batch15, load frames use: 4.37826704979s
step 3372/400000: train loss: 0.254089
epoch:73, batch16, load frames use: 3.2891190052s
step 3373/400000: train loss: 0.235668
epoch:73, batch17, load frames use: 3.88824605942s
step 3374/400000: train loss: 0.228359
epoch:73, batch18, load frames use: 2.7072520256s
step 3375/400000: train loss: 0.251094
epoch:73, batch19, load frames use: 3.19804811478s
step 3376/400000: train loss: 0.236849
epoch:73, batch20, load frames use: 3.8083319664s
step 3377/400000: train loss: 0.249336
epoch:73, batch21, load frames use: 4.85426211357s
step 3378/400000: train loss: 0.239117
epoch:73, batch22, load frames use: 3.02584695816s
step 3379/400000: train loss: 0.236443
epoch:73, batch23, load frames use: 3.46274614334s
step 3380/400000: train loss: 0.278068
epoch:73, batch24, load frames use: 4.45043206215s
step 3381/400000: train loss: 0.241322
epoch:73, batch25, load frames use: 2.96854305267s
step 3382/400000: train loss: 0.245074
epoch:73, batch26, load frames use: 3.66188502312s
step 3383/400000: train loss: 0.245814
epoch:73, batch27, load frames use: 3.62938189507s
step 3384/400000: train loss: 0.227528
epoch:73, batch28, load frames use: 2.89718699455s
step 3385/400000: train loss: 0.235180
epoch:73, batch29, load frames use: 3.54193806648s
step 3386/400000: train loss: 0.249345
epoch:73, batch30, load frames use: 3.38433408737s
step 3387/400000: train loss: 0.265072
epoch:73, batch31, load frames use: 3.86656785011s
step 3388/400000: train loss: 0.237711
epoch:73, batch32, load frames use: 3.84928798676s
step 3389/400000: train loss: 0.272105
epoch:73, batch33, load frames use: 3.24562811852s
step 3390/400000: train loss: 0.282307
epoch:73, batch34, load frames use: 3.02364611626s
step 3391/400000: train loss: 0.254332
epoch:73, batch35, load frames use: 3.53163194656s
step 3392/400000: train loss: 0.218080
epoch:73, batch36, load frames use: 2.91107177734s
step 3393/400000: train loss: 0.235708
epoch:73, batch37, load frames use: 3.59968900681s
step 3394/400000: train loss: 0.234945
epoch:73, batch38, load frames use: 3.37455391884s
step 3395/400000: train loss: 0.251472
epoch:73, batch39, load frames use: 3.25380897522s
step 3396/400000: train loss: 0.277845
epoch:73, batch40, load frames use: 3.05747795105s
step 3397/400000: train loss: 0.228910
epoch:73, batch41, load frames use: 3.31378102303s
step 3398/400000: train loss: 0.217346
epoch:73, batch42, load frames use: 3.50021409988s
step 3399/400000: train loss: 0.240335
epoch:73, batch43, load frames use: 3.34193515778s
step 3400/400000: train loss: 0.232518
step 3400/400000: validation loss: 0.225464
epoch:73, batch44, load frames use: 3.55478191376s
step 3401/400000: train loss: 0.239005
epoch:73, batch45, load frames use: 3.07126402855s
step 3402/400000: train loss: 0.230164
epoch:73, batch46, load frames use: 3.08644914627s
step 3403/400000: train loss: 0.222979
epoch:74, batch1, load frames use: 4.2277109623s
step 3404/400000: train loss: 0.247024
epoch:74, batch2, load frames use: 3.50994801521s
step 3405/400000: train loss: 0.201745
epoch:74, batch3, load frames use: 3.22827792168s
step 3406/400000: train loss: 0.217441
epoch:74, batch4, load frames use: 3.44254899025s
step 3407/400000: train loss: 0.234863
epoch:74, batch5, load frames use: 3.34040403366s
step 3408/400000: train loss: 0.226779
epoch:74, batch6, load frames use: 2.82329511642s
step 3409/400000: train loss: 0.254075
epoch:74, batch7, load frames use: 3.11470508575s
step 3410/400000: train loss: 0.276780
epoch:74, batch8, load frames use: 3.78106999397s
step 3411/400000: train loss: 0.234492
epoch:74, batch9, load frames use: 2.8782248497s
step 3412/400000: train loss: 0.216476
epoch:74, batch10, load frames use: 2.94106197357s
step 3413/400000: train loss: 0.247908
epoch:74, batch11, load frames use: 3.88264203072s
step 3414/400000: train loss: 0.210324
epoch:74, batch12, load frames use: 3.83780002594s
step 3415/400000: train loss: 0.236691
epoch:74, batch13, load frames use: 3.07449316978s
step 3416/400000: train loss: 0.195714
epoch:74, batch14, load frames use: 3.55269813538s
step 3417/400000: train loss: 0.240471
epoch:74, batch15, load frames use: 3.32581591606s
step 3418/400000: train loss: 0.245063
epoch:74, batch16, load frames use: 3.12647080421s
step 3419/400000: train loss: 0.224352
epoch:74, batch17, load frames use: 3.96950006485s
step 3420/400000: train loss: 0.241939
epoch:74, batch18, load frames use: 3.03277111053s
step 3421/400000: train loss: 0.239608
epoch:74, batch19, load frames use: 3.87129616737s
step 3422/400000: train loss: 0.233929
epoch:74, batch20, load frames use: 3.12694191933s
step 3423/400000: train loss: 0.216754
epoch:74, batch21, load frames use: 4.24700689316s
step 3424/400000: train loss: 0.208858
epoch:74, batch22, load frames use: 3.88096690178s
step 3425/400000: train loss: 0.262001
epoch:74, batch23, load frames use: 3.22757387161s
step 3426/400000: train loss: 0.260732
epoch:74, batch24, load frames use: 2.96249318123s
step 3427/400000: train loss: 0.261724
epoch:74, batch25, load frames use: 3.99630212784s
step 3428/400000: train loss: 0.230914
epoch:74, batch26, load frames use: 4.27219891548s
step 3429/400000: train loss: 0.223491
epoch:74, batch27, load frames use: 3.22759985924s
step 3430/400000: train loss: 0.210871
epoch:74, batch28, load frames use: 3.20640897751s
step 3431/400000: train loss: 0.251199
epoch:74, batch29, load frames use: 3.90341711044s
step 3432/400000: train loss: 0.244683
epoch:74, batch30, load frames use: 2.95727109909s
step 3433/400000: train loss: 0.229332
epoch:74, batch31, load frames use: 3.2345790863s
step 3434/400000: train loss: 0.275726
epoch:74, batch32, load frames use: 3.2040719986s
step 3435/400000: train loss: 0.269074
epoch:74, batch33, load frames use: 3.5331389904s
step 3436/400000: train loss: 0.231882
epoch:74, batch34, load frames use: 2.85970306396s
step 3437/400000: train loss: 0.246121
epoch:74, batch35, load frames use: 4.31169891357s
step 3438/400000: train loss: 0.258404
epoch:74, batch36, load frames use: 4.00996208191s
step 3439/400000: train loss: 0.275045
epoch:74, batch37, load frames use: 3.52970385551s
step 3440/400000: train loss: 0.246778
epoch:74, batch38, load frames use: 2.96901392937s
step 3441/400000: train loss: 0.227559
epoch:74, batch39, load frames use: 4.07007312775s
step 3442/400000: train loss: 0.220561
epoch:74, batch40, load frames use: 3.49264883995s
step 3443/400000: train loss: 0.252738
epoch:74, batch41, load frames use: 3.25570583344s
step 3444/400000: train loss: 0.259231
epoch:74, batch42, load frames use: 3.13884401321s
step 3445/400000: train loss: 0.221336
epoch:74, batch43, load frames use: 3.65304493904s
step 3446/400000: train loss: 0.236142
epoch:74, batch44, load frames use: 2.95642900467s
step 3447/400000: train loss: 0.243971
epoch:74, batch45, load frames use: 3.20393896103s
step 3448/400000: train loss: 0.247648
epoch:74, batch46, load frames use: 3.36786484718s
step 3449/400000: train loss: 0.226862
epoch:75, batch1, load frames use: 3.03143405914s
step 3450/400000: train loss: 0.230904
epoch:75, batch2, load frames use: 3.74737119675s
step 3451/400000: train loss: 0.267923
epoch:75, batch3, load frames use: 3.26464796066s
step 3452/400000: train loss: 0.213000
epoch:75, batch4, load frames use: 3.2777659893s
step 3453/400000: train loss: 0.252502
epoch:75, batch5, load frames use: 2.86934494972s
step 3454/400000: train loss: 0.207659
epoch:75, batch6, load frames use: 3.4663670063s
step 3455/400000: train loss: 0.223114
epoch:75, batch7, load frames use: 2.71295905113s
step 3456/400000: train loss: 0.210005
epoch:75, batch8, load frames use: 3.65125107765s
step 3457/400000: train loss: 0.252749
epoch:75, batch9, load frames use: 3.97815799713s
step 3458/400000: train loss: 0.235838
epoch:75, batch10, load frames use: 3.4935760498s
step 3459/400000: train loss: 0.232139
epoch:75, batch11, load frames use: 3.0937628746s
step 3460/400000: train loss: 0.223153
epoch:75, batch12, load frames use: 4.14352202415s
step 3461/400000: train loss: 0.208625
epoch:75, batch13, load frames use: 2.81679606438s
step 3462/400000: train loss: 0.246163
epoch:75, batch14, load frames use: 3.24414205551s
step 3463/400000: train loss: 0.226799
epoch:75, batch15, load frames use: 3.25846910477s
step 3464/400000: train loss: 0.217183
epoch:75, batch16, load frames use: 3.16733503342s
step 3465/400000: train loss: 0.243866
epoch:75, batch17, load frames use: 2.98871397972s
step 3466/400000: train loss: 0.216577
epoch:75, batch18, load frames use: 4.2994799614s
step 3467/400000: train loss: 0.232388
epoch:75, batch19, load frames use: 3.01143217087s
step 3468/400000: train loss: 0.277420
epoch:75, batch20, load frames use: 3.30786514282s
step 3469/400000: train loss: 0.249798
epoch:75, batch21, load frames use: 3.30833601952s
step 3470/400000: train loss: 0.238076
epoch:75, batch22, load frames use: 3.61798191071s
step 3471/400000: train loss: 0.230386
epoch:75, batch23, load frames use: 2.86836314201s
step 3472/400000: train loss: 0.247814
epoch:75, batch24, load frames use: 4.1471760273s
step 3473/400000: train loss: 0.234376
epoch:75, batch25, load frames use: 3.24354982376s
step 3474/400000: train loss: 0.237012
epoch:75, batch26, load frames use: 3.13184690475s
step 3475/400000: train loss: 0.246300
epoch:75, batch27, load frames use: 3.37465500832s
step 3476/400000: train loss: 0.226463
epoch:75, batch28, load frames use: 3.46473908424s
step 3477/400000: train loss: 0.217504
epoch:75, batch29, load frames use: 2.95726895332s
step 3478/400000: train loss: 0.274570
epoch:75, batch30, load frames use: 4.09637308121s
step 3479/400000: train loss: 0.247048
epoch:75, batch31, load frames use: 2.95120000839s
step 3480/400000: train loss: 0.249922
epoch:75, batch32, load frames use: 3.27655410767s
step 3481/400000: train loss: 0.223316
epoch:75, batch33, load frames use: 2.98178815842s
step 3482/400000: train loss: 0.258885
epoch:75, batch34, load frames use: 3.54059696198s
step 3483/400000: train loss: 0.259662
epoch:75, batch35, load frames use: 5.09203886986s
step 3484/400000: train loss: 0.235489
epoch:75, batch36, load frames use: 4.3494989872s
step 3485/400000: train loss: 0.254383
epoch:75, batch37, load frames use: 4.65752601624s
step 3486/400000: train loss: 0.224350
epoch:75, batch38, load frames use: 6.24018406868s
step 3487/400000: train loss: 0.231851
epoch:75, batch39, load frames use: 6.80726599693s
step 3488/400000: train loss: 0.235127
epoch:75, batch40, load frames use: 7.69995212555s
step 3489/400000: train loss: 0.249675
epoch:75, batch41, load frames use: 4.17607712746s
step 3490/400000: train loss: 0.242599
epoch:75, batch42, load frames use: 4.48207998276s
step 3491/400000: train loss: 0.259711
epoch:75, batch43, load frames use: 3.32448482513s
step 3492/400000: train loss: 0.234634
epoch:75, batch44, load frames use: 4.00507116318s
step 3493/400000: train loss: 0.216115
epoch:75, batch45, load frames use: 3.98578619957s
step 3494/400000: train loss: 0.227361
epoch:75, batch46, load frames use: 3.19147109985s
step 3495/400000: train loss: 0.257627
epoch:76, batch1, load frames use: 3.22038221359s
step 3496/400000: train loss: 0.225127
epoch:76, batch2, load frames use: 3.19394207001s
step 3497/400000: train loss: 0.195500
epoch:76, batch3, load frames use: 7.02119016647s
step 3498/400000: train loss: 0.227441
epoch:76, batch4, load frames use: 3.96490311623s
step 3499/400000: train loss: 0.239158
epoch:76, batch5, load frames use: 3.7877368927s
step 3500/400000: train loss: 0.221756
step 3500/400000: validation loss: 0.234208
epoch:76, batch6, load frames use: 6.43759703636s
step 3501/400000: train loss: 0.231126
epoch:76, batch7, load frames use: 5.27188181877s
step 3502/400000: train loss: 0.263345
epoch:76, batch8, load frames use: 3.60110187531s
step 3503/400000: train loss: 0.238761
epoch:76, batch9, load frames use: 4.07007002831s
step 3504/400000: train loss: 0.237974
epoch:76, batch10, load frames use: 3.60197901726s
step 3505/400000: train loss: 0.241236
epoch:76, batch11, load frames use: 5.13720583916s
step 3506/400000: train loss: 0.230749
epoch:76, batch12, load frames use: 3.90040707588s
step 3507/400000: train loss: 0.262123
epoch:76, batch13, load frames use: 4.09629511833s
step 3508/400000: train loss: 0.254772
epoch:76, batch14, load frames use: 4.62495112419s
step 3509/400000: train loss: 0.237368
epoch:76, batch15, load frames use: 3.44283509254s
step 3510/400000: train loss: 0.273857
epoch:76, batch16, load frames use: 2.83157706261s
step 3511/400000: train loss: 0.213619
epoch:76, batch17, load frames use: 4.15783405304s
step 3512/400000: train loss: 0.216800
epoch:76, batch18, load frames use: 2.88951587677s
step 3513/400000: train loss: 0.231426
epoch:76, batch19, load frames use: 3.04371809959s
step 3514/400000: train loss: 0.228848
epoch:76, batch20, load frames use: 3.55087614059s
step 3515/400000: train loss: 0.212155
epoch:76, batch21, load frames use: 3.64120101929s
step 3516/400000: train loss: 0.215151
epoch:76, batch22, load frames use: 3.00598406792s
step 3517/400000: train loss: 0.248005
epoch:76, batch23, load frames use: 4.11407279968s
step 3518/400000: train loss: 0.218678
epoch:76, batch24, load frames use: 3.45744180679s
step 3519/400000: train loss: 0.269571
epoch:76, batch25, load frames use: 3.30422210693s
step 3520/400000: train loss: 0.282040
epoch:76, batch26, load frames use: 3.21385788918s
step 3521/400000: train loss: 0.250370
epoch:76, batch27, load frames use: 4.63675999641s
step 3522/400000: train loss: 0.224413
epoch:76, batch28, load frames use: 4.20437598228s
step 3523/400000: train loss: 0.238257
epoch:76, batch29, load frames use: 3.25857496262s
step 3524/400000: train loss: 0.229792
epoch:76, batch30, load frames use: 3.41080999374s
step 3525/400000: train loss: 0.231252
epoch:76, batch31, load frames use: 3.54582595825s
step 3526/400000: train loss: 0.217773
epoch:76, batch32, load frames use: 4.57956314087s
step 3527/400000: train loss: 0.249932
epoch:76, batch33, load frames use: 3.16765284538s
step 3528/400000: train loss: 0.216028
epoch:76, batch34, load frames use: 4.03280711174s
step 3529/400000: train loss: 0.251471
epoch:76, batch35, load frames use: 3.2592010498s
step 3530/400000: train loss: 0.266712
epoch:76, batch36, load frames use: 3.24274492264s
step 3531/400000: train loss: 0.267479
epoch:76, batch37, load frames use: 3.18484091759s
step 3532/400000: train loss: 0.226878
epoch:76, batch38, load frames use: 4.53268885612s
step 3533/400000: train loss: 0.240651
epoch:76, batch39, load frames use: 4.38842797279s
step 3534/400000: train loss: 0.249124
epoch:76, batch40, load frames use: 2.99424290657s
step 3535/400000: train loss: 0.233799
epoch:76, batch41, load frames use: 3.56592512131s
step 3536/400000: train loss: 0.270083
epoch:76, batch42, load frames use: 3.40777993202s
step 3537/400000: train loss: 0.209484
epoch:76, batch43, load frames use: 3.38354897499s
step 3538/400000: train loss: 0.249385
epoch:76, batch44, load frames use: 3.49082589149s
step 3539/400000: train loss: 0.282383
epoch:76, batch45, load frames use: 3.96846985817s
step 3540/400000: train loss: 0.236783
epoch:76, batch46, load frames use: 3.21310806274s
step 3541/400000: train loss: 0.264018
epoch:77, batch1, load frames use: 2.94194293022s
step 3542/400000: train loss: 0.239217
epoch:77, batch2, load frames use: 2.90060114861s
step 3543/400000: train loss: 0.215459
epoch:77, batch3, load frames use: 3.522397995s
step 3544/400000: train loss: 0.218568
epoch:77, batch4, load frames use: 3.15211796761s
step 3545/400000: train loss: 0.235495
epoch:77, batch5, load frames use: 3.22145915031s
step 3546/400000: train loss: 0.227795
epoch:77, batch6, load frames use: 3.96070480347s
step 3547/400000: train loss: 0.247642
epoch:77, batch7, load frames use: 3.57251000404s
step 3548/400000: train loss: 0.231003
epoch:77, batch8, load frames use: 3.22606086731s
step 3549/400000: train loss: 0.198162
epoch:77, batch9, load frames use: 2.90087985992s
step 3550/400000: train loss: 0.229207
epoch:77, batch10, load frames use: 4.59144020081s
step 3551/400000: train loss: 0.235515
epoch:77, batch11, load frames use: 3.78487586975s
step 3552/400000: train loss: 0.218318
epoch:77, batch12, load frames use: 3.78088879585s
step 3553/400000: train loss: 0.223763
epoch:77, batch13, load frames use: 2.96893596649s
step 3554/400000: train loss: 0.263802
epoch:77, batch14, load frames use: 3.76994013786s
step 3555/400000: train loss: 0.227597
epoch:77, batch15, load frames use: 3.48103809357s
step 3556/400000: train loss: 0.241047
epoch:77, batch16, load frames use: 3.1554889679s
step 3557/400000: train loss: 0.227118
epoch:77, batch17, load frames use: 3.22659087181s
step 3558/400000: train loss: 0.233711
epoch:77, batch18, load frames use: 4.1747879982s
step 3559/400000: train loss: 0.239996
epoch:77, batch19, load frames use: 3.3214058876s
step 3560/400000: train loss: 0.223485
epoch:77, batch20, load frames use: 3.42813396454s
step 3561/400000: train loss: 0.230752
epoch:77, batch21, load frames use: 3.01212191582s
step 3562/400000: train loss: 0.269012
epoch:77, batch22, load frames use: 3.89335584641s
step 3563/400000: train loss: 0.232423
epoch:77, batch23, load frames use: 2.76203417778s
step 3564/400000: train loss: 0.221658
epoch:77, batch24, load frames use: 3.6159389019s
step 3565/400000: train loss: 0.231330
epoch:77, batch25, load frames use: 3.57537889481s
step 3566/400000: train loss: 0.261545
epoch:77, batch26, load frames use: 2.95219182968s
step 3567/400000: train loss: 0.247805
epoch:77, batch27, load frames use: 3.32495212555s
step 3568/400000: train loss: 0.221481
epoch:77, batch28, load frames use: 3.51363801956s
step 3569/400000: train loss: 0.231485
epoch:77, batch29, load frames use: 3.34929800034s
step 3570/400000: train loss: 0.239000
epoch:77, batch30, load frames use: 2.94015789032s
step 3571/400000: train loss: 0.255185
epoch:77, batch31, load frames use: 4.36994695663s
step 3572/400000: train loss: 0.265599
epoch:77, batch32, load frames use: 3.85645413399s
step 3573/400000: train loss: 0.235790
epoch:77, batch33, load frames use: 3.20609307289s
step 3574/400000: train loss: 0.217824
epoch:77, batch34, load frames use: 2.97988700867s
step 3575/400000: train loss: 0.255915
epoch:77, batch35, load frames use: 3.32703089714s
step 3576/400000: train loss: 0.270790
epoch:77, batch36, load frames use: 3.76493191719s
step 3577/400000: train loss: 0.262085
epoch:77, batch37, load frames use: 3.01831197739s
step 3578/400000: train loss: 0.256952
epoch:77, batch38, load frames use: 4.19141697884s
step 3579/400000: train loss: 0.215546
epoch:77, batch39, load frames use: 3.88492012024s
step 3580/400000: train loss: 0.255755
epoch:77, batch40, load frames use: 3.44264602661s
step 3581/400000: train loss: 0.237079
epoch:77, batch41, load frames use: 3.05729913712s
step 3582/400000: train loss: 0.240326
epoch:77, batch42, load frames use: 4.08746886253s
step 3583/400000: train loss: 0.253818
epoch:77, batch43, load frames use: 2.9867579937s
step 3584/400000: train loss: 0.249549
epoch:77, batch44, load frames use: 2.76864314079s
step 3585/400000: train loss: 0.211383
epoch:77, batch45, load frames use: 3.9769859314s
step 3586/400000: train loss: 0.282795
epoch:77, batch46, load frames use: 4.30575895309s
step 3587/400000: train loss: 0.234984
epoch:78, batch1, load frames use: 2.69617795944s
step 3588/400000: train loss: 0.222346
epoch:78, batch2, load frames use: 4.45413804054s
step 3589/400000: train loss: 0.224495
epoch:78, batch3, load frames use: 3.22160696983s
step 3590/400000: train loss: 0.260099
epoch:78, batch4, load frames use: 7.21032905579s
step 3591/400000: train loss: 0.266733
epoch:78, batch5, load frames use: 3.81474995613s
step 3592/400000: train loss: 0.245585
epoch:78, batch6, load frames use: 3.62834692001s
step 3593/400000: train loss: 0.221160
epoch:78, batch7, load frames use: 3.27321505547s
step 3594/400000: train loss: 0.262808
epoch:78, batch8, load frames use: 4.47529220581s
step 3595/400000: train loss: 0.215276
epoch:78, batch9, load frames use: 3.37493515015s
step 3596/400000: train loss: 0.213391
epoch:78, batch10, load frames use: 5.61098504066s
step 3597/400000: train loss: 0.226755
epoch:78, batch11, load frames use: 3.29854202271s
step 3598/400000: train loss: 0.256368
epoch:78, batch12, load frames use: 3.44359707832s
step 3599/400000: train loss: 0.228647
epoch:78, batch13, load frames use: 2.56042695045s
step 3600/400000: train loss: 0.246713
step 3600/400000: validation loss: 0.221318
epoch:78, batch14, load frames use: 4.30899500847s
step 3601/400000: train loss: 0.231185
epoch:78, batch15, load frames use: 3.41423606873s
step 3602/400000: train loss: 0.214367
epoch:78, batch16, load frames use: 6.60858798027s
step 3603/400000: train loss: 0.219482
epoch:78, batch17, load frames use: 3.48522686958s
step 3604/400000: train loss: 0.234210
epoch:78, batch18, load frames use: 5.47737288475s
step 3605/400000: train loss: 0.185184
epoch:78, batch19, load frames use: 4.38468503952s
step 3606/400000: train loss: 0.265731
epoch:78, batch20, load frames use: 3.32251787186s
step 3607/400000: train loss: 0.251237
epoch:78, batch21, load frames use: 4.43128585815s
step 3608/400000: train loss: 0.256638
epoch:78, batch22, load frames use: 6.29816102982s
step 3609/400000: train loss: 0.219388
epoch:78, batch23, load frames use: 2.90758299828s
step 3610/400000: train loss: 0.230520
epoch:78, batch24, load frames use: 3.80591702461s
step 3611/400000: train loss: 0.240985
epoch:78, batch25, load frames use: 4.02595186234s
step 3612/400000: train loss: 0.267572
epoch:78, batch26, load frames use: 3.56940889359s
step 3613/400000: train loss: 0.238837
epoch:78, batch27, load frames use: 4.18359684944s
step 3614/400000: train loss: 0.260576
epoch:78, batch28, load frames use: 4.31744503975s
step 3615/400000: train loss: 0.264603
epoch:78, batch29, load frames use: 3.71290397644s
step 3616/400000: train loss: 0.211421
epoch:78, batch30, load frames use: 2.99892091751s
step 3617/400000: train loss: 0.239615
epoch:78, batch31, load frames use: 2.81759285927s
step 3618/400000: train loss: 0.245637
epoch:78, batch32, load frames use: 3.91407394409s
step 3619/400000: train loss: 0.247526
epoch:78, batch33, load frames use: 3.40978908539s
step 3620/400000: train loss: 0.262866
epoch:78, batch34, load frames use: 4.12390899658s
step 3621/400000: train loss: 0.260986
epoch:78, batch35, load frames use: 3.5217499733s
step 3622/400000: train loss: 0.238806
epoch:78, batch36, load frames use: 4.27996015549s
step 3623/400000: train loss: 0.233537
epoch:78, batch37, load frames use: 4.22637796402s
step 3624/400000: train loss: 0.234230
epoch:78, batch38, load frames use: 3.60983300209s
step 3625/400000: train loss: 0.217346
epoch:78, batch39, load frames use: 5.78700017929s
step 3626/400000: train loss: 0.217254
epoch:78, batch40, load frames use: 3.74268889427s
step 3627/400000: train loss: 0.223171
epoch:78, batch41, load frames use: 2.98370814323s
step 3628/400000: train loss: 0.246340
epoch:78, batch42, load frames use: 3.5144329071s
step 3629/400000: train loss: 0.209343
epoch:78, batch43, load frames use: 3.56415987015s
step 3630/400000: train loss: 0.224910
epoch:78, batch44, load frames use: 3.03095817566s
step 3631/400000: train loss: 0.215202
epoch:78, batch45, load frames use: 3.08348703384s
step 3632/400000: train loss: 0.244465
epoch:78, batch46, load frames use: 3.20350599289s
step 3633/400000: train loss: 0.240218
epoch:79, batch1, load frames use: 4.02270507812s
step 3634/400000: train loss: 0.246744
epoch:79, batch2, load frames use: 3.06995487213s
step 3635/400000: train loss: 0.218404
epoch:79, batch3, load frames use: 3.29321193695s
step 3636/400000: train loss: 0.256069
epoch:79, batch4, load frames use: 3.11209201813s
step 3637/400000: train loss: 0.261787
epoch:79, batch5, load frames use: 3.30491995811s
step 3638/400000: train loss: 0.209362
epoch:79, batch6, load frames use: 3.0133638382s
step 3639/400000: train loss: 0.218669
epoch:79, batch7, load frames use: 3.69364905357s
step 3640/400000: train loss: 0.238987
epoch:79, batch8, load frames use: 2.89376592636s
step 3641/400000: train loss: 0.248900
epoch:79, batch9, load frames use: 4.33465409279s
step 3642/400000: train loss: 0.223320
epoch:79, batch10, load frames use: 3.23631501198s
step 3643/400000: train loss: 0.214493
epoch:79, batch11, load frames use: 4.99161505699s
step 3644/400000: train loss: 0.235383
epoch:79, batch12, load frames use: 4.38154506683s
step 3645/400000: train loss: 0.234385
epoch:79, batch13, load frames use: 3.12542891502s
step 3646/400000: train loss: 0.234281
epoch:79, batch14, load frames use: 3.32351183891s
step 3647/400000: train loss: 0.205954
epoch:79, batch15, load frames use: 3.69825005531s
step 3648/400000: train loss: 0.191133
epoch:79, batch16, load frames use: 3.01881098747s
step 3649/400000: train loss: 0.228806
epoch:79, batch17, load frames use: 3.27737593651s
step 3650/400000: train loss: 0.238904
epoch:79, batch18, load frames use: 3.25734400749s
step 3651/400000: train loss: 0.230653
epoch:79, batch19, load frames use: 3.44742488861s
step 3652/400000: train loss: 0.246014
epoch:79, batch20, load frames use: 3.03430199623s
step 3653/400000: train loss: 0.231736
epoch:79, batch21, load frames use: 3.59042906761s
step 3654/400000: train loss: 0.219157
epoch:79, batch22, load frames use: 3.24220705032s
step 3655/400000: train loss: 0.269840
epoch:79, batch23, load frames use: 3.16406488419s
step 3656/400000: train loss: 0.219382
epoch:79, batch24, load frames use: 2.92459702492s
step 3657/400000: train loss: 0.228181
epoch:79, batch25, load frames use: 3.8744161129s
step 3658/400000: train loss: 0.282869
epoch:79, batch26, load frames use: 3.52493691444s
step 3659/400000: train loss: 0.233289
epoch:79, batch27, load frames use: 3.42361092567s
step 3660/400000: train loss: 0.235270
epoch:79, batch28, load frames use: 2.84183883667s
step 3661/400000: train loss: 0.208137
epoch:79, batch29, load frames use: 4.25010991096s
step 3662/400000: train loss: 0.229065
epoch:79, batch30, load frames use: 3.90553593636s
step 3663/400000: train loss: 0.224846
epoch:79, batch31, load frames use: 3.04019093513s
step 3664/400000: train loss: 0.228660
epoch:79, batch32, load frames use: 3.76854491234s
step 3665/400000: train loss: 0.226617
epoch:79, batch33, load frames use: 3.13722991943s
step 3666/400000: train loss: 0.245696
epoch:79, batch34, load frames use: 3.06934404373s
step 3667/400000: train loss: 0.261952
epoch:79, batch35, load frames use: 3.23108696938s
step 3668/400000: train loss: 0.227342
epoch:79, batch36, load frames use: 2.77261185646s
step 3669/400000: train loss: 0.281383
epoch:79, batch37, load frames use: 3.74786400795s
step 3670/400000: train loss: 0.222860
epoch:79, batch38, load frames use: 4.02957010269s
step 3671/400000: train loss: 0.220101
epoch:79, batch39, load frames use: 6.61048293114s
step 3672/400000: train loss: 0.224608
epoch:79, batch40, load frames use: 2.90053701401s
step 3673/400000: train loss: 0.231116
epoch:79, batch41, load frames use: 3.62344384193s
step 3674/400000: train loss: 0.221305
epoch:79, batch42, load frames use: 5.57102584839s
step 3675/400000: train loss: 0.232046
epoch:79, batch43, load frames use: 3.68321013451s
step 3676/400000: train loss: 0.253528
epoch:79, batch44, load frames use: 4.82164001465s
step 3677/400000: train loss: 0.221538
epoch:79, batch45, load frames use: 4.25708580017s
step 3678/400000: train loss: 0.252063
epoch:79, batch46, load frames use: 3.9200720787s
step 3679/400000: train loss: 0.250391
epoch:80, batch1, load frames use: 4.23379802704s
step 3680/400000: train loss: 0.235415
epoch:80, batch2, load frames use: 7.4008910656s
step 3681/400000: train loss: 0.249564
epoch:80, batch3, load frames use: 4.97347092628s
step 3682/400000: train loss: 0.222960
epoch:80, batch4, load frames use: 4.43743491173s
step 3683/400000: train loss: 0.219076
epoch:80, batch5, load frames use: 4.40477609634s
step 3684/400000: train loss: 0.228243
epoch:80, batch6, load frames use: 4.0422308445s
step 3685/400000: train loss: 0.239510
epoch:80, batch7, load frames use: 3.10094380379s
step 3686/400000: train loss: 0.214469
epoch:80, batch8, load frames use: 4.07156610489s
step 3687/400000: train loss: 0.210908
epoch:80, batch9, load frames use: 4.85245990753s
step 3688/400000: train loss: 0.224485
epoch:80, batch10, load frames use: 3.20099711418s
step 3689/400000: train loss: 0.269793
epoch:80, batch11, load frames use: 3.58364105225s
step 3690/400000: train loss: 0.228525
epoch:80, batch12, load frames use: 3.32958602905s
step 3691/400000: train loss: 0.236032
epoch:80, batch13, load frames use: 3.9190299511s
step 3692/400000: train loss: 0.211467
epoch:80, batch14, load frames use: 4.25628685951s
step 3693/400000: train loss: 0.221296
epoch:80, batch15, load frames use: 2.93319606781s
step 3694/400000: train loss: 0.268258
epoch:80, batch16, load frames use: 3.57129383087s
step 3695/400000: train loss: 0.245254
epoch:80, batch17, load frames use: 3.8230741024s
step 3696/400000: train loss: 0.195490
epoch:80, batch18, load frames use: 2.8319041729s
step 3697/400000: train loss: 0.260671
epoch:80, batch19, load frames use: 4.2770819664s
step 3698/400000: train loss: 0.245193
epoch:80, batch20, load frames use: 4.49739813805s
step 3699/400000: train loss: 0.209858
epoch:80, batch21, load frames use: 3.60411190987s
step 3700/400000: train loss: 0.211104
step 3700/400000: validation loss: 0.208830
epoch:80, batch22, load frames use: 3.4430680275s
step 3701/400000: train loss: 0.209217
epoch:80, batch23, load frames use: 3.05755090714s
step 3702/400000: train loss: 0.264534
epoch:80, batch24, load frames use: 3.13251590729s
step 3703/400000: train loss: 0.220278
epoch:80, batch25, load frames use: 2.70180296898s
step 3704/400000: train loss: 0.235374
epoch:80, batch26, load frames use: 3.18000793457s
step 3705/400000: train loss: 0.264708
epoch:80, batch27, load frames use: 4.41534805298s
step 3706/400000: train loss: 0.220278
epoch:80, batch28, load frames use: 3.74741005898s
step 3707/400000: train loss: 0.229518
epoch:80, batch29, load frames use: 3.53984808922s
step 3708/400000: train loss: 0.237562
epoch:80, batch30, load frames use: 2.93843197823s
step 3709/400000: train loss: 0.237600
epoch:80, batch31, load frames use: 3.5471739769s
step 3710/400000: train loss: 0.218074
epoch:80, batch32, load frames use: 3.04999804497s
step 3711/400000: train loss: 0.248025
epoch:80, batch33, load frames use: 3.26622009277s
step 3712/400000: train loss: 0.251276
epoch:80, batch34, load frames use: 3.88011002541s
step 3713/400000: train loss: 0.203810
epoch:80, batch35, load frames use: 4.0372171402s
step 3714/400000: train loss: 0.231721
epoch:80, batch36, load frames use: 3.0299038887s
step 3715/400000: train loss: 0.247535
epoch:80, batch37, load frames use: 3.39221191406s
step 3716/400000: train loss: 0.220239
epoch:80, batch38, load frames use: 3.57998800278s
step 3717/400000: train loss: 0.249524
epoch:80, batch39, load frames use: 3.26190495491s
step 3718/400000: train loss: 0.240899
epoch:80, batch40, load frames use: 3.39758896828s
step 3719/400000: train loss: 0.219903
epoch:80, batch41, load frames use: 3.41189885139s
step 3720/400000: train loss: 0.211954
epoch:80, batch42, load frames use: 3.64538216591s
step 3721/400000: train loss: 0.259548
epoch:80, batch43, load frames use: 3.2167198658s
step 3722/400000: train loss: 0.239391
epoch:80, batch44, load frames use: 3.88135313988s
step 3723/400000: train loss: 0.196754
epoch:80, batch45, load frames use: 3.7374651432s
step 3724/400000: train loss: 0.222864
epoch:80, batch46, load frames use: 3.69798994064s
step 3725/400000: train loss: 0.220751
epoch:81, batch1, load frames use: 3.43590211868s
step 3726/400000: train loss: 0.215912
epoch:81, batch2, load frames use: 3.62994599342s
step 3727/400000: train loss: 0.227104
epoch:81, batch3, load frames use: 3.655397892s
step 3728/400000: train loss: 0.227326
epoch:81, batch4, load frames use: 3.50403904915s
step 3729/400000: train loss: 0.233792
epoch:81, batch5, load frames use: 3.16331887245s
step 3730/400000: train loss: 0.229311
epoch:81, batch6, load frames use: 4.17878293991s
step 3731/400000: train loss: 0.255296
epoch:81, batch7, load frames use: 2.95696091652s
step 3732/400000: train loss: 0.218546
epoch:81, batch8, load frames use: 4.53974413872s
step 3733/400000: train loss: 0.238119
epoch:81, batch9, load frames use: 3.85942721367s
step 3734/400000: train loss: 0.213232
epoch:81, batch10, load frames use: 3.41858482361s
step 3735/400000: train loss: 0.236053
epoch:81, batch11, load frames use: 3.53347301483s
step 3736/400000: train loss: 0.225205
epoch:81, batch12, load frames use: 3.73089790344s
step 3737/400000: train loss: 0.204592
epoch:81, batch13, load frames use: 3.52013087273s
step 3738/400000: train loss: 0.242448
epoch:81, batch14, load frames use: 4.70620989799s
step 3739/400000: train loss: 0.228127
epoch:81, batch15, load frames use: 6.42927598953s
step 3740/400000: train loss: 0.232358
epoch:81, batch16, load frames use: 6.15964913368s
step 3741/400000: train loss: 0.227482
epoch:81, batch17, load frames use: 8.25888609886s
step 3742/400000: train loss: 0.233657
epoch:81, batch18, load frames use: 3.65871810913s
step 3743/400000: train loss: 0.241604
epoch:81, batch19, load frames use: 3.46655011177s
step 3744/400000: train loss: 0.208162
epoch:81, batch20, load frames use: 4.03230309486s
step 3745/400000: train loss: 0.217094
epoch:81, batch21, load frames use: 5.33131098747s
step 3746/400000: train loss: 0.211676
epoch:81, batch22, load frames use: 2.93978214264s
step 3747/400000: train loss: 0.217238
epoch:81, batch23, load frames use: 3.93611502647s
step 3748/400000: train loss: 0.203756
epoch:81, batch24, load frames use: 3.47546577454s
step 3749/400000: train loss: 0.216773
epoch:81, batch25, load frames use: 3.38936185837s
step 3750/400000: train loss: 0.242227
epoch:81, batch26, load frames use: 2.99726200104s
step 3751/400000: train loss: 0.241662
epoch:81, batch27, load frames use: 4.41120195389s
step 3752/400000: train loss: 0.229580
epoch:81, batch28, load frames use: 3.23005795479s
step 3753/400000: train loss: 0.243317
epoch:81, batch29, load frames use: 3.02314591408s
step 3754/400000: train loss: 0.233015
epoch:81, batch30, load frames use: 3.09995985031s
step 3755/400000: train loss: 0.256088
epoch:81, batch31, load frames use: 2.93795013428s
step 3756/400000: train loss: 0.255639
epoch:81, batch32, load frames use: 3.12267303467s
step 3757/400000: train loss: 0.246530
epoch:81, batch33, load frames use: 4.31170797348s
step 3758/400000: train loss: 0.252719
epoch:81, batch34, load frames use: 3.0701520443s
step 3759/400000: train loss: 0.219237
epoch:81, batch35, load frames use: 3.49781489372s
step 3760/400000: train loss: 0.233376
epoch:81, batch36, load frames use: 3.22815704346s
step 3761/400000: train loss: 0.212173
epoch:81, batch37, load frames use: 4.10937714577s
step 3762/400000: train loss: 0.243261
epoch:81, batch38, load frames use: 4.39454293251s
step 3763/400000: train loss: 0.237644
epoch:81, batch39, load frames use: 3.05297803879s
step 3764/400000: train loss: 0.224246
epoch:81, batch40, load frames use: 3.85715389252s
step 3765/400000: train loss: 0.237848
epoch:81, batch41, load frames use: 3.05708312988s
step 3766/400000: train loss: 0.226988
epoch:81, batch42, load frames use: 3.49110794067s
step 3767/400000: train loss: 0.186691
epoch:81, batch43, load frames use: 3.28321194649s
step 3768/400000: train loss: 0.204923
epoch:81, batch44, load frames use: 3.91644191742s
step 3769/400000: train loss: 0.215740
epoch:81, batch45, load frames use: 2.98102807999s
step 3770/400000: train loss: 0.243933
epoch:81, batch46, load frames use: 3.30755996704s
step 3771/400000: train loss: 0.226731
epoch:82, batch1, load frames use: 3.11848711967s
step 3772/400000: train loss: 0.216585
epoch:82, batch2, load frames use: 4.64786505699s
step 3773/400000: train loss: 0.204787
epoch:82, batch3, load frames use: 3.98072695732s
step 3774/400000: train loss: 0.230875
epoch:82, batch4, load frames use: 3.24244689941s
step 3775/400000: train loss: 0.246953
epoch:82, batch5, load frames use: 2.95288586617s
step 3776/400000: train loss: 0.219176
epoch:82, batch6, load frames use: 3.19025492668s
step 3777/400000: train loss: 0.220862
epoch:82, batch7, load frames use: 3.71049189568s
step 3778/400000: train loss: 0.230829
epoch:82, batch8, load frames use: 2.89522790909s
step 3779/400000: train loss: 0.216911
epoch:82, batch9, load frames use: 4.40813016891s
step 3780/400000: train loss: 0.207075
epoch:82, batch10, load frames use: 3.68771696091s
step 3781/400000: train loss: 0.202014
epoch:82, batch11, load frames use: 4.17747187614s
step 3782/400000: train loss: 0.211991
epoch:82, batch12, load frames use: 2.77681303024s
step 3783/400000: train loss: 0.245101
epoch:82, batch13, load frames use: 4.52118301392s
step 3784/400000: train loss: 0.210413
epoch:82, batch14, load frames use: 3.92613387108s
step 3785/400000: train loss: 0.241146
epoch:82, batch15, load frames use: 3.20033097267s
step 3786/400000: train loss: 0.257783
epoch:82, batch16, load frames use: 3.7096362114s
step 3787/400000: train loss: 0.223520
epoch:82, batch17, load frames use: 3.01433610916s
step 3788/400000: train loss: 0.204665
epoch:82, batch18, load frames use: 3.41006612778s
step 3789/400000: train loss: 0.205223
epoch:82, batch19, load frames use: 2.8621070385s
step 3790/400000: train loss: 0.236437
epoch:82, batch20, load frames use: 4.24846816063s
step 3791/400000: train loss: 0.204698
epoch:82, batch21, load frames use: 3.42335987091s
step 3792/400000: train loss: 0.216544
epoch:82, batch22, load frames use: 2.71295595169s
step 3793/400000: train loss: 0.241968
epoch:82, batch23, load frames use: 3.16274189949s
step 3794/400000: train loss: 0.198236
epoch:82, batch24, load frames use: 3.25586009026s
step 3795/400000: train loss: 0.218487
epoch:82, batch25, load frames use: 3.07136392593s
step 3796/400000: train loss: 0.220623
epoch:82, batch26, load frames use: 4.94780993462s
step 3797/400000: train loss: 0.205208
epoch:82, batch27, load frames use: 3.49576807022s
step 3798/400000: train loss: 0.226107
epoch:82, batch28, load frames use: 4.66000819206s
step 3799/400000: train loss: 0.228438
epoch:82, batch29, load frames use: 3.5261490345s
step 3800/400000: train loss: 0.216295
step 3800/400000: validation loss: 0.211456
epoch:82, batch30, load frames use: 3.30762910843s
step 3801/400000: train loss: 0.241673
epoch:82, batch31, load frames use: 3.62761688232s
step 3802/400000: train loss: 0.268099
epoch:82, batch32, load frames use: 3.82056593895s
step 3803/400000: train loss: 0.246136
epoch:82, batch33, load frames use: 3.64112210274s
step 3804/400000: train loss: 0.226141
epoch:82, batch34, load frames use: 3.66527700424s
step 3805/400000: train loss: 0.207045
epoch:82, batch35, load frames use: 4.12983798981s
step 3806/400000: train loss: 0.227768
epoch:82, batch36, load frames use: 3.54887413979s
step 3807/400000: train loss: 0.238429
epoch:82, batch37, load frames use: 3.5494530201s
step 3808/400000: train loss: 0.200233
epoch:82, batch38, load frames use: 3.7343518734s
step 3809/400000: train loss: 0.229269
epoch:82, batch39, load frames use: 3.42291784286s
step 3810/400000: train loss: 0.202761
epoch:82, batch40, load frames use: 3.29460310936s
step 3811/400000: train loss: 0.250932
epoch:82, batch41, load frames use: 3.56613302231s
step 3812/400000: train loss: 0.201533
epoch:82, batch42, load frames use: 3.73015403748s
step 3813/400000: train loss: 0.218390
epoch:82, batch43, load frames use: 3.94369482994s
step 3814/400000: train loss: 0.225601
epoch:82, batch44, load frames use: 3.4496421814s
step 3815/400000: train loss: 0.242091
epoch:82, batch45, load frames use: 4.07443213463s
step 3816/400000: train loss: 0.221285
epoch:82, batch46, load frames use: 4.05122494698s
step 3817/400000: train loss: 0.237260
epoch:83, batch1, load frames use: 4.22989797592s
step 3818/400000: train loss: 0.213361
epoch:83, batch2, load frames use: 3.20966720581s
step 3819/400000: train loss: 0.234187
epoch:83, batch3, load frames use: 3.49875593185s
step 3820/400000: train loss: 0.215150
epoch:83, batch4, load frames use: 4.21212792397s
step 3821/400000: train loss: 0.187815
epoch:83, batch5, load frames use: 3.42020106316s
step 3822/400000: train loss: 0.217613
epoch:83, batch6, load frames use: 3.72670698166s
step 3823/400000: train loss: 0.251220
epoch:83, batch7, load frames use: 3.3507540226s
step 3824/400000: train loss: 0.226455
epoch:83, batch8, load frames use: 3.6194999218s
step 3825/400000: train loss: 0.252941
epoch:83, batch9, load frames use: 4.22406101227s
step 3826/400000: train loss: 0.210523
epoch:83, batch10, load frames use: 3.58072495461s
step 3827/400000: train loss: 0.220515
epoch:83, batch11, load frames use: 3.7211329937s
step 3828/400000: train loss: 0.211360
epoch:83, batch12, load frames use: 3.29263019562s
step 3829/400000: train loss: 0.212612
epoch:83, batch13, load frames use: 4.2983891964s
step 3830/400000: train loss: 0.227478
epoch:83, batch14, load frames use: 4.11307191849s
step 3831/400000: train loss: 0.227702
epoch:83, batch15, load frames use: 3.36715102196s
step 3832/400000: train loss: 0.223471
epoch:83, batch16, load frames use: 3.71230697632s
step 3833/400000: train loss: 0.218003
epoch:83, batch17, load frames use: 3.27417016029s
step 3834/400000: train loss: 0.242690
epoch:83, batch18, load frames use: 4.61633300781s
step 3835/400000: train loss: 0.251804
epoch:83, batch19, load frames use: 4.49159908295s
step 3836/400000: train loss: 0.243274
epoch:83, batch20, load frames use: 4.00958299637s
step 3837/400000: train loss: 0.214332
epoch:83, batch21, load frames use: 3.82089495659s
step 3838/400000: train loss: 0.225319
epoch:83, batch22, load frames use: 3.52421498299s
step 3839/400000: train loss: 0.217994
epoch:83, batch23, load frames use: 3.81321406364s
step 3840/400000: train loss: 0.236356
epoch:83, batch24, load frames use: 3.78910899162s
step 3841/400000: train loss: 0.202256
epoch:83, batch25, load frames use: 3.91829681396s
step 3842/400000: train loss: 0.224730
epoch:83, batch26, load frames use: 3.79341006279s
step 3843/400000: train loss: 0.217596
epoch:83, batch27, load frames use: 3.70338797569s
step 3844/400000: train loss: 0.234214
epoch:83, batch28, load frames use: 3.67643094063s
step 3845/400000: train loss: 0.230172
epoch:83, batch29, load frames use: 3.57804918289s
step 3846/400000: train loss: 0.203940
epoch:83, batch30, load frames use: 4.32887983322s
step 3847/400000: train loss: 0.235658
epoch:83, batch31, load frames use: 3.31153297424s
step 3848/400000: train loss: 0.238139
epoch:83, batch32, load frames use: 4.29791688919s
step 3849/400000: train loss: 0.239398
epoch:83, batch33, load frames use: 7.03323483467s
step 3850/400000: train loss: 0.224161
epoch:83, batch34, load frames use: 6.18787288666s
step 3851/400000: train loss: 0.240487
epoch:83, batch35, load frames use: 4.17360901833s
step 3852/400000: train loss: 0.211807
epoch:83, batch36, load frames use: 7.07641100883s
step 3853/400000: train loss: 0.234988
epoch:83, batch37, load frames use: 4.65491914749s
step 3854/400000: train loss: 0.244681
epoch:83, batch38, load frames use: 4.76772499084s
step 3855/400000: train loss: 0.199803
epoch:83, batch39, load frames use: 4.22540903091s
step 3856/400000: train loss: 0.227246
epoch:83, batch40, load frames use: 5.02894592285s
step 3857/400000: train loss: 0.246575
epoch:83, batch41, load frames use: 5.16397309303s
step 3858/400000: train loss: 0.199670
epoch:83, batch42, load frames use: 3.94063997269s
step 3859/400000: train loss: 0.245792
epoch:83, batch43, load frames use: 5.77133703232s
step 3860/400000: train loss: 0.214579
epoch:83, batch44, load frames use: 6.79143309593s
step 3861/400000: train loss: 0.254646
epoch:83, batch45, load frames use: 6.96624588966s
step 3862/400000: train loss: 0.256761
epoch:83, batch46, load frames use: 6.67916202545s
step 3863/400000: train loss: 0.219307
epoch:84, batch1, load frames use: 3.51038002968s
step 3864/400000: train loss: 0.196066
epoch:84, batch2, load frames use: 4.62823295593s
step 3865/400000: train loss: 0.188275
epoch:84, batch3, load frames use: 4.15226483345s
step 3866/400000: train loss: 0.220764
epoch:84, batch4, load frames use: 7.18081617355s
step 3867/400000: train loss: 0.226203
epoch:84, batch5, load frames use: 3.28408098221s
step 3868/400000: train loss: 0.201065
epoch:84, batch6, load frames use: 6.85822606087s
step 3869/400000: train loss: 0.207475
epoch:84, batch7, load frames use: 5.38688516617s
step 3870/400000: train loss: 0.220019
epoch:84, batch8, load frames use: 3.35280609131s
step 3871/400000: train loss: 0.216922
epoch:84, batch9, load frames use: 3.08768200874s
step 3872/400000: train loss: 0.216066
epoch:84, batch10, load frames use: 3.65183091164s
step 3873/400000: train loss: 0.217309
epoch:84, batch11, load frames use: 3.86612010002s
step 3874/400000: train loss: 0.225204
epoch:84, batch12, load frames use: 3.67975401878s
step 3875/400000: train loss: 0.205399
epoch:84, batch13, load frames use: 6.90469884872s
step 3876/400000: train loss: 0.218933
epoch:84, batch14, load frames use: 6.38932299614s
step 3877/400000: train loss: 0.208708
epoch:84, batch15, load frames use: 5.42737913132s
step 3878/400000: train loss: 0.200861
epoch:84, batch16, load frames use: 3.74629092216s
step 3879/400000: train loss: 0.214636
epoch:84, batch17, load frames use: 3.75864291191s
step 3880/400000: train loss: 0.195775
epoch:84, batch18, load frames use: 6.68283200264s
step 3881/400000: train loss: 0.221889
epoch:84, batch19, load frames use: 7.05392599106s
step 3882/400000: train loss: 0.230118
epoch:84, batch20, load frames use: 3.31258606911s
step 3883/400000: train loss: 0.231569
epoch:84, batch21, load frames use: 4.3051841259s
step 3884/400000: train loss: 0.203548
epoch:84, batch22, load frames use: 5.09689688683s
step 3885/400000: train loss: 0.193619
epoch:84, batch23, load frames use: 4.24509000778s
step 3886/400000: train loss: 0.228477
epoch:84, batch24, load frames use: 4.56553792953s
step 3887/400000: train loss: 0.231072
epoch:84, batch25, load frames use: 4.34528684616s
step 3888/400000: train loss: 0.209626
epoch:84, batch26, load frames use: 4.97873306274s
step 3889/400000: train loss: 0.222055
epoch:84, batch27, load frames use: 6.74699997902s
step 3890/400000: train loss: 0.197371
epoch:84, batch28, load frames use: 4.46598601341s
step 3891/400000: train loss: 0.211604
epoch:84, batch29, load frames use: 3.04053997993s
step 3892/400000: train loss: 0.230515
epoch:84, batch30, load frames use: 4.40695405006s
step 3893/400000: train loss: 0.235915
epoch:84, batch31, load frames use: 6.90111494064s
step 3894/400000: train loss: 0.240153
epoch:84, batch32, load frames use: 3.39610981941s
step 3895/400000: train loss: 0.233261
epoch:84, batch33, load frames use: 7.65165901184s
step 3896/400000: train loss: 0.229210
epoch:84, batch34, load frames use: 5.74695897102s
step 3897/400000: train loss: 0.202707
epoch:84, batch35, load frames use: 3.55592894554s
step 3898/400000: train loss: 0.219377
epoch:84, batch36, load frames use: 3.62773704529s
step 3899/400000: train loss: 0.239407
epoch:84, batch37, load frames use: 3.94418501854s
step 3900/400000: train loss: 0.252079
step 3900/400000: validation loss: 0.250083
epoch:84, batch38, load frames use: 3.79517006874s
step 3901/400000: train loss: 0.227991
epoch:84, batch39, load frames use: 3.1542930603s
step 3902/400000: train loss: 0.251946
epoch:84, batch40, load frames use: 4.4673140049s
step 3903/400000: train loss: 0.224110
epoch:84, batch41, load frames use: 3.85824108124s
step 3904/400000: train loss: 0.223458
epoch:84, batch42, load frames use: 4.50286006927s
step 3905/400000: train loss: 0.221287
epoch:84, batch43, load frames use: 4.44050884247s
step 3906/400000: train loss: 0.215015
epoch:84, batch44, load frames use: 4.53054213524s
step 3907/400000: train loss: 0.227012
epoch:84, batch45, load frames use: 3.68861985207s
step 3908/400000: train loss: 0.238150
epoch:84, batch46, load frames use: 4.99683880806s
step 3909/400000: train loss: 0.215649
epoch:85, batch1, load frames use: 5.93877720833s
step 3910/400000: train loss: 0.210404
epoch:85, batch2, load frames use: 4.05759501457s
step 3911/400000: train loss: 0.222400
epoch:85, batch3, load frames use: 6.24521899223s
step 3912/400000: train loss: 0.206760
epoch:85, batch4, load frames use: 3.59425210953s
step 3913/400000: train loss: 0.228470
epoch:85, batch5, load frames use: 4.69954395294s
step 3914/400000: train loss: 0.207208
epoch:85, batch6, load frames use: 6.09939694405s
step 3915/400000: train loss: 0.225566
epoch:85, batch7, load frames use: 3.4769961834s
step 3916/400000: train loss: 0.201758
epoch:85, batch8, load frames use: 3.48931002617s
step 3917/400000: train loss: 0.214641
epoch:85, batch9, load frames use: 5.49549794197s
step 3918/400000: train loss: 0.221914
epoch:85, batch10, load frames use: 4.61879515648s
step 3919/400000: train loss: 0.221627
epoch:85, batch11, load frames use: 4.16543602943s
step 3920/400000: train loss: 0.240566
epoch:85, batch12, load frames use: 7.13817000389s
step 3921/400000: train loss: 0.226219
epoch:85, batch13, load frames use: 3.61359596252s
step 3922/400000: train loss: 0.225048
epoch:85, batch14, load frames use: 3.79141187668s
step 3923/400000: train loss: 0.241959
epoch:85, batch15, load frames use: 7.11537599564s
step 3924/400000: train loss: 0.219378
epoch:85, batch16, load frames use: 6.92894101143s
step 3925/400000: train loss: 0.214267
epoch:85, batch17, load frames use: 4.50137901306s
step 3926/400000: train loss: 0.210316
epoch:85, batch18, load frames use: 5.12532711029s
step 3927/400000: train loss: 0.236084
epoch:85, batch19, load frames use: 4.24129986763s
step 3928/400000: train loss: 0.237430
epoch:85, batch20, load frames use: 4.21287107468s
step 3929/400000: train loss: 0.213664
epoch:85, batch21, load frames use: 4.92014312744s
step 3930/400000: train loss: 0.220485
epoch:85, batch22, load frames use: 5.37711691856s
step 3931/400000: train loss: 0.226109
epoch:85, batch23, load frames use: 6.40738201141s
step 3932/400000: train loss: 0.210871
epoch:85, batch24, load frames use: 3.56641197205s
step 3933/400000: train loss: 0.218775
epoch:85, batch25, load frames use: 3.92414093018s
step 3934/400000: train loss: 0.211889
epoch:85, batch26, load frames use: 3.93121504784s
step 3935/400000: train loss: 0.219577
epoch:85, batch27, load frames use: 5.66495299339s
step 3936/400000: train loss: 0.245917
epoch:85, batch28, load frames use: 3.98286485672s
step 3937/400000: train loss: 0.236078
epoch:85, batch29, load frames use: 7.04678487778s
step 3938/400000: train loss: 0.216560
epoch:85, batch30, load frames use: 5.53020000458s
step 3939/400000: train loss: 0.200763
epoch:85, batch31, load frames use: 5.39241099358s
step 3940/400000: train loss: 0.248852
epoch:85, batch32, load frames use: 5.64124584198s
step 3941/400000: train loss: 0.246989
epoch:85, batch33, load frames use: 4.34995412827s
step 3942/400000: train loss: 0.232701
epoch:85, batch34, load frames use: 3.9384469986s
step 3943/400000: train loss: 0.246006
epoch:85, batch35, load frames use: 6.88336110115s
step 3944/400000: train loss: 0.216525
epoch:85, batch36, load frames use: 5.27803492546s
step 3945/400000: train loss: 0.210139
epoch:85, batch37, load frames use: 7.42449307442s
step 3946/400000: train loss: 0.255908
epoch:85, batch38, load frames use: 4.56003499031s
step 3947/400000: train loss: 0.226904
epoch:85, batch39, load frames use: 6.48034906387s
step 3948/400000: train loss: 0.226364
epoch:85, batch40, load frames use: 5.57063984871s
step 3949/400000: train loss: 0.201337
epoch:85, batch41, load frames use: 3.99085688591s
step 3950/400000: train loss: 0.209235
epoch:85, batch42, load frames use: 5.34956598282s
step 3951/400000: train loss: 0.213733
epoch:85, batch43, load frames use: 3.70013904572s
step 3952/400000: train loss: 0.222942
epoch:85, batch44, load frames use: 3.26512908936s
step 3953/400000: train loss: 0.238162
epoch:85, batch45, load frames use: 4.4637260437s
step 3954/400000: train loss: 0.225494
epoch:85, batch46, load frames use: 3.15937399864s
step 3955/400000: train loss: 0.238640
epoch:86, batch1, load frames use: 5.06624293327s
step 3956/400000: train loss: 0.208001
epoch:86, batch2, load frames use: 5.93769407272s
step 3957/400000: train loss: 0.218276
epoch:86, batch3, load frames use: 4.76363420486s
step 3958/400000: train loss: 0.211640
epoch:86, batch4, load frames use: 3.94975113869s
step 3959/400000: train loss: 0.196731
epoch:86, batch5, load frames use: 4.18745017052s
step 3960/400000: train loss: 0.195480
epoch:86, batch6, load frames use: 4.51957702637s
step 3961/400000: train loss: 0.218566
epoch:86, batch7, load frames use: 4.90463089943s
step 3962/400000: train loss: 0.227996
epoch:86, batch8, load frames use: 5.17015004158s
step 3963/400000: train loss: 0.209429
epoch:86, batch9, load frames use: 5.69668602943s
step 3964/400000: train loss: 0.202387
epoch:86, batch10, load frames use: 4.57608795166s
step 3965/400000: train loss: 0.207751
epoch:86, batch11, load frames use: 5.13269805908s
step 3966/400000: train loss: 0.213763
epoch:86, batch12, load frames use: 3.76533412933s
step 3967/400000: train loss: 0.203664
epoch:86, batch13, load frames use: 3.29557299614s
step 3968/400000: train loss: 0.236616
epoch:86, batch14, load frames use: 4.59125185013s
step 3969/400000: train loss: 0.218177
epoch:86, batch15, load frames use: 4.00781416893s
step 3970/400000: train loss: 0.223530
epoch:86, batch16, load frames use: 7.10559391975s
step 3971/400000: train loss: 0.199721
epoch:86, batch17, load frames use: 5.08286499977s
step 3972/400000: train loss: 0.209268
epoch:86, batch18, load frames use: 5.60613489151s
step 3973/400000: train loss: 0.197846
epoch:86, batch19, load frames use: 3.81236195564s
step 3974/400000: train loss: 0.239640
epoch:86, batch20, load frames use: 7.88972306252s
step 3975/400000: train loss: 0.196817
epoch:86, batch21, load frames use: 7.73236608505s
step 3976/400000: train loss: 0.199880
epoch:86, batch22, load frames use: 9.20665502548s
step 3977/400000: train loss: 0.201100
epoch:86, batch23, load frames use: 6.35575199127s
step 3978/400000: train loss: 0.216009
epoch:86, batch24, load frames use: 4.26415300369s
step 3979/400000: train loss: 0.205554
epoch:86, batch25, load frames use: 7.97508788109s
step 3980/400000: train loss: 0.233237
epoch:86, batch26, load frames use: 6.17825913429s
step 3981/400000: train loss: 0.229186
epoch:86, batch27, load frames use: 6.20659303665s
step 3982/400000: train loss: 0.214674
epoch:86, batch28, load frames use: 3.56955599785s
step 3983/400000: train loss: 0.192733
epoch:86, batch29, load frames use: 3.39179205894s
step 3984/400000: train loss: 0.217940
epoch:86, batch30, load frames use: 3.8624689579s
step 3985/400000: train loss: 0.211522
epoch:86, batch31, load frames use: 3.16465711594s
step 3986/400000: train loss: 0.190878
epoch:86, batch32, load frames use: 3.87693691254s
step 3987/400000: train loss: 0.195868
epoch:86, batch33, load frames use: 3.23293399811s
step 3988/400000: train loss: 0.222596
epoch:86, batch34, load frames use: 3.08754491806s
step 3989/400000: train loss: 0.203101
epoch:86, batch35, load frames use: 3.24171590805s
step 3990/400000: train loss: 0.204529
epoch:86, batch36, load frames use: 4.23919701576s
step 3991/400000: train loss: 0.244397
epoch:86, batch37, load frames use: 3.19861006737s
step 3992/400000: train loss: 0.217054
epoch:86, batch38, load frames use: 3.70485520363s
step 3993/400000: train loss: 0.241301
epoch:86, batch39, load frames use: 3.2372379303s
step 3994/400000: train loss: 0.209772
epoch:86, batch40, load frames use: 4.17119717598s
step 3995/400000: train loss: 0.219939
epoch:86, batch41, load frames use: 3.27008414268s
step 3996/400000: train loss: 0.260014
epoch:86, batch42, load frames use: 3.16490983963s
step 3997/400000: train loss: 0.227985
epoch:86, batch43, load frames use: 3.27431297302s
step 3998/400000: train loss: 0.232646
epoch:86, batch44, load frames use: 3.56407594681s
step 3999/400000: train loss: 0.208856
epoch:86, batch45, load frames use: 3.12672495842s
step 4000/400000: train loss: 0.224807
step 4000/400000: validation loss: 0.230701
epoch:86, batch46, load frames use: 4.00347113609s
step 4001/400000: train loss: 0.223328
epoch:87, batch1, load frames use: 3.61794900894s
step 4002/400000: train loss: 0.236744
epoch:87, batch2, load frames use: 3.36331701279s
step 4003/400000: train loss: 0.218002
epoch:87, batch3, load frames use: 3.53771996498s
step 4004/400000: train loss: 0.208220
epoch:87, batch4, load frames use: 2.77265501022s
step 4005/400000: train loss: 0.208775
epoch:87, batch5, load frames use: 3.50139093399s
step 4006/400000: train loss: 0.173557
epoch:87, batch6, load frames use: 2.85672402382s
step 4007/400000: train loss: 0.206251
epoch:87, batch7, load frames use: 3.23056578636s
step 4008/400000: train loss: 0.244378
epoch:87, batch8, load frames use: 3.21765804291s
step 4009/400000: train loss: 0.206432
epoch:87, batch9, load frames use: 3.24784088135s
step 4010/400000: train loss: 0.222147
epoch:87, batch10, load frames use: 2.85407686234s
step 4011/400000: train loss: 0.205332
epoch:87, batch11, load frames use: 4.41665291786s
step 4012/400000: train loss: 0.194746
epoch:87, batch12, load frames use: 4.1692969799s
step 4013/400000: train loss: 0.222926
epoch:87, batch13, load frames use: 3.09802293777s
step 4014/400000: train loss: 0.188392
epoch:87, batch14, load frames use: 2.78694701195s
step 4015/400000: train loss: 0.190633
epoch:87, batch15, load frames use: 4.09781599045s
step 4016/400000: train loss: 0.223145
epoch:87, batch16, load frames use: 3.95049905777s
step 4017/400000: train loss: 0.219271
epoch:87, batch17, load frames use: 3.41364097595s
step 4018/400000: train loss: 0.237790
epoch:87, batch18, load frames use: 3.06539201736s
step 4019/400000: train loss: 0.237090
epoch:87, batch19, load frames use: 3.70840191841s
step 4020/400000: train loss: 0.219781
epoch:87, batch20, load frames use: 3.20026707649s
step 4021/400000: train loss: 0.209311
epoch:87, batch21, load frames use: 3.23678898811s
step 4022/400000: train loss: 0.227865
epoch:87, batch22, load frames use: 3.589179039s
step 4023/400000: train loss: 0.202446
epoch:87, batch23, load frames use: 3.25380992889s
step 4024/400000: train loss: 0.237509
epoch:87, batch24, load frames use: 3.20170092583s
step 4025/400000: train loss: 0.217372
epoch:87, batch25, load frames use: 3.61765694618s
step 4026/400000: train loss: 0.231691
epoch:87, batch26, load frames use: 3.41878604889s
step 4027/400000: train loss: 0.206737
epoch:87, batch27, load frames use: 3.3447470665s
step 4028/400000: train loss: 0.206337
epoch:87, batch28, load frames use: 3.63252806664s
step 4029/400000: train loss: 0.202475
epoch:87, batch29, load frames use: 3.35044598579s
step 4030/400000: train loss: 0.214709
epoch:87, batch30, load frames use: 3.18526387215s
step 4031/400000: train loss: 0.219869
epoch:87, batch31, load frames use: 4.06644701958s
step 4032/400000: train loss: 0.214911
epoch:87, batch32, load frames use: 5.51591396332s
step 4033/400000: train loss: 0.228588
epoch:87, batch33, load frames use: 4.64304113388s
step 4034/400000: train loss: 0.221578
epoch:87, batch34, load frames use: 4.95195794106s
step 4035/400000: train loss: 0.191179
epoch:87, batch35, load frames use: 5.72947382927s
step 4036/400000: train loss: 0.230395
epoch:87, batch36, load frames use: 3.408233881s
step 4037/400000: train loss: 0.216237
epoch:87, batch37, load frames use: 4.67582798004s
step 4038/400000: train loss: 0.210851
epoch:87, batch38, load frames use: 3.89861297607s
step 4039/400000: train loss: 0.233536
epoch:87, batch39, load frames use: 6.92683291435s
step 4040/400000: train loss: 0.240214
epoch:87, batch40, load frames use: 4.75242805481s
step 4041/400000: train loss: 0.198882
epoch:87, batch41, load frames use: 5.07925200462s
step 4042/400000: train loss: 0.253244
epoch:87, batch42, load frames use: 2.95222902298s
step 4043/400000: train loss: 0.221203
epoch:87, batch43, load frames use: 4.18177700043s
step 4044/400000: train loss: 0.229662
epoch:87, batch44, load frames use: 6.60619282722s
step 4045/400000: train loss: 0.230685
epoch:87, batch45, load frames use: 3.64723610878s
step 4046/400000: train loss: 0.214593
epoch:87, batch46, load frames use: 4.61723399162s
step 4047/400000: train loss: 0.218308
epoch:88, batch1, load frames use: 3.55284285545s
step 4048/400000: train loss: 0.183933
epoch:88, batch2, load frames use: 2.91509103775s
step 4049/400000: train loss: 0.203979
epoch:88, batch3, load frames use: 3.3602411747s
step 4050/400000: train loss: 0.212106
epoch:88, batch4, load frames use: 3.5595870018s
step 4051/400000: train loss: 0.222453
epoch:88, batch5, load frames use: 3.02773594856s
step 4052/400000: train loss: 0.223064
epoch:88, batch6, load frames use: 3.51152014732s
step 4053/400000: train loss: 0.229624
epoch:88, batch7, load frames use: 3.59605789185s
step 4054/400000: train loss: 0.183676
epoch:88, batch8, load frames use: 3.81708598137s
step 4055/400000: train loss: 0.212353
epoch:88, batch9, load frames use: 3.14260983467s
step 4056/400000: train loss: 0.205582
epoch:88, batch10, load frames use: 3.39957499504s
step 4057/400000: train loss: 0.214736
epoch:88, batch11, load frames use: 3.75371599197s
step 4058/400000: train loss: 0.208216
epoch:88, batch12, load frames use: 3.45297002792s
step 4059/400000: train loss: 0.209400
epoch:88, batch13, load frames use: 3.31177902222s
step 4060/400000: train loss: 0.209560
epoch:88, batch14, load frames use: 3.02699899673s
step 4061/400000: train loss: 0.212792
epoch:88, batch15, load frames use: 3.29531502724s
step 4062/400000: train loss: 0.224437
epoch:88, batch16, load frames use: 3.4335000515s
step 4063/400000: train loss: 0.214934
epoch:88, batch17, load frames use: 3.05745887756s
step 4064/400000: train loss: 0.224899
epoch:88, batch18, load frames use: 3.77037191391s
step 4065/400000: train loss: 0.228618
epoch:88, batch19, load frames use: 3.53996992111s
step 4066/400000: train loss: 0.242788
epoch:88, batch20, load frames use: 3.46953296661s
step 4067/400000: train loss: 0.233199
epoch:88, batch21, load frames use: 2.81371188164s
step 4068/400000: train loss: 0.198042
epoch:88, batch22, load frames use: 3.42857503891s
step 4069/400000: train loss: 0.212042
epoch:88, batch23, load frames use: 3.33182096481s
step 4070/400000: train loss: 0.219901
epoch:88, batch24, load frames use: 2.89291000366s
step 4071/400000: train loss: 0.190089
epoch:88, batch25, load frames use: 3.46891379356s
step 4072/400000: train loss: 0.222148
epoch:88, batch26, load frames use: 4.45787215233s
step 4073/400000: train loss: 0.236858
epoch:88, batch27, load frames use: 3.73905301094s
step 4074/400000: train loss: 0.203120
epoch:88, batch28, load frames use: 2.76095795631s
step 4075/400000: train loss: 0.243253
epoch:88, batch29, load frames use: 3.8974981308s
step 4076/400000: train loss: 0.236818
epoch:88, batch30, load frames use: 3.86716914177s
step 4077/400000: train loss: 0.247972
epoch:88, batch31, load frames use: 3.80019307137s
step 4078/400000: train loss: 0.196109
epoch:88, batch32, load frames use: 3.54160904884s
step 4079/400000: train loss: 0.235900
epoch:88, batch33, load frames use: 4.68960595131s
step 4080/400000: train loss: 0.217467
epoch:88, batch34, load frames use: 2.78928613663s
step 4081/400000: train loss: 0.213916
epoch:88, batch35, load frames use: 3.5997800827s
step 4082/400000: train loss: 0.210546
epoch:88, batch36, load frames use: 3.56549096107s
step 4083/400000: train loss: 0.212227
epoch:88, batch37, load frames use: 2.96133995056s
step 4084/400000: train loss: 0.232483
epoch:88, batch38, load frames use: 3.6770670414s
step 4085/400000: train loss: 0.243927
epoch:88, batch39, load frames use: 3.09718298912s
step 4086/400000: train loss: 0.242554
epoch:88, batch40, load frames use: 3.47663092613s
step 4087/400000: train loss: 0.212458
epoch:88, batch41, load frames use: 3.03856897354s
step 4088/400000: train loss: 0.207524
epoch:88, batch42, load frames use: 3.36906003952s
step 4089/400000: train loss: 0.212332
epoch:88, batch43, load frames use: 3.39633607864s
step 4090/400000: train loss: 0.217801
epoch:88, batch44, load frames use: 2.85477495193s
step 4091/400000: train loss: 0.215757
epoch:88, batch45, load frames use: 3.89488387108s
step 4092/400000: train loss: 0.224716
epoch:88, batch46, load frames use: 3.06023693085s
step 4093/400000: train loss: 0.226311
epoch:89, batch1, load frames use: 4.54116415977s
step 4094/400000: train loss: 0.186531
epoch:89, batch2, load frames use: 3.88461995125s
step 4095/400000: train loss: 0.217344
epoch:89, batch3, load frames use: 3.75284194946s
step 4096/400000: train loss: 0.216036
epoch:89, batch4, load frames use: 2.52062487602s
step 4097/400000: train loss: 0.201805
epoch:89, batch5, load frames use: 3.48932099342s
step 4098/400000: train loss: 0.211634
epoch:89, batch6, load frames use: 3.03224778175s
step 4099/400000: train loss: 0.209409
epoch:89, batch7, load frames use: 3.01572084427s
step 4100/400000: train loss: 0.207924
step 4100/400000: validation loss: 0.196196
epoch:89, batch8, load frames use: 7.52898097038s
step 4101/400000: train loss: 0.191302
epoch:89, batch9, load frames use: 3.77496600151s
step 4102/400000: train loss: 0.204769
epoch:89, batch10, load frames use: 3.90101790428s
step 4103/400000: train loss: 0.217383
epoch:89, batch11, load frames use: 5.50002980232s
step 4104/400000: train loss: 0.213553
epoch:89, batch12, load frames use: 5.25235295296s
step 4105/400000: train loss: 0.194508
epoch:89, batch13, load frames use: 4.00206279755s
step 4106/400000: train loss: 0.190938
epoch:89, batch14, load frames use: 6.73394584656s
step 4107/400000: train loss: 0.205226
epoch:89, batch15, load frames use: 3.03395915031s
step 4108/400000: train loss: 0.232805
epoch:89, batch16, load frames use: 4.20893192291s
step 4109/400000: train loss: 0.209382
epoch:89, batch17, load frames use: 4.38552904129s
step 4110/400000: train loss: 0.182956
epoch:89, batch18, load frames use: 3.32953906059s
step 4111/400000: train loss: 0.203659
epoch:89, batch19, load frames use: 3.6625521183s
step 4112/400000: train loss: 0.223483
epoch:89, batch20, load frames use: 5.68928003311s
step 4113/400000: train loss: 0.212911
epoch:89, batch21, load frames use: 3.00294303894s
step 4114/400000: train loss: 0.231640
epoch:89, batch22, load frames use: 5.44085502625s
step 4115/400000: train loss: 0.219415
epoch:89, batch23, load frames use: 5.1659040451s
step 4116/400000: train loss: 0.229587
epoch:89, batch24, load frames use: 2.9629149437s
step 4117/400000: train loss: 0.215544
epoch:89, batch25, load frames use: 4.15298104286s
step 4118/400000: train loss: 0.226731
epoch:89, batch26, load frames use: 4.87532091141s
step 4119/400000: train loss: 0.246042
epoch:89, batch27, load frames use: 4.23258209229s
step 4120/400000: train loss: 0.235512
epoch:89, batch28, load frames use: 3.48035383224s
step 4121/400000: train loss: 0.224492
epoch:89, batch29, load frames use: 2.85165381432s
step 4122/400000: train loss: 0.221385
epoch:89, batch30, load frames use: 3.98926115036s
step 4123/400000: train loss: 0.195000
epoch:89, batch31, load frames use: 5.49778294563s
step 4124/400000: train loss: 0.217307
epoch:89, batch32, load frames use: 3.63211798668s
step 4125/400000: train loss: 0.236583
epoch:89, batch33, load frames use: 4.69432115555s
step 4126/400000: train loss: 0.216233
epoch:89, batch34, load frames use: 3.33746814728s
step 4127/400000: train loss: 0.200516
epoch:89, batch35, load frames use: 4.19628810883s
step 4128/400000: train loss: 0.237974
epoch:89, batch36, load frames use: 3.00696110725s
step 4129/400000: train loss: 0.239344
epoch:89, batch37, load frames use: 3.86768293381s
step 4130/400000: train loss: 0.217295
epoch:89, batch38, load frames use: 4.18439102173s
step 4131/400000: train loss: 0.211250
epoch:89, batch39, load frames use: 5.6352481842s
step 4132/400000: train loss: 0.230970
epoch:89, batch40, load frames use: 6.84721398354s
step 4133/400000: train loss: 0.224016
epoch:89, batch41, load frames use: 4.04448699951s
step 4134/400000: train loss: 0.235654
epoch:89, batch42, load frames use: 4.25247693062s
step 4135/400000: train loss: 0.238042
epoch:89, batch43, load frames use: 5.09977483749s
step 4136/400000: train loss: 0.240144
epoch:89, batch44, load frames use: 5.16847109795s
step 4137/400000: train loss: 0.205798
epoch:89, batch45, load frames use: 3.80436992645s
step 4138/400000: train loss: 0.225653
epoch:89, batch46, load frames use: 5.30678987503s
step 4139/400000: train loss: 0.224436
epoch:90, batch1, load frames use: 4.31733107567s
step 4140/400000: train loss: 0.213880
epoch:90, batch2, load frames use: 4.53573298454s
step 4141/400000: train loss: 0.215290
epoch:90, batch3, load frames use: 6.75022101402s
step 4142/400000: train loss: 0.228069
epoch:90, batch4, load frames use: 2.62949085236s
step 4143/400000: train loss: 0.224519
epoch:90, batch5, load frames use: 3.4258658886s
step 4144/400000: train loss: 0.225563
epoch:90, batch6, load frames use: 2.92187285423s
step 4145/400000: train loss: 0.201207
epoch:90, batch7, load frames use: 5.4699921608s
step 4146/400000: train loss: 0.248289
epoch:90, batch8, load frames use: 5.86288690567s
step 4147/400000: train loss: 0.261330
epoch:90, batch9, load frames use: 3.66531682014s
step 4148/400000: train loss: 0.207973
epoch:90, batch10, load frames use: 3.84358096123s
step 4149/400000: train loss: 0.203553
epoch:90, batch11, load frames use: 3.67196798325s
step 4150/400000: train loss: 0.229938
epoch:90, batch12, load frames use: 3.01520490646s
step 4151/400000: train loss: 0.198093
epoch:90, batch13, load frames use: 3.29316210747s
step 4152/400000: train loss: 0.223451
epoch:90, batch14, load frames use: 4.47885203362s
step 4153/400000: train loss: 0.218035
epoch:90, batch15, load frames use: 3.13548994064s
step 4154/400000: train loss: 0.206692
epoch:90, batch16, load frames use: 3.8823120594s
step 4155/400000: train loss: 0.219624
epoch:90, batch17, load frames use: 2.93341803551s
step 4156/400000: train loss: 0.231998
epoch:90, batch18, load frames use: 4.92812299728s
step 4157/400000: train loss: 0.199964
epoch:90, batch19, load frames use: 2.98796606064s
step 4158/400000: train loss: 0.230326
epoch:90, batch20, load frames use: 3.73237609863s
step 4159/400000: train loss: 0.210488
epoch:90, batch21, load frames use: 4.74810886383s
step 4160/400000: train loss: 0.217489
epoch:90, batch22, load frames use: 3.95114398003s
step 4161/400000: train loss: 0.204591
epoch:90, batch23, load frames use: 3.39613699913s
step 4162/400000: train loss: 0.209838
epoch:90, batch24, load frames use: 4.64569497108s
step 4163/400000: train loss: 0.211604
epoch:90, batch25, load frames use: 3.41277718544s
step 4164/400000: train loss: 0.232507
epoch:90, batch26, load frames use: 3.24600410461s
step 4165/400000: train loss: 0.210045
epoch:90, batch27, load frames use: 3.54992699623s
step 4166/400000: train loss: 0.200934
epoch:90, batch28, load frames use: 4.05104994774s
step 4167/400000: train loss: 0.232089
epoch:90, batch29, load frames use: 4.03810191154s
step 4168/400000: train loss: 0.209151
epoch:90, batch30, load frames use: 3.82741308212s
step 4169/400000: train loss: 0.211195
epoch:90, batch31, load frames use: 3.51381611824s
step 4170/400000: train loss: 0.222467
epoch:90, batch32, load frames use: 3.77464199066s
step 4171/400000: train loss: 0.228972
epoch:90, batch33, load frames use: 5.42899990082s
step 4172/400000: train loss: 0.219923
epoch:90, batch34, load frames use: 3.53547620773s
step 4173/400000: train loss: 0.212648
epoch:90, batch35, load frames use: 3.28197002411s
step 4174/400000: train loss: 0.188356
epoch:90, batch36, load frames use: 2.89837884903s
step 4175/400000: train loss: 0.195221
epoch:90, batch37, load frames use: 3.31893587112s
step 4176/400000: train loss: 0.213822
epoch:90, batch38, load frames use: 4.61394000053s
step 4177/400000: train loss: 0.236094
epoch:90, batch39, load frames use: 3.74364900589s
step 4178/400000: train loss: 0.208838
epoch:90, batch40, load frames use: 5.62008905411s
step 4179/400000: train loss: 0.222694
epoch:90, batch41, load frames use: 5.64547681808s
step 4180/400000: train loss: 0.205786
epoch:90, batch42, load frames use: 4.84013485909s
step 4181/400000: train loss: 0.211915
epoch:90, batch43, load frames use: 4.64745211601s
step 4182/400000: train loss: 0.251343
epoch:90, batch44, load frames use: 5.70651102066s
step 4183/400000: train loss: 0.210762
epoch:90, batch45, load frames use: 3.50413513184s
step 4184/400000: train loss: 0.225853
epoch:90, batch46, load frames use: 7.52912998199s
step 4185/400000: train loss: 0.236894
epoch:91, batch1, load frames use: 4.49478197098s
step 4186/400000: train loss: 0.200394
epoch:91, batch2, load frames use: 3.31442117691s
step 4187/400000: train loss: 0.176687
epoch:91, batch3, load frames use: 4.74648714066s
step 4188/400000: train loss: 0.210288
epoch:91, batch4, load frames use: 5.40936899185s
step 4189/400000: train loss: 0.239093
epoch:91, batch5, load frames use: 5.22741103172s
step 4190/400000: train loss: 0.209707
epoch:91, batch6, load frames use: 4.55794596672s
step 4191/400000: train loss: 0.223589
epoch:91, batch7, load frames use: 7.97756099701s
step 4192/400000: train loss: 0.204669
epoch:91, batch8, load frames use: 3.26682305336s
step 4193/400000: train loss: 0.218676
epoch:91, batch9, load frames use: 4.66791391373s
step 4194/400000: train loss: 0.205746
epoch:91, batch10, load frames use: 6.16682004929s
step 4195/400000: train loss: 0.229272
epoch:91, batch11, load frames use: 6.94719696045s
step 4196/400000: train loss: 0.208570
epoch:91, batch12, load frames use: 3.94435501099s
step 4197/400000: train loss: 0.242281
epoch:91, batch13, load frames use: 3.41780996323s
step 4198/400000: train loss: 0.215152
epoch:91, batch14, load frames use: 3.23776292801s
step 4199/400000: train loss: 0.223710
epoch:91, batch15, load frames use: 6.4366209507s
step 4200/400000: train loss: 0.197503
step 4200/400000: validation loss: 0.178544
epoch:91, batch16, load frames use: 3.19131803513s
step 4201/400000: train loss: 0.230268
epoch:91, batch17, load frames use: 4.4302611351s
step 4202/400000: train loss: 0.186581
epoch:91, batch18, load frames use: 3.23545908928s
step 4203/400000: train loss: 0.202169
epoch:91, batch19, load frames use: 3.51690101624s
step 4204/400000: train loss: 0.223189
epoch:91, batch20, load frames use: 2.72745299339s
step 4205/400000: train loss: 0.257832
epoch:91, batch21, load frames use: 4.27765202522s
step 4206/400000: train loss: 0.189405
epoch:91, batch22, load frames use: 2.90252113342s
step 4207/400000: train loss: 0.206950
epoch:91, batch23, load frames use: 3.41494107246s
step 4208/400000: train loss: 0.196002
epoch:91, batch24, load frames use: 2.95197582245s
step 4209/400000: train loss: 0.208460
epoch:91, batch25, load frames use: 3.7763428688s
step 4210/400000: train loss: 0.220755
epoch:91, batch26, load frames use: 3.08256006241s
step 4211/400000: train loss: 0.212911
epoch:91, batch27, load frames use: 4.54731607437s
step 4212/400000: train loss: 0.218979
epoch:91, batch28, load frames use: 3.71652603149s
step 4213/400000: train loss: 0.213911
epoch:91, batch29, load frames use: 3.6312148571s
step 4214/400000: train loss: 0.212391
epoch:91, batch30, load frames use: 2.83091783524s
step 4215/400000: train loss: 0.219990
epoch:91, batch31, load frames use: 3.98531484604s
step 4216/400000: train loss: 0.203741
epoch:91, batch32, load frames use: 3.5994579792s
step 4217/400000: train loss: 0.230699
epoch:91, batch33, load frames use: 3.61136698723s
step 4218/400000: train loss: 0.202519
epoch:91, batch34, load frames use: 2.72968196869s
step 4219/400000: train loss: 0.207258
epoch:91, batch35, load frames use: 4.64879393578s
step 4220/400000: train loss: 0.240314
epoch:91, batch36, load frames use: 3.57954788208s
step 4221/400000: train loss: 0.240457
epoch:91, batch37, load frames use: 3.19912409782s
step 4222/400000: train loss: 0.243161
epoch:91, batch38, load frames use: 2.9543530941s
step 4223/400000: train loss: 0.197573
epoch:91, batch39, load frames use: 5.51890707016s
step 4224/400000: train loss: 0.191873
epoch:91, batch40, load frames use: 3.74800801277s
step 4225/400000: train loss: 0.227556
epoch:91, batch41, load frames use: 3.14752411842s
step 4226/400000: train loss: 0.222748
epoch:91, batch42, load frames use: 3.1161570549s
step 4227/400000: train loss: 0.211626
epoch:91, batch43, load frames use: 3.64753198624s
step 4228/400000: train loss: 0.231783
epoch:91, batch44, load frames use: 3.10726499557s
step 4229/400000: train loss: 0.243006
epoch:91, batch45, load frames use: 3.69325995445s
step 4230/400000: train loss: 0.217620
epoch:91, batch46, load frames use: 3.26906895638s
step 4231/400000: train loss: 0.231218
epoch:92, batch1, load frames use: 3.70841097832s
step 4232/400000: train loss: 0.243012
epoch:92, batch2, load frames use: 3.62828993797s
step 4233/400000: train loss: 0.258475
epoch:92, batch3, load frames use: 3.39707398415s
step 4234/400000: train loss: 0.239350
epoch:92, batch4, load frames use: 3.52985191345s
step 4235/400000: train loss: 0.229531
epoch:92, batch5, load frames use: 3.39639186859s
step 4236/400000: train loss: 0.205235
epoch:92, batch6, load frames use: 2.95511198044s
step 4237/400000: train loss: 0.230295
epoch:92, batch7, load frames use: 3.69315004349s
step 4238/400000: train loss: 0.211076
epoch:92, batch8, load frames use: 3.10402917862s
step 4239/400000: train loss: 0.199380
epoch:92, batch9, load frames use: 4.19140505791s
step 4240/400000: train loss: 0.239911
epoch:92, batch10, load frames use: 3.42560100555s
step 4241/400000: train loss: 0.209469
epoch:92, batch11, load frames use: 3.40289998055s
step 4242/400000: train loss: 0.200611
epoch:92, batch12, load frames use: 3.25665521622s
step 4243/400000: train loss: 0.243923
epoch:92, batch13, load frames use: 4.72488212585s
step 4244/400000: train loss: 0.219986
epoch:92, batch14, load frames use: 4.26565694809s
step 4245/400000: train loss: 0.239028
epoch:92, batch15, load frames use: 3.69691419601s
step 4246/400000: train loss: 0.188124
epoch:92, batch16, load frames use: 3.50500583649s
step 4247/400000: train loss: 0.216290
epoch:92, batch17, load frames use: 3.77479100227s
step 4248/400000: train loss: 0.220778
epoch:92, batch18, load frames use: 3.42521286011s
step 4249/400000: train loss: 0.210456
epoch:92, batch19, load frames use: 3.15836501122s
step 4250/400000: train loss: 0.203253
epoch:92, batch20, load frames use: 3.30857491493s
step 4251/400000: train loss: 0.217484
epoch:92, batch21, load frames use: 4.07221007347s
step 4252/400000: train loss: 0.209456
epoch:92, batch22, load frames use: 4.48942899704s
step 4253/400000: train loss: 0.216567
epoch:92, batch23, load frames use: 3.54931092262s
step 4254/400000: train loss: 0.225410
epoch:92, batch24, load frames use: 3.99484300613s
step 4255/400000: train loss: 0.218397
epoch:92, batch25, load frames use: 3.43745303154s
step 4256/400000: train loss: 0.226421
epoch:92, batch26, load frames use: 3.39785408974s
step 4257/400000: train loss: 0.211455
epoch:92, batch27, load frames use: 3.45308089256s
step 4258/400000: train loss: 0.198700
epoch:92, batch28, load frames use: 3.41569113731s
step 4259/400000: train loss: 0.215657
epoch:92, batch29, load frames use: 3.93104791641s
step 4260/400000: train loss: 0.199789
epoch:92, batch30, load frames use: 3.50720310211s
step 4261/400000: train loss: 0.211877
epoch:92, batch31, load frames use: 3.28822398186s
step 4262/400000: train loss: 0.213793
epoch:92, batch32, load frames use: 3.61181807518s
step 4263/400000: train loss: 0.228302
epoch:92, batch33, load frames use: 3.56751394272s
step 4264/400000: train loss: 0.196748
epoch:92, batch34, load frames use: 3.08278512955s
step 4265/400000: train loss: 0.203824
epoch:92, batch35, load frames use: 3.81042599678s
step 4266/400000: train loss: 0.220405
epoch:92, batch36, load frames use: 3.39947295189s
step 4267/400000: train loss: 0.209567
epoch:92, batch37, load frames use: 3.59225606918s
step 4268/400000: train loss: 0.222353
epoch:92, batch38, load frames use: 4.08330488205s
step 4269/400000: train loss: 0.196237
epoch:92, batch39, load frames use: 3.60307407379s
step 4270/400000: train loss: 0.233558
epoch:92, batch40, load frames use: 3.55333995819s
step 4271/400000: train loss: 0.204279
epoch:92, batch41, load frames use: 3.54800796509s
step 4272/400000: train loss: 0.236884
epoch:92, batch42, load frames use: 3.62508606911s
step 4273/400000: train loss: 0.202794
epoch:92, batch43, load frames use: 3.58275699615s
step 4274/400000: train loss: 0.221722
epoch:92, batch44, load frames use: 3.00993299484s
step 4275/400000: train loss: 0.203612
epoch:92, batch45, load frames use: 3.78583598137s
step 4276/400000: train loss: 0.227945
epoch:92, batch46, load frames use: 3.45729207993s
step 4277/400000: train loss: 0.197346
epoch:93, batch1, load frames use: 3.80294489861s
step 4278/400000: train loss: 0.212042
epoch:93, batch2, load frames use: 3.35980701447s
step 4279/400000: train loss: 0.200050
epoch:93, batch3, load frames use: 3.42844390869s
step 4280/400000: train loss: 0.211978
epoch:93, batch4, load frames use: 3.23028993607s
step 4281/400000: train loss: 0.233580
epoch:93, batch5, load frames use: 3.63240218163s
step 4282/400000: train loss: 0.212038
epoch:93, batch6, load frames use: 3.61378884315s
step 4283/400000: train loss: 0.222430
epoch:93, batch7, load frames use: 3.44665503502s
step 4284/400000: train loss: 0.234083
epoch:93, batch8, load frames use: 3.47198987007s
step 4285/400000: train loss: 0.199030
epoch:93, batch9, load frames use: 3.40938305855s
step 4286/400000: train loss: 0.218278
epoch:93, batch10, load frames use: 3.42537283897s
step 4287/400000: train loss: 0.204526
epoch:93, batch11, load frames use: 4.03405189514s
step 4288/400000: train loss: 0.189149
epoch:93, batch12, load frames use: 3.99177694321s
step 4289/400000: train loss: 0.222088
epoch:93, batch13, load frames use: 3.0921421051s
step 4290/400000: train loss: 0.204216
epoch:93, batch14, load frames use: 3.43261313438s
step 4291/400000: train loss: 0.225550
epoch:93, batch15, load frames use: 3.61553406715s
step 4292/400000: train loss: 0.211225
epoch:93, batch16, load frames use: 4.88769888878s
step 4293/400000: train loss: 0.224330
epoch:93, batch17, load frames use: 5.02324104309s
step 4294/400000: train loss: 0.212928
epoch:93, batch18, load frames use: 7.20599389076s
step 4295/400000: train loss: 0.215457
epoch:93, batch19, load frames use: 4.18892383575s
step 4296/400000: train loss: 0.180727
epoch:93, batch20, load frames use: 3.38823914528s
step 4297/400000: train loss: 0.214153
epoch:93, batch21, load frames use: 3.80417895317s
step 4298/400000: train loss: 0.206765
epoch:93, batch22, load frames use: 4.01217699051s
step 4299/400000: train loss: 0.192702
epoch:93, batch23, load frames use: 4.06243610382s
step 4300/400000: train loss: 0.196107
step 4300/400000: validation loss: 0.190659
epoch:93, batch24, load frames use: 3.49881100655s
step 4301/400000: train loss: 0.204436
epoch:93, batch25, load frames use: 3.44810509682s
step 4302/400000: train loss: 0.189359
epoch:93, batch26, load frames use: 4.18249511719s
step 4303/400000: train loss: 0.196882
epoch:93, batch27, load frames use: 3.06280183792s
step 4304/400000: train loss: 0.199696
epoch:93, batch28, load frames use: 3.13865804672s
step 4305/400000: train loss: 0.192823
epoch:93, batch29, load frames use: 2.91288304329s
step 4306/400000: train loss: 0.212175
epoch:93, batch30, load frames use: 4.85649991035s
step 4307/400000: train loss: 0.241840
epoch:93, batch31, load frames use: 3.58439493179s
step 4308/400000: train loss: 0.196706
epoch:93, batch32, load frames use: 3.24027514458s
step 4309/400000: train loss: 0.201500
epoch:93, batch33, load frames use: 3.36117601395s
step 4310/400000: train loss: 0.219415
epoch:93, batch34, load frames use: 3.72544789314s
step 4311/400000: train loss: 0.215395
epoch:93, batch35, load frames use: 3.36279988289s
step 4312/400000: train loss: 0.202617
epoch:93, batch36, load frames use: 3.34356093407s
step 4313/400000: train loss: 0.230269
epoch:93, batch37, load frames use: 3.43651199341s
step 4314/400000: train loss: 0.237227
epoch:93, batch38, load frames use: 3.55650305748s
step 4315/400000: train loss: 0.199612
epoch:93, batch39, load frames use: 3.06315088272s
step 4316/400000: train loss: 0.229353
epoch:93, batch40, load frames use: 4.28925299644s
step 4317/400000: train loss: 0.197989
epoch:93, batch41, load frames use: 3.43419694901s
step 4318/400000: train loss: 0.219298
epoch:93, batch42, load frames use: 3.15284109116s
step 4319/400000: train loss: 0.237413
epoch:93, batch43, load frames use: 3.08187794685s
step 4320/400000: train loss: 0.253932
epoch:93, batch44, load frames use: 4.22331500053s
step 4321/400000: train loss: 0.204276
epoch:93, batch45, load frames use: 3.57812905312s
step 4322/400000: train loss: 0.214788
epoch:93, batch46, load frames use: 3.3594789505s
step 4323/400000: train loss: 0.204887
epoch:94, batch1, load frames use: 3.12725186348s
step 4324/400000: train loss: 0.183743
epoch:94, batch2, load frames use: 3.695333004s
step 4325/400000: train loss: 0.243349
epoch:94, batch3, load frames use: 3.23268389702s
step 4326/400000: train loss: 0.200330
epoch:94, batch4, load frames use: 3.35456204414s
step 4327/400000: train loss: 0.211448
epoch:94, batch5, load frames use: 2.99503111839s
step 4328/400000: train loss: 0.207467
epoch:94, batch6, load frames use: 3.4211909771s
step 4329/400000: train loss: 0.189040
epoch:94, batch7, load frames use: 3.21432185173s
step 4330/400000: train loss: 0.189914
epoch:94, batch8, load frames use: 4.01703095436s
step 4331/400000: train loss: 0.182676
epoch:94, batch9, load frames use: 3.29962205887s
step 4332/400000: train loss: 0.187339
epoch:94, batch10, load frames use: 3.28282690048s
step 4333/400000: train loss: 0.201713
epoch:94, batch11, load frames use: 3.23010182381s
step 4334/400000: train loss: 0.201416
epoch:94, batch12, load frames use: 3.17018485069s
step 4335/400000: train loss: 0.221423
epoch:94, batch13, load frames use: 3.17719888687s
step 4336/400000: train loss: 0.185754
epoch:94, batch14, load frames use: 3.76557803154s
step 4337/400000: train loss: 0.214848
epoch:94, batch15, load frames use: 3.23760008812s
step 4338/400000: train loss: 0.216765
epoch:94, batch16, load frames use: 3.06910514832s
step 4339/400000: train loss: 0.207752
epoch:94, batch17, load frames use: 3.77861595154s
step 4340/400000: train loss: 0.210619
epoch:94, batch18, load frames use: 3.04805016518s
step 4341/400000: train loss: 0.242143
epoch:94, batch19, load frames use: 3.38413691521s
step 4342/400000: train loss: 0.228765
epoch:94, batch20, load frames use: 3.24740195274s
step 4343/400000: train loss: 0.199117
epoch:94, batch21, load frames use: 3.55852198601s
step 4344/400000: train loss: 0.231622
epoch:94, batch22, load frames use: 2.9608039856s
step 4345/400000: train loss: 0.231881
epoch:94, batch23, load frames use: 6.43438482285s
step 4346/400000: train loss: 0.227772
epoch:94, batch24, load frames use: 4.99750804901s
step 4347/400000: train loss: 0.214531
epoch:94, batch25, load frames use: 4.60896396637s
step 4348/400000: train loss: 0.205322
epoch:94, batch26, load frames use: 6.95530891418s
step 4349/400000: train loss: 0.232643
epoch:94, batch27, load frames use: 4.2844479084s
step 4350/400000: train loss: 0.205555
epoch:94, batch28, load frames use: 3.68809485435s
step 4351/400000: train loss: 0.215622
epoch:94, batch29, load frames use: 6.82149195671s
step 4352/400000: train loss: 0.220099
epoch:94, batch30, load frames use: 6.13788509369s
step 4353/400000: train loss: 0.198923
epoch:94, batch31, load frames use: 5.5168068409s
step 4354/400000: train loss: 0.211742
epoch:94, batch32, load frames use: 6.25928711891s
step 4355/400000: train loss: 0.218895
epoch:94, batch33, load frames use: 3.90887784958s
step 4356/400000: train loss: 0.209532
epoch:94, batch34, load frames use: 3.84644007683s
step 4357/400000: train loss: 0.210337
epoch:94, batch35, load frames use: 4.74756193161s
step 4358/400000: train loss: 0.203420
epoch:94, batch36, load frames use: 3.60202383995s
step 4359/400000: train loss: 0.204624
epoch:94, batch37, load frames use: 3.81332087517s
step 4360/400000: train loss: 0.191924
epoch:94, batch38, load frames use: 3.56048893929s
step 4361/400000: train loss: 0.227363
epoch:94, batch39, load frames use: 3.62775206566s
step 4362/400000: train loss: 0.215810
epoch:94, batch40, load frames use: 3.12616682053s
step 4363/400000: train loss: 0.199574
epoch:94, batch41, load frames use: 3.69484996796s
step 4364/400000: train loss: 0.229631
epoch:94, batch42, load frames use: 3.05333399773s
step 4365/400000: train loss: 0.229966
epoch:94, batch43, load frames use: 3.70423603058s
step 4366/400000: train loss: 0.218626
epoch:94, batch44, load frames use: 3.46472215652s
step 4367/400000: train loss: 0.222424
epoch:94, batch45, load frames use: 3.66337299347s
step 4368/400000: train loss: 0.232157
epoch:94, batch46, load frames use: 3.22787714005s
step 4369/400000: train loss: 0.206626
epoch:95, batch1, load frames use: 3.63812184334s
step 4370/400000: train loss: 0.208019
epoch:95, batch2, load frames use: 3.0876967907s
step 4371/400000: train loss: 0.193674
epoch:95, batch3, load frames use: 3.29650712013s
step 4372/400000: train loss: 0.189424
epoch:95, batch4, load frames use: 3.07652688026s
step 4373/400000: train loss: 0.213331
epoch:95, batch5, load frames use: 3.75414395332s
step 4374/400000: train loss: 0.186062
epoch:95, batch6, load frames use: 3.67086386681s
step 4375/400000: train loss: 0.230287
epoch:95, batch7, load frames use: 3.48386001587s
step 4376/400000: train loss: 0.193655
epoch:95, batch8, load frames use: 4.36444091797s
step 4377/400000: train loss: 0.200299
epoch:95, batch9, load frames use: 4.22654891014s
step 4378/400000: train loss: 0.207807
epoch:95, batch10, load frames use: 3.71246194839s
step 4379/400000: train loss: 0.198314
epoch:95, batch11, load frames use: 2.92987298965s
step 4380/400000: train loss: 0.212293
epoch:95, batch12, load frames use: 4.91575884819s
step 4381/400000: train loss: 0.207533
epoch:95, batch13, load frames use: 3.78941297531s
step 4382/400000: train loss: 0.221343
epoch:95, batch14, load frames use: 3.48211812973s
step 4383/400000: train loss: 0.233617
epoch:95, batch15, load frames use: 3.1006500721s
step 4384/400000: train loss: 0.210456
epoch:95, batch16, load frames use: 4.1614921093s
step 4385/400000: train loss: 0.214342
epoch:95, batch17, load frames use: 3.20915985107s
step 4386/400000: train loss: 0.242537
epoch:95, batch18, load frames use: 3.42891907692s
step 4387/400000: train loss: 0.210173
epoch:95, batch19, load frames use: 3.5926990509s
step 4388/400000: train loss: 0.193381
epoch:95, batch20, load frames use: 3.53423810005s
step 4389/400000: train loss: 0.210475
epoch:95, batch21, load frames use: 3.32363200188s
step 4390/400000: train loss: 0.192111
epoch:95, batch22, load frames use: 3.41789197922s
step 4391/400000: train loss: 0.217134
epoch:95, batch23, load frames use: 3.32982802391s
step 4392/400000: train loss: 0.219035
epoch:95, batch24, load frames use: 3.17927098274s
step 4393/400000: train loss: 0.198600
epoch:95, batch25, load frames use: 4.07331085205s
step 4394/400000: train loss: 0.218376
epoch:95, batch26, load frames use: 3.51122307777s
step 4395/400000: train loss: 0.188277
epoch:95, batch27, load frames use: 3.5068590641s
step 4396/400000: train loss: 0.190633
epoch:95, batch28, load frames use: 3.38094115257s
step 4397/400000: train loss: 0.219824
epoch:95, batch29, load frames use: 3.99053502083s
step 4398/400000: train loss: 0.228994
epoch:95, batch30, load frames use: 3.0817129612s
step 4399/400000: train loss: 0.226630
epoch:95, batch31, load frames use: 4.15618610382s
step 4400/400000: train loss: 0.203838
step 4400/400000: validation loss: 0.201684
epoch:95, batch32, load frames use: 3.19344305992s
step 4401/400000: train loss: 0.220615
epoch:95, batch33, load frames use: 3.69383811951s
step 4402/400000: train loss: 0.216186
epoch:95, batch34, load frames use: 3.56677889824s
step 4403/400000: train loss: 0.217794
epoch:95, batch35, load frames use: 3.70891094208s
step 4404/400000: train loss: 0.219997
epoch:95, batch36, load frames use: 3.16291809082s
step 4405/400000: train loss: 0.222371
epoch:95, batch37, load frames use: 2.99278688431s
step 4406/400000: train loss: 0.209097
epoch:95, batch38, load frames use: 3.24263596535s
step 4407/400000: train loss: 0.194000
epoch:95, batch39, load frames use: 3.34784603119s
step 4408/400000: train loss: 0.214524
epoch:95, batch40, load frames use: 3.42497301102s
step 4409/400000: train loss: 0.207578
epoch:95, batch41, load frames use: 3.48921394348s
step 4410/400000: train loss: 0.201581
epoch:95, batch42, load frames use: 4.22607803345s
step 4411/400000: train loss: 0.215548
epoch:95, batch43, load frames use: 3.21669483185s
step 4412/400000: train loss: 0.196149
epoch:95, batch44, load frames use: 2.85567188263s
step 4413/400000: train loss: 0.228952
epoch:95, batch45, load frames use: 3.74853205681s
step 4414/400000: train loss: 0.220587
epoch:95, batch46, load frames use: 3.25616598129s
step 4415/400000: train loss: 0.218772
epoch:96, batch1, load frames use: 3.34725403786s
step 4416/400000: train loss: 0.206892
epoch:96, batch2, load frames use: 3.16287517548s
step 4417/400000: train loss: 0.200137
epoch:96, batch3, load frames use: 2.82956981659s
step 4418/400000: train loss: 0.189696
epoch:96, batch4, load frames use: 3.4683380127s
step 4419/400000: train loss: 0.197331
epoch:96, batch5, load frames use: 3.19390416145s
step 4420/400000: train loss: 0.201300
epoch:96, batch6, load frames use: 3.26305198669s
step 4421/400000: train loss: 0.203345
epoch:96, batch7, load frames use: 2.94884991646s
step 4422/400000: train loss: 0.217778
epoch:96, batch8, load frames use: 3.01884102821s
step 4423/400000: train loss: 0.195446
epoch:96, batch9, load frames use: 4.00416111946s
step 4424/400000: train loss: 0.203593
epoch:96, batch10, load frames use: 2.82803893089s
step 4425/400000: train loss: 0.200017
epoch:96, batch11, load frames use: 3.34051394463s
step 4426/400000: train loss: 0.182870
epoch:96, batch12, load frames use: 2.99855089188s
step 4427/400000: train loss: 0.209429
epoch:96, batch13, load frames use: 3.14250206947s
step 4428/400000: train loss: 0.224950
epoch:96, batch14, load frames use: 3.05896401405s
step 4429/400000: train loss: 0.190427
epoch:96, batch15, load frames use: 4.27735686302s
step 4430/400000: train loss: 0.216172
epoch:96, batch16, load frames use: 3.01360797882s
step 4431/400000: train loss: 0.219470
epoch:96, batch17, load frames use: 3.33233118057s
step 4432/400000: train loss: 0.202994
epoch:96, batch18, load frames use: 3.45120215416s
step 4433/400000: train loss: 0.208712
epoch:96, batch19, load frames use: 3.86655282974s
step 4434/400000: train loss: 0.221455
epoch:96, batch20, load frames use: 3.16075706482s
step 4435/400000: train loss: 0.189126
epoch:96, batch21, load frames use: 3.29931211472s
step 4436/400000: train loss: 0.207004
epoch:96, batch22, load frames use: 3.57399702072s
step 4437/400000: train loss: 0.210943
epoch:96, batch23, load frames use: 2.99771809578s
step 4438/400000: train loss: 0.236136
epoch:96, batch24, load frames use: 3.23228597641s
step 4439/400000: train loss: 0.230264
epoch:96, batch25, load frames use: 3.29944896698s
step 4440/400000: train loss: 0.220867
epoch:96, batch26, load frames use: 3.91975593567s
step 4441/400000: train loss: 0.206597
epoch:96, batch27, load frames use: 3.23592710495s
step 4442/400000: train loss: 0.178323
epoch:96, batch28, load frames use: 3.55510282516s
step 4443/400000: train loss: 0.207365
epoch:96, batch29, load frames use: 3.36019802094s
step 4444/400000: train loss: 0.229259
epoch:96, batch30, load frames use: 2.99479818344s
step 4445/400000: train loss: 0.209742
epoch:96, batch31, load frames use: 3.32540178299s
step 4446/400000: train loss: 0.227741
epoch:96, batch32, load frames use: 3.13063001633s
step 4447/400000: train loss: 0.203981
epoch:96, batch33, load frames use: 3.2289121151s
step 4448/400000: train loss: 0.199922
epoch:96, batch34, load frames use: 3.61311006546s
step 4449/400000: train loss: 0.202276
epoch:96, batch35, load frames use: 2.84326696396s
step 4450/400000: train loss: 0.198953
epoch:96, batch36, load frames use: 3.47531199455s
step 4451/400000: train loss: 0.191846
epoch:96, batch37, load frames use: 2.990213871s
step 4452/400000: train loss: 0.226369
epoch:96, batch38, load frames use: 3.66592097282s
step 4453/400000: train loss: 0.226663
epoch:96, batch39, load frames use: 3.17476987839s
step 4454/400000: train loss: 0.222741
epoch:96, batch40, load frames use: 3.88476896286s
step 4455/400000: train loss: 0.205287
epoch:96, batch41, load frames use: 2.95568609238s
step 4456/400000: train loss: 0.249645
epoch:96, batch42, load frames use: 3.6498169899s
step 4457/400000: train loss: 0.211029
epoch:96, batch43, load frames use: 2.95035004616s
step 4458/400000: train loss: 0.213602
epoch:96, batch44, load frames use: 3.46740412712s
step 4459/400000: train loss: 0.223137
epoch:96, batch45, load frames use: 3.41411304474s
step 4460/400000: train loss: 0.190633
epoch:96, batch46, load frames use: 3.43064498901s
step 4461/400000: train loss: 0.203289
epoch:97, batch1, load frames use: 2.78642201424s
step 4462/400000: train loss: 0.200926
epoch:97, batch2, load frames use: 3.47249484062s
step 4463/400000: train loss: 0.218954
epoch:97, batch3, load frames use: 3.23373794556s
step 4464/400000: train loss: 0.183135
epoch:97, batch4, load frames use: 2.99979400635s
step 4465/400000: train loss: 0.196360
epoch:97, batch5, load frames use: 3.18604302406s
step 4466/400000: train loss: 0.210008
epoch:97, batch6, load frames use: 3.70166921616s
step 4467/400000: train loss: 0.211309
epoch:97, batch7, load frames use: 2.6327381134s
step 4468/400000: train loss: 0.200007
epoch:97, batch8, load frames use: 3.23733305931s
step 4469/400000: train loss: 0.199408
epoch:97, batch9, load frames use: 3.85088396072s
step 4470/400000: train loss: 0.206227
epoch:97, batch10, load frames use: 7.4430270195s
step 4471/400000: train loss: 0.214023
epoch:97, batch11, load frames use: 3.41517806053s
step 4472/400000: train loss: 0.217542
epoch:97, batch12, load frames use: 4.53256797791s
step 4473/400000: train loss: 0.215580
epoch:97, batch13, load frames use: 3.65688204765s
step 4474/400000: train loss: 0.194377
epoch:97, batch14, load frames use: 3.20164012909s
step 4475/400000: train loss: 0.216594
epoch:97, batch15, load frames use: 3.18738698959s
step 4476/400000: train loss: 0.231210
epoch:97, batch16, load frames use: 4.40628099442s
step 4477/400000: train loss: 0.179000
epoch:97, batch17, load frames use: 3.22242712975s
step 4478/400000: train loss: 0.213185
epoch:97, batch18, load frames use: 4.3532640934s
step 4479/400000: train loss: 0.207388
epoch:97, batch19, load frames use: 4.0911989212s
step 4480/400000: train loss: 0.232458
epoch:97, batch20, load frames use: 3.03397488594s
step 4481/400000: train loss: 0.219436
epoch:97, batch21, load frames use: 3.77714705467s
step 4482/400000: train loss: 0.230928
epoch:97, batch22, load frames use: 3.89154791832s
step 4483/400000: train loss: 0.221022
epoch:97, batch23, load frames use: 3.2444691658s
step 4484/400000: train loss: 0.204069
epoch:97, batch24, load frames use: 3.29309201241s
step 4485/400000: train loss: 0.231645
epoch:97, batch25, load frames use: 4.368901968s
step 4486/400000: train loss: 0.189740
epoch:97, batch26, load frames use: 3.29735898972s
step 4487/400000: train loss: 0.232894
epoch:97, batch27, load frames use: 3.61236405373s
step 4488/400000: train loss: 0.204748
epoch:97, batch28, load frames use: 2.81166005135s
step 4489/400000: train loss: 0.219852
epoch:97, batch29, load frames use: 4.31419491768s
step 4490/400000: train loss: 0.223027
epoch:97, batch30, load frames use: 3.96129798889s
step 4491/400000: train loss: 0.232890
epoch:97, batch31, load frames use: 3.39684700966s
step 4492/400000: train loss: 0.236895
epoch:97, batch32, load frames use: 3.02419996262s
step 4493/400000: train loss: 0.212178
epoch:97, batch33, load frames use: 4.01937103271s
step 4494/400000: train loss: 0.208648
epoch:97, batch34, load frames use: 3.06967306137s
step 4495/400000: train loss: 0.207299
epoch:97, batch35, load frames use: 3.41873717308s
step 4496/400000: train loss: 0.227843
epoch:97, batch36, load frames use: 2.81438302994s
step 4497/400000: train loss: 0.205519
epoch:97, batch37, load frames use: 3.42348194122s
step 4498/400000: train loss: 0.199907
epoch:97, batch38, load frames use: 3.39554715157s
step 4499/400000: train loss: 0.214047
epoch:97, batch39, load frames use: 3.21952605247s
step 4500/400000: train loss: 0.214035
step 4500/400000: validation loss: 0.227152
epoch:97, batch40, load frames use: 2.96532297134s
step 4501/400000: train loss: 0.192674
epoch:97, batch41, load frames use: 3.21103215218s
step 4502/400000: train loss: 0.209008
epoch:97, batch42, load frames use: 2.95807099342s
step 4503/400000: train loss: 0.197287
epoch:97, batch43, load frames use: 3.97202897072s
step 4504/400000: train loss: 0.216924
epoch:97, batch44, load frames use: 3.05396699905s
step 4505/400000: train loss: 0.208734
epoch:97, batch45, load frames use: 3.61186599731s
step 4506/400000: train loss: 0.213052
epoch:97, batch46, load frames use: 3.06735515594s
step 4507/400000: train loss: 0.213017
epoch:98, batch1, load frames use: 3.34747195244s
step 4508/400000: train loss: 0.186493
epoch:98, batch2, load frames use: 2.95190906525s
step 4509/400000: train loss: 0.203412
epoch:98, batch3, load frames use: 3.80574202538s
step 4510/400000: train loss: 0.209409
epoch:98, batch4, load frames use: 4.01179504395s
step 4511/400000: train loss: 0.168504
epoch:98, batch5, load frames use: 3.19765901566s
step 4512/400000: train loss: 0.203571
epoch:98, batch6, load frames use: 3.21371889114s
step 4513/400000: train loss: 0.211725
epoch:98, batch7, load frames use: 3.28911805153s
step 4514/400000: train loss: 0.210208
epoch:98, batch8, load frames use: 3.34084892273s
step 4515/400000: train loss: 0.177080
epoch:98, batch9, load frames use: 3.16083598137s
step 4516/400000: train loss: 0.196509
epoch:98, batch10, load frames use: 4.01804685593s
step 4517/400000: train loss: 0.198486
epoch:98, batch11, load frames use: 3.6573870182s
step 4518/400000: train loss: 0.201261
epoch:98, batch12, load frames use: 3.36235284805s
step 4519/400000: train loss: 0.218021
epoch:98, batch13, load frames use: 3.41385197639s
step 4520/400000: train loss: 0.204384
epoch:98, batch14, load frames use: 3.69498205185s
step 4521/400000: train loss: 0.172555
epoch:98, batch15, load frames use: 3.05777716637s
step 4522/400000: train loss: 0.220235
epoch:98, batch16, load frames use: 3.65150308609s
step 4523/400000: train loss: 0.218134
epoch:98, batch17, load frames use: 3.31108093262s
step 4524/400000: train loss: 0.200223
epoch:98, batch18, load frames use: 3.73259210587s
step 4525/400000: train loss: 0.185827
epoch:98, batch19, load frames use: 3.02437090874s
step 4526/400000: train loss: 0.203004
epoch:98, batch20, load frames use: 3.73984599113s
step 4527/400000: train loss: 0.196184
epoch:98, batch21, load frames use: 3.11986112595s
step 4528/400000: train loss: 0.198166
epoch:98, batch22, load frames use: 3.54737281799s
step 4529/400000: train loss: 0.207645
epoch:98, batch23, load frames use: 3.73536896706s
step 4530/400000: train loss: 0.211935
epoch:98, batch24, load frames use: 3.02533197403s
step 4531/400000: train loss: 0.222082
epoch:98, batch25, load frames use: 3.73684000969s
step 4532/400000: train loss: 0.203188
epoch:98, batch26, load frames use: 3.66003298759s
step 4533/400000: train loss: 0.196682
epoch:98, batch27, load frames use: 2.8944029808s
step 4534/400000: train loss: 0.194639
epoch:98, batch28, load frames use: 4.05363512039s
step 4535/400000: train loss: 0.215277
epoch:98, batch29, load frames use: 3.23268699646s
step 4536/400000: train loss: 0.195504
epoch:98, batch30, load frames use: 3.27366495132s
step 4537/400000: train loss: 0.213605
epoch:98, batch31, load frames use: 3.2326669693s
step 4538/400000: train loss: 0.196657
epoch:98, batch32, load frames use: 4.57858991623s
step 4539/400000: train loss: 0.206313
epoch:98, batch33, load frames use: 3.67108893394s
step 4540/400000: train loss: 0.180544
epoch:98, batch34, load frames use: 3.36097192764s
step 4541/400000: train loss: 0.195737
epoch:98, batch35, load frames use: 2.88423609734s
step 4542/400000: train loss: 0.198992
epoch:98, batch36, load frames use: 4.39340901375s
step 4543/400000: train loss: 0.213839
epoch:98, batch37, load frames use: 4.20098114014s
step 4544/400000: train loss: 0.235230
epoch:98, batch38, load frames use: 3.99906206131s
step 4545/400000: train loss: 0.220261
epoch:98, batch39, load frames use: 3.22918605804s
step 4546/400000: train loss: 0.206623
epoch:98, batch40, load frames use: 3.45451211929s
step 4547/400000: train loss: 0.215024
epoch:98, batch41, load frames use: 3.34129309654s
step 4548/400000: train loss: 0.203440
epoch:98, batch42, load frames use: 3.64096784592s
step 4549/400000: train loss: 0.209763
epoch:98, batch43, load frames use: 3.23535513878s
step 4550/400000: train loss: 0.194491
epoch:98, batch44, load frames use: 3.93946290016s
step 4551/400000: train loss: 0.193554
epoch:98, batch45, load frames use: 4.09677910805s
step 4552/400000: train loss: 0.218640
epoch:98, batch46, load frames use: 3.23634886742s
step 4553/400000: train loss: 0.189046
epoch:99, batch1, load frames use: 2.79296898842s
step 4554/400000: train loss: 0.188460
epoch:99, batch2, load frames use: 4.14459085464s
step 4555/400000: train loss: 0.211108
epoch:99, batch3, load frames use: 3.65666389465s
step 4556/400000: train loss: 0.216473
epoch:99, batch4, load frames use: 3.26868700981s
step 4557/400000: train loss: 0.206546
epoch:99, batch5, load frames use: 3.35047793388s
step 4558/400000: train loss: 0.204349
epoch:99, batch6, load frames use: 3.79065394402s
step 4559/400000: train loss: 0.200006
epoch:99, batch7, load frames use: 3.62035489082s
step 4560/400000: train loss: 0.183113
epoch:99, batch8, load frames use: 3.54507493973s
step 4561/400000: train loss: 0.208552
epoch:99, batch9, load frames use: 3.32352495193s
step 4562/400000: train loss: 0.194766
epoch:99, batch10, load frames use: 3.79428195953s
step 4563/400000: train loss: 0.196066
epoch:99, batch11, load frames use: 4.25722503662s
step 4564/400000: train loss: 0.208583
epoch:99, batch12, load frames use: 3.01537394524s
step 4565/400000: train loss: 0.208024
epoch:99, batch13, load frames use: 3.67938709259s
step 4566/400000: train loss: 0.209140
epoch:99, batch14, load frames use: 3.00193309784s
step 4567/400000: train loss: 0.202769
epoch:99, batch15, load frames use: 3.56226491928s
step 4568/400000: train loss: 0.195015
epoch:99, batch16, load frames use: 2.92404198647s
step 4569/400000: train loss: 0.204636
epoch:99, batch17, load frames use: 3.99365210533s
step 4570/400000: train loss: 0.203840
epoch:99, batch18, load frames use: 2.85825419426s
step 4571/400000: train loss: 0.204018
epoch:99, batch19, load frames use: 3.02295589447s
step 4572/400000: train loss: 0.208180
epoch:99, batch20, load frames use: 3.31029701233s
step 4573/400000: train loss: 0.212477
epoch:99, batch21, load frames use: 3.46851801872s
step 4574/400000: train loss: 0.214861
epoch:99, batch22, load frames use: 2.8558781147s
step 4575/400000: train loss: 0.196555
epoch:99, batch23, load frames use: 4.54090309143s
step 4576/400000: train loss: 0.227376
epoch:99, batch24, load frames use: 6.55218195915s
step 4577/400000: train loss: 0.201895
epoch:99, batch25, load frames use: 5.68587493896s
step 4578/400000: train loss: 0.213882
epoch:99, batch26, load frames use: 4.01466012001s
step 4579/400000: train loss: 0.188318
epoch:99, batch27, load frames use: 6.91401004791s
step 4580/400000: train loss: 0.199990
epoch:99, batch28, load frames use: 3.44594907761s
step 4581/400000: train loss: 0.208141
epoch:99, batch29, load frames use: 6.39673280716s
step 4582/400000: train loss: 0.194475
epoch:99, batch30, load frames use: 3.60296702385s
step 4583/400000: train loss: 0.193683
epoch:99, batch31, load frames use: 4.22525691986s
step 4584/400000: train loss: 0.234006
epoch:99, batch32, load frames use: 4.13140797615s
step 4585/400000: train loss: 0.244150
epoch:99, batch33, load frames use: 4.48044204712s
step 4586/400000: train loss: 0.194498
epoch:99, batch34, load frames use: 2.78699707985s
step 4587/400000: train loss: 0.215850
epoch:99, batch35, load frames use: 4.09432601929s
step 4588/400000: train loss: 0.221050
epoch:99, batch36, load frames use: 3.52258181572s
step 4589/400000: train loss: 0.232969
epoch:99, batch37, load frames use: 3.49034905434s
step 4590/400000: train loss: 0.209299
epoch:99, batch38, load frames use: 2.9146900177s
step 4591/400000: train loss: 0.233075
epoch:99, batch39, load frames use: 3.81553196907s
step 4592/400000: train loss: 0.226962
epoch:99, batch40, load frames use: 3.12626004219s
step 4593/400000: train loss: 0.225986
epoch:99, batch41, load frames use: 3.17774415016s
step 4594/400000: train loss: 0.204899
epoch:99, batch42, load frames use: 3.78110289574s
step 4595/400000: train loss: 0.221392
epoch:99, batch43, load frames use: 3.92545104027s
step 4596/400000: train loss: 0.229296
epoch:99, batch44, load frames use: 3.30035591125s
step 4597/400000: train loss: 0.227837
epoch:99, batch45, load frames use: 3.82401895523s
step 4598/400000: train loss: 0.209185
epoch:99, batch46, load frames use: 2.85644006729s
step 4599/400000: train loss: 0.212310
epoch:100, batch1, load frames use: 3.98390293121s
step 4600/400000: train loss: 0.165074
step 4600/400000: validation loss: 0.172405
epoch:100, batch2, load frames use: 3.05062913895s
step 4601/400000: train loss: 0.187939
epoch:100, batch3, load frames use: 3.46290206909s
step 4602/400000: train loss: 0.201279
epoch:100, batch4, load frames use: 3.2015349865s
step 4603/400000: train loss: 0.200459
epoch:100, batch5, load frames use: 3.69440698624s
step 4604/400000: train loss: 0.206781
epoch:100, batch6, load frames use: 3.66820907593s
step 4605/400000: train loss: 0.197131
epoch:100, batch7, load frames use: 4.10283303261s
step 4606/400000: train loss: 0.187674
epoch:100, batch8, load frames use: 4.05354380608s
step 4607/400000: train loss: 0.190467
epoch:100, batch9, load frames use: 3.12131214142s
step 4608/400000: train loss: 0.201573
epoch:100, batch10, load frames use: 3.46166396141s
step 4609/400000: train loss: 0.187315
epoch:100, batch11, load frames use: 4.00762486458s
step 4610/400000: train loss: 0.196218
epoch:100, batch12, load frames use: 3.66578698158s
step 4611/400000: train loss: 0.190203
epoch:100, batch13, load frames use: 2.53724479675s
step 4612/400000: train loss: 0.184285
epoch:100, batch14, load frames use: 3.12189102173s
step 4613/400000: train loss: 0.208050
epoch:100, batch15, load frames use: 3.01676797867s
step 4614/400000: train loss: 0.206329
epoch:100, batch16, load frames use: 3.33868908882s
step 4615/400000: train loss: 0.218923
epoch:100, batch17, load frames use: 3.04892396927s
step 4616/400000: train loss: 0.189769
epoch:100, batch18, load frames use: 3.47601699829s
step 4617/400000: train loss: 0.223487
epoch:100, batch19, load frames use: 3.11321401596s
step 4618/400000: train loss: 0.208649
epoch:100, batch20, load frames use: 4.2582321167s
step 4619/400000: train loss: 0.202367
epoch:100, batch21, load frames use: 3.68335103989s
step 4620/400000: train loss: 0.210022
epoch:100, batch22, load frames use: 3.09789705276s
step 4621/400000: train loss: 0.211389
epoch:100, batch23, load frames use: 3.49800014496s
step 4622/400000: train loss: 0.231466
epoch:100, batch24, load frames use: 4.48979783058s
step 4623/400000: train loss: 0.201214
epoch:100, batch25, load frames use: 4.18962883949s
step 4624/400000: train loss: 0.186957
epoch:100, batch26, load frames use: 3.8950150013s
step 4625/400000: train loss: 0.189205
epoch:100, batch27, load frames use: 3.52666091919s
step 4626/400000: train loss: 0.205196
epoch:100, batch28, load frames use: 4.73530101776s
step 4627/400000: train loss: 0.213861
epoch:100, batch29, load frames use: 6.83549690247s
step 4628/400000: train loss: 0.204290
epoch:100, batch30, load frames use: 6.51801085472s
step 4629/400000: train loss: 0.208692
epoch:100, batch31, load frames use: 3.88489699364s
step 4630/400000: train loss: 0.210047
epoch:100, batch32, load frames use: 7.11412191391s
step 4631/400000: train loss: 0.236165
epoch:100, batch33, load frames use: 4.09359192848s
step 4632/400000: train loss: 0.191292
epoch:100, batch34, load frames use: 3.83817005157s
step 4633/400000: train loss: 0.222861
epoch:100, batch35, load frames use: 3.04668903351s
step 4634/400000: train loss: 0.204448
epoch:100, batch36, load frames use: 6.16749691963s
step 4635/400000: train loss: 0.196140
epoch:100, batch37, load frames use: 3.82564496994s
step 4636/400000: train loss: 0.190519
epoch:100, batch38, load frames use: 3.84826803207s
step 4637/400000: train loss: 0.196795
epoch:100, batch39, load frames use: 3.75552487373s
step 4638/400000: train loss: 0.201627
epoch:100, batch40, load frames use: 4.771682024s
step 4639/400000: train loss: 0.227834
epoch:100, batch41, load frames use: 3.11095714569s
step 4640/400000: train loss: 0.219685
epoch:100, batch42, load frames use: 3.64351296425s
step 4641/400000: train loss: 0.202329
epoch:100, batch43, load frames use: 3.73688101768s
step 4642/400000: train loss: 0.224614
epoch:100, batch44, load frames use: 3.00697088242s
step 4643/400000: train loss: 0.200018
epoch:100, batch45, load frames use: 4.10780906677s
step 4644/400000: train loss: 0.195034
epoch:100, batch46, load frames use: 4.1072371006s
step 4645/400000: train loss: 0.196560
epoch:101, batch1, load frames use: 3.22240900993s
step 4646/400000: train loss: 0.208816
epoch:101, batch2, load frames use: 3.24807214737s
step 4647/400000: train loss: 0.202516
epoch:101, batch3, load frames use: 4.23570394516s
step 4648/400000: train loss: 0.211734
epoch:101, batch4, load frames use: 4.12343382835s
step 4649/400000: train loss: 0.195222
epoch:101, batch5, load frames use: 4.73912501335s
step 4650/400000: train loss: 0.185026
epoch:101, batch6, load frames use: 3.13108801842s
step 4651/400000: train loss: 0.209821
epoch:101, batch7, load frames use: 4.14934778214s
step 4652/400000: train loss: 0.226034
epoch:101, batch8, load frames use: 3.58517885208s
step 4653/400000: train loss: 0.201425
epoch:101, batch9, load frames use: 3.46323299408s
step 4654/400000: train loss: 0.188865
epoch:101, batch10, load frames use: 2.87452292442s
step 4655/400000: train loss: 0.221975
epoch:101, batch11, load frames use: 4.17013597488s
step 4656/400000: train loss: 0.195502
epoch:101, batch12, load frames use: 4.20208883286s
step 4657/400000: train loss: 0.183196
epoch:101, batch13, load frames use: 7.66988515854s
step 4658/400000: train loss: 0.224400
epoch:101, batch14, load frames use: 3.10564398766s
step 4659/400000: train loss: 0.218594
epoch:101, batch15, load frames use: 4.43705606461s
step 4660/400000: train loss: 0.183826
epoch:101, batch16, load frames use: 4.18644881248s
step 4661/400000: train loss: 0.177555
epoch:101, batch17, load frames use: 4.34677600861s
step 4662/400000: train loss: 0.191276
epoch:101, batch18, load frames use: 3.25549602509s
step 4663/400000: train loss: 0.206665
epoch:101, batch19, load frames use: 3.67766690254s
step 4664/400000: train loss: 0.196547
epoch:101, batch20, load frames use: 4.31678199768s
step 4665/400000: train loss: 0.236065
epoch:101, batch21, load frames use: 3.93582010269s
step 4666/400000: train loss: 0.200793
epoch:101, batch22, load frames use: 2.87286806107s
step 4667/400000: train loss: 0.182439
epoch:101, batch23, load frames use: 4.29135394096s
step 4668/400000: train loss: 0.176955
epoch:101, batch24, load frames use: 3.51513910294s
step 4669/400000: train loss: 0.196667
epoch:101, batch25, load frames use: 4.43606805801s
step 4670/400000: train loss: 0.189150
epoch:101, batch26, load frames use: 3.71671199799s
step 4671/400000: train loss: 0.185449
epoch:101, batch27, load frames use: 3.66546702385s
step 4672/400000: train loss: 0.202323
epoch:101, batch28, load frames use: 3.17622590065s
step 4673/400000: train loss: 0.183791
epoch:101, batch29, load frames use: 4.34319591522s
step 4674/400000: train loss: 0.208379
epoch:101, batch30, load frames use: 4.11019015312s
step 4675/400000: train loss: 0.208677
epoch:101, batch31, load frames use: 4.03286910057s
step 4676/400000: train loss: 0.204993
epoch:101, batch32, load frames use: 3.45017814636s
step 4677/400000: train loss: 0.191981
epoch:101, batch33, load frames use: 3.72299599648s
step 4678/400000: train loss: 0.206108
epoch:101, batch34, load frames use: 4.41272807121s
step 4679/400000: train loss: 0.214463
epoch:101, batch35, load frames use: 3.72441482544s
step 4680/400000: train loss: 0.180421
epoch:101, batch36, load frames use: 3.3717918396s
step 4681/400000: train loss: 0.203706
epoch:101, batch37, load frames use: 4.13812613487s
step 4682/400000: train loss: 0.231219
epoch:101, batch38, load frames use: 4.34788417816s
step 4683/400000: train loss: 0.209767
epoch:101, batch39, load frames use: 4.0142428875s
step 4684/400000: train loss: 0.197926
epoch:101, batch40, load frames use: 3.56170296669s
step 4685/400000: train loss: 0.216939
epoch:101, batch41, load frames use: 3.3034350872s
step 4686/400000: train loss: 0.191409
epoch:101, batch42, load frames use: 3.63738322258s
step 4687/400000: train loss: 0.199364
epoch:101, batch43, load frames use: 4.03247404099s
step 4688/400000: train loss: 0.215563
epoch:101, batch44, load frames use: 3.52615714073s
step 4689/400000: train loss: 0.185684
epoch:101, batch45, load frames use: 3.58343887329s
step 4690/400000: train loss: 0.221214
epoch:101, batch46, load frames use: 3.63335514069s
step 4691/400000: train loss: 0.214550
epoch:102, batch1, load frames use: 3.34573888779s
step 4692/400000: train loss: 0.205940
epoch:102, batch2, load frames use: 5.54728388786s
step 4693/400000: train loss: 0.181908
epoch:102, batch3, load frames use: 4.00581407547s
step 4694/400000: train loss: 0.205972
epoch:102, batch4, load frames use: 3.31560206413s
step 4695/400000: train loss: 0.190677
epoch:102, batch5, load frames use: 4.03993487358s
step 4696/400000: train loss: 0.187353
epoch:102, batch6, load frames use: 3.64842009544s
step 4697/400000: train loss: 0.237302
epoch:102, batch7, load frames use: 3.51679396629s
step 4698/400000: train loss: 0.217651
epoch:102, batch8, load frames use: 3.76942396164s
step 4699/400000: train loss: 0.193116
epoch:102, batch9, load frames use: 3.33864092827s
step 4700/400000: train loss: 0.212016
step 4700/400000: validation loss: 0.197577
epoch:102, batch10, load frames use: 3.82633185387s
step 4701/400000: train loss: 0.184024
epoch:102, batch11, load frames use: 4.38742089272s
step 4702/400000: train loss: 0.193901
epoch:102, batch12, load frames use: 2.99602508545s
step 4703/400000: train loss: 0.199000
epoch:102, batch13, load frames use: 3.45937895775s
step 4704/400000: train loss: 0.203488
epoch:102, batch14, load frames use: 2.98005104065s
step 4705/400000: train loss: 0.174218
epoch:102, batch15, load frames use: 3.10671806335s
step 4706/400000: train loss: 0.187289
epoch:102, batch16, load frames use: 3.8105609417s
step 4707/400000: train loss: 0.199570
epoch:102, batch17, load frames use: 2.81280899048s
step 4708/400000: train loss: 0.190822
epoch:102, batch18, load frames use: 2.96434998512s
step 4709/400000: train loss: 0.212331
epoch:102, batch19, load frames use: 3.87139296532s
step 4710/400000: train loss: 0.224840
epoch:102, batch20, load frames use: 3.14508509636s
step 4711/400000: train loss: 0.197914
epoch:102, batch21, load frames use: 3.3795671463s
step 4712/400000: train loss: 0.222512
epoch:102, batch22, load frames use: 3.14020419121s
step 4713/400000: train loss: 0.205681
epoch:102, batch23, load frames use: 2.83334088326s
step 4714/400000: train loss: 0.225199
epoch:102, batch24, load frames use: 2.88160705566s
step 4715/400000: train loss: 0.191525
epoch:102, batch25, load frames use: 3.70454216003s
step 4716/400000: train loss: 0.199166
epoch:102, batch26, load frames use: 3.22846508026s
step 4717/400000: train loss: 0.214473
epoch:102, batch27, load frames use: 3.31175208092s
step 4718/400000: train loss: 0.183679
epoch:102, batch28, load frames use: 3.9602689743s
step 4719/400000: train loss: 0.242959
epoch:102, batch29, load frames use: 3.03541994095s
step 4720/400000: train loss: 0.208029
epoch:102, batch30, load frames use: 3.20058703423s
step 4721/400000: train loss: 0.221044
epoch:102, batch31, load frames use: 3.54187607765s
step 4722/400000: train loss: 0.179326
epoch:102, batch32, load frames use: 2.93151021004s
step 4723/400000: train loss: 0.191513
epoch:102, batch33, load frames use: 3.51360583305s
step 4724/400000: train loss: 0.189605
epoch:102, batch34, load frames use: 3.30940008163s
step 4725/400000: train loss: 0.214107
epoch:102, batch35, load frames use: 2.6324532032s
step 4726/400000: train loss: 0.219749
epoch:102, batch36, load frames use: 2.91338300705s
step 4727/400000: train loss: 0.201972
epoch:102, batch37, load frames use: 3.81759786606s
step 4728/400000: train loss: 0.204664
epoch:102, batch38, load frames use: 2.69000005722s
step 4729/400000: train loss: 0.207839
epoch:102, batch39, load frames use: 2.87935495377s
step 4730/400000: train loss: 0.182737
epoch:102, batch40, load frames use: 3.88574910164s
step 4731/400000: train loss: 0.220900
epoch:102, batch41, load frames use: 3.04828596115s
step 4732/400000: train loss: 0.211342
epoch:102, batch42, load frames use: 3.22514200211s
step 4733/400000: train loss: 0.224704
epoch:102, batch43, load frames use: 3.30444598198s
step 4734/400000: train loss: 0.217071
epoch:102, batch44, load frames use: 3.47966909409s
step 4735/400000: train loss: 0.230471
epoch:102, batch45, load frames use: 2.63578414917s
step 4736/400000: train loss: 0.220800
epoch:102, batch46, load frames use: 3.94485020638s
step 4737/400000: train loss: 0.211608
epoch:103, batch1, load frames use: 3.21665501595s
step 4738/400000: train loss: 0.191906
epoch:103, batch2, load frames use: 3.32274317741s
step 4739/400000: train loss: 0.200407
epoch:103, batch3, load frames use: 3.29293203354s
step 4740/400000: train loss: 0.200838
epoch:103, batch4, load frames use: 2.86067199707s
step 4741/400000: train loss: 0.215269
epoch:103, batch5, load frames use: 3.38172721863s
step 4742/400000: train loss: 0.199861
epoch:103, batch6, load frames use: 3.81719303131s
step 4743/400000: train loss: 0.184317
epoch:103, batch7, load frames use: 3.40324115753s
step 4744/400000: train loss: 0.188769
epoch:103, batch8, load frames use: 3.30157804489s
step 4745/400000: train loss: 0.183950
epoch:103, batch9, load frames use: 9.49672818184s
step 4746/400000: train loss: 0.214483
epoch:103, batch10, load frames use: 2.84706091881s
step 4747/400000: train loss: 0.221646
epoch:103, batch11, load frames use: 4.19536113739s
step 4748/400000: train loss: 0.204455
epoch:103, batch12, load frames use: 3.63637685776s
step 4749/400000: train loss: 0.193045
epoch:103, batch13, load frames use: 5.15764093399s
step 4750/400000: train loss: 0.192123
epoch:103, batch14, load frames use: 10.7606971264s
step 4751/400000: train loss: 0.207992
epoch:103, batch15, load frames use: 3.00991415977s
step 4752/400000: train loss: 0.217809
epoch:103, batch16, load frames use: 3.10480308533s
step 4753/400000: train loss: 0.183094
epoch:103, batch17, load frames use: 3.32490897179s
step 4754/400000: train loss: 0.187617
epoch:103, batch18, load frames use: 2.97864222527s
step 4755/400000: train loss: 0.200433
epoch:103, batch19, load frames use: 6.82527017593s
step 4756/400000: train loss: 0.230893
epoch:103, batch20, load frames use: 3.0485868454s
step 4757/400000: train loss: 0.198309
epoch:103, batch21, load frames use: 3.211769104s
step 4758/400000: train loss: 0.224280
epoch:103, batch22, load frames use: 3.09786605835s
step 4759/400000: train loss: 0.205536
epoch:103, batch23, load frames use: 3.0486240387s
step 4760/400000: train loss: 0.217778
epoch:103, batch24, load frames use: 5.06500816345s
step 4761/400000: train loss: 0.200185
epoch:103, batch25, load frames use: 4.25376796722s
step 4762/400000: train loss: 0.190354
epoch:103, batch26, load frames use: 3.62337183952s
step 4763/400000: train loss: 0.219857
epoch:103, batch27, load frames use: 3.32035088539s
step 4764/400000: train loss: 0.207278
epoch:103, batch28, load frames use: 2.91120100021s
step 4765/400000: train loss: 0.196248
epoch:103, batch29, load frames use: 7.02893090248s
step 4766/400000: train loss: 0.212230
epoch:103, batch30, load frames use: 7.4981341362s
step 4767/400000: train loss: 0.220863
epoch:103, batch31, load frames use: 6.78740906715s
step 4768/400000: train loss: 0.194511
epoch:103, batch32, load frames use: 4.07927012444s
step 4769/400000: train loss: 0.244027
epoch:103, batch33, load frames use: 2.96872591972s
step 4770/400000: train loss: 0.203937
epoch:103, batch34, load frames use: 9.67693209648s
step 4771/400000: train loss: 0.220135
epoch:103, batch35, load frames use: 7.53675484657s
step 4772/400000: train loss: 0.203108
epoch:103, batch36, load frames use: 3.17516493797s
step 4773/400000: train loss: 0.252325
epoch:103, batch37, load frames use: 3.21524906158s
step 4774/400000: train loss: 0.214715
epoch:103, batch38, load frames use: 4.94792914391s
step 4775/400000: train loss: 0.211618
epoch:103, batch39, load frames use: 4.23878788948s
step 4776/400000: train loss: 0.183731
epoch:103, batch40, load frames use: 3.70226311684s
step 4777/400000: train loss: 0.180266
epoch:103, batch41, load frames use: 6.89452409744s
step 4778/400000: train loss: 0.187421
epoch:103, batch42, load frames use: 3.09467411041s
step 4779/400000: train loss: 0.173677
epoch:103, batch43, load frames use: 3.54265618324s
step 4780/400000: train loss: 0.187229
epoch:103, batch44, load frames use: 6.89095211029s
step 4781/400000: train loss: 0.223334
epoch:103, batch45, load frames use: 7.07250404358s
step 4782/400000: train loss: 0.218326
epoch:103, batch46, load frames use: 5.38467884064s
step 4783/400000: train loss: 0.225204
epoch:104, batch1, load frames use: 3.03581905365s
step 4784/400000: train loss: 0.214430
epoch:104, batch2, load frames use: 2.83108305931s
step 4785/400000: train loss: 0.182499
epoch:104, batch3, load frames use: 5.01071214676s
step 4786/400000: train loss: 0.195059
epoch:104, batch4, load frames use: 5.83756399155s
step 4787/400000: train loss: 0.173617
epoch:104, batch5, load frames use: 3.31687021255s
step 4788/400000: train loss: 0.188745
epoch:104, batch6, load frames use: 3.23130106926s
step 4789/400000: train loss: 0.185524
epoch:104, batch7, load frames use: 4.43921899796s
step 4790/400000: train loss: 0.207161
epoch:104, batch8, load frames use: 6.16575503349s
step 4791/400000: train loss: 0.197944
epoch:104, batch9, load frames use: 3.69274783134s
step 4792/400000: train loss: 0.210801
epoch:104, batch10, load frames use: 3.94201588631s
step 4793/400000: train loss: 0.228695
epoch:104, batch11, load frames use: 3.55251908302s
step 4794/400000: train loss: 0.211773
epoch:104, batch12, load frames use: 3.64142298698s
step 4795/400000: train loss: 0.194437
epoch:104, batch13, load frames use: 3.22360086441s
step 4796/400000: train loss: 0.213171
epoch:104, batch14, load frames use: 3.15870308876s
step 4797/400000: train loss: 0.183791
epoch:104, batch15, load frames use: 3.33777499199s
step 4798/400000: train loss: 0.186332
epoch:104, batch16, load frames use: 3.65120005608s
step 4799/400000: train loss: 0.213126
epoch:104, batch17, load frames use: 3.59311294556s
step 4800/400000: train loss: 0.200187
step 4800/400000: validation loss: 0.181766
epoch:104, batch18, load frames use: 3.5956120491s
step 4801/400000: train loss: 0.195062
epoch:104, batch19, load frames use: 3.34177994728s
step 4802/400000: train loss: 0.178917
epoch:104, batch20, load frames use: 3.70857596397s
step 4803/400000: train loss: 0.227895
epoch:104, batch21, load frames use: 3.51220607758s
step 4804/400000: train loss: 0.180355
epoch:104, batch22, load frames use: 3.07221794128s
step 4805/400000: train loss: 0.184249
epoch:104, batch23, load frames use: 3.20027017593s
step 4806/400000: train loss: 0.208837
epoch:104, batch24, load frames use: 3.1951508522s
step 4807/400000: train loss: 0.208634
epoch:104, batch25, load frames use: 3.20781803131s
step 4808/400000: train loss: 0.203233
epoch:104, batch26, load frames use: 3.75769090652s
step 4809/400000: train loss: 0.178055
epoch:104, batch27, load frames use: 2.79313898087s
step 4810/400000: train loss: 0.211030
epoch:104, batch28, load frames use: 3.31970977783s
step 4811/400000: train loss: 0.210753
epoch:104, batch29, load frames use: 3.1638610363s
step 4812/400000: train loss: 0.212692
epoch:104, batch30, load frames use: 3.90591406822s
step 4813/400000: train loss: 0.189344
epoch:104, batch31, load frames use: 3.26763796806s
step 4814/400000: train loss: 0.218863
epoch:104, batch32, load frames use: 3.7154071331s
step 4815/400000: train loss: 0.207432
epoch:104, batch33, load frames use: 3.17422389984s
step 4816/400000: train loss: 0.214025
epoch:104, batch34, load frames use: 3.54152202606s
step 4817/400000: train loss: 0.181793
epoch:104, batch35, load frames use: 3.02948093414s
step 4818/400000: train loss: 0.184309
epoch:104, batch36, load frames use: 4.25000810623s
step 4819/400000: train loss: 0.224617
epoch:104, batch37, load frames use: 3.83050894737s
step 4820/400000: train loss: 0.196982
epoch:104, batch38, load frames use: 3.14078497887s
step 4821/400000: train loss: 0.179661
epoch:104, batch39, load frames use: 3.5841319561s
step 4822/400000: train loss: 0.194204
epoch:104, batch40, load frames use: 3.27175211906s
step 4823/400000: train loss: 0.203832
epoch:104, batch41, load frames use: 3.24819302559s
step 4824/400000: train loss: 0.194982
epoch:104, batch42, load frames use: 3.16595411301s
step 4825/400000: train loss: 0.191686
epoch:104, batch43, load frames use: 3.69332695007s
step 4826/400000: train loss: 0.220891
epoch:104, batch44, load frames use: 3.11540603638s
step 4827/400000: train loss: 0.185305
epoch:104, batch45, load frames use: 3.421626091s
step 4828/400000: train loss: 0.237672
epoch:104, batch46, load frames use: 3.32268381119s
step 4829/400000: train loss: 0.248047
epoch:105, batch1, load frames use: 4.90231704712s
step 4830/400000: train loss: 0.182193
epoch:105, batch2, load frames use: 3.22433590889s
step 4831/400000: train loss: 0.204287
epoch:105, batch3, load frames use: 3.57041788101s
step 4832/400000: train loss: 0.193536
epoch:105, batch4, load frames use: 3.07592797279s
step 4833/400000: train loss: 0.190305
epoch:105, batch5, load frames use: 4.3225300312s
step 4834/400000: train loss: 0.211080
epoch:105, batch6, load frames use: 4.38507795334s
step 4835/400000: train loss: 0.208580
epoch:105, batch7, load frames use: 3.6497220993s
step 4836/400000: train loss: 0.177046
epoch:105, batch8, load frames use: 4.59068608284s
step 4837/400000: train loss: 0.204521
epoch:105, batch9, load frames use: 3.34652209282s
step 4838/400000: train loss: 0.197364
epoch:105, batch10, load frames use: 3.81286001205s
step 4839/400000: train loss: 0.209209
epoch:105, batch11, load frames use: 3.54248380661s
step 4840/400000: train loss: 0.197334
epoch:105, batch12, load frames use: 3.39822006226s
step 4841/400000: train loss: 0.172519
epoch:105, batch13, load frames use: 4.28144907951s
step 4842/400000: train loss: 0.187182
epoch:105, batch14, load frames use: 3.11666989326s
step 4843/400000: train loss: 0.223113
epoch:105, batch15, load frames use: 3.55682396889s
step 4844/400000: train loss: 0.197569
epoch:105, batch16, load frames use: 3.40427303314s
step 4845/400000: train loss: 0.195435
epoch:105, batch17, load frames use: 3.38348698616s
step 4846/400000: train loss: 0.200778
epoch:105, batch18, load frames use: 3.18635606766s
step 4847/400000: train loss: 0.205291
epoch:105, batch19, load frames use: 3.64602708817s
step 4848/400000: train loss: 0.195196
epoch:105, batch20, load frames use: 3.10450291634s
step 4849/400000: train loss: 0.208339
epoch:105, batch21, load frames use: 4.32728981972s
step 4850/400000: train loss: 0.191374
epoch:105, batch22, load frames use: 3.48511600494s
step 4851/400000: train loss: 0.188377
epoch:105, batch23, load frames use: 3.35428905487s
step 4852/400000: train loss: 0.194287
epoch:105, batch24, load frames use: 3.1777780056s
step 4853/400000: train loss: 0.184430
epoch:105, batch25, load frames use: 3.14304494858s
step 4854/400000: train loss: 0.191181
epoch:105, batch26, load frames use: 4.43019509315s
step 4855/400000: train loss: 0.200267
epoch:105, batch27, load frames use: 3.55789709091s
step 4856/400000: train loss: 0.201789
epoch:105, batch28, load frames use: 2.61897921562s
step 4857/400000: train loss: 0.213501
epoch:105, batch29, load frames use: 3.15009093285s
step 4858/400000: train loss: 0.203088
epoch:105, batch30, load frames use: 4.72676205635s
step 4859/400000: train loss: 0.198468
epoch:105, batch31, load frames use: 4.751527071s
step 4860/400000: train loss: 0.208566
epoch:105, batch32, load frames use: 3.73489904404s
step 4861/400000: train loss: 0.195761
epoch:105, batch33, load frames use: 2.91301393509s
step 4862/400000: train loss: 0.206524
epoch:105, batch34, load frames use: 3.94414782524s
step 4863/400000: train loss: 0.217202
epoch:105, batch35, load frames use: 3.93613815308s
step 4864/400000: train loss: 0.216874
epoch:105, batch36, load frames use: 3.97206401825s
step 4865/400000: train loss: 0.201579
epoch:105, batch37, load frames use: 3.2043659687s
step 4866/400000: train loss: 0.209734
epoch:105, batch38, load frames use: 3.36769914627s
step 4867/400000: train loss: 0.219667
epoch:105, batch39, load frames use: 3.86416602135s
step 4868/400000: train loss: 0.203737
epoch:105, batch40, load frames use: 3.24500083923s
step 4869/400000: train loss: 0.193697
epoch:105, batch41, load frames use: 3.28545022011s
step 4870/400000: train loss: 0.220288
epoch:105, batch42, load frames use: 3.93370604515s
step 4871/400000: train loss: 0.189365
epoch:105, batch43, load frames use: 3.15567302704s
step 4872/400000: train loss: 0.205761
epoch:105, batch44, load frames use: 3.46696090698s
step 4873/400000: train loss: 0.185174
epoch:105, batch45, load frames use: 3.3787150383s
step 4874/400000: train loss: 0.213329
epoch:105, batch46, load frames use: 3.37866306305s
step 4875/400000: train loss: 0.197061
epoch:106, batch1, load frames use: 3.15796113014s
step 4876/400000: train loss: 0.192862
epoch:106, batch2, load frames use: 4.12820982933s
step 4877/400000: train loss: 0.208664
epoch:106, batch3, load frames use: 4.03286504745s
step 4878/400000: train loss: 0.189933
epoch:106, batch4, load frames use: 3.92329406738s
step 4879/400000: train loss: 0.178109
epoch:106, batch5, load frames use: 3.40436816216s
step 4880/400000: train loss: 0.198367
epoch:106, batch6, load frames use: 3.23734688759s
step 4881/400000: train loss: 0.214232
epoch:106, batch7, load frames use: 4.50983285904s
step 4882/400000: train loss: 0.202434
epoch:106, batch8, load frames use: 3.77671504021s
step 4883/400000: train loss: 0.194993
epoch:106, batch9, load frames use: 3.28472208977s
step 4884/400000: train loss: 0.190176
epoch:106, batch10, load frames use: 3.40303397179s
step 4885/400000: train loss: 0.217674
epoch:106, batch11, load frames use: 3.28753805161s
step 4886/400000: train loss: 0.212577
epoch:106, batch12, load frames use: 3.51116681099s
step 4887/400000: train loss: 0.225436
epoch:106, batch13, load frames use: 4.04518604279s
step 4888/400000: train loss: 0.206307
epoch:106, batch14, load frames use: 3.31188488007s
step 4889/400000: train loss: 0.179697
epoch:106, batch15, load frames use: 3.46438598633s
step 4890/400000: train loss: 0.193100
epoch:106, batch16, load frames use: 3.37540006638s
step 4891/400000: train loss: 0.194035
epoch:106, batch17, load frames use: 3.95755386353s
step 4892/400000: train loss: 0.226339
epoch:106, batch18, load frames use: 3.20051598549s
step 4893/400000: train loss: 0.186599
epoch:106, batch19, load frames use: 4.17539596558s
step 4894/400000: train loss: 0.201387
epoch:106, batch20, load frames use: 4.01137804985s
step 4895/400000: train loss: 0.209698
epoch:106, batch21, load frames use: 3.54124498367s
step 4896/400000: train loss: 0.187878
epoch:106, batch22, load frames use: 3.26711416245s
step 4897/400000: train loss: 0.228064
epoch:106, batch23, load frames use: 4.24773097038s
step 4898/400000: train loss: 0.195795
epoch:106, batch24, load frames use: 3.63182783127s
step 4899/400000: train loss: 0.177114
epoch:106, batch25, load frames use: 3.87875199318s
step 4900/400000: train loss: 0.193367
step 4900/400000: validation loss: 0.191146
epoch:106, batch26, load frames use: 3.66864609718s
step 4901/400000: train loss: 0.199544
epoch:106, batch27, load frames use: 3.37171506882s
step 4902/400000: train loss: 0.201240
epoch:106, batch28, load frames use: 3.45806694031s
step 4903/400000: train loss: 0.198832
epoch:106, batch29, load frames use: 3.36901307106s
step 4904/400000: train loss: 0.205137
epoch:106, batch30, load frames use: 3.43141198158s
step 4905/400000: train loss: 0.196500
epoch:106, batch31, load frames use: 3.64721989632s
step 4906/400000: train loss: 0.187026
epoch:106, batch32, load frames use: 3.53220200539s
step 4907/400000: train loss: 0.189256
epoch:106, batch33, load frames use: 3.34260821342s
step 4908/400000: train loss: 0.199871
epoch:106, batch34, load frames use: 3.2970559597s
step 4909/400000: train loss: 0.216752
epoch:106, batch35, load frames use: 3.31298494339s
step 4910/400000: train loss: 0.207450
epoch:106, batch36, load frames use: 3.56428790092s
step 4911/400000: train loss: 0.208899
epoch:106, batch37, load frames use: 3.85697412491s
step 4912/400000: train loss: 0.241821
epoch:106, batch38, load frames use: 3.57135415077s
step 4913/400000: train loss: 0.216232
epoch:106, batch39, load frames use: 3.97528791428s
step 4914/400000: train loss: 0.224420
epoch:106, batch40, load frames use: 3.17603182793s
step 4915/400000: train loss: 0.208491
epoch:106, batch41, load frames use: 4.2020111084s
step 4916/400000: train loss: 0.204559
epoch:106, batch42, load frames use: 3.45563793182s
step 4917/400000: train loss: 0.196398
epoch:106, batch43, load frames use: 3.45295119286s
step 4918/400000: train loss: 0.208545
epoch:106, batch44, load frames use: 3.24897003174s
step 4919/400000: train loss: 0.187690
epoch:106, batch45, load frames use: 3.6965649128s
step 4920/400000: train loss: 0.186982
epoch:106, batch46, load frames use: 4.22369503975s
step 4921/400000: train loss: 0.216207
epoch:107, batch1, load frames use: 3.69478988647s
step 4922/400000: train loss: 0.194073
epoch:107, batch2, load frames use: 3.64111399651s
step 4923/400000: train loss: 0.170102
epoch:107, batch3, load frames use: 3.95244479179s
step 4924/400000: train loss: 0.201419
epoch:107, batch4, load frames use: 3.47422504425s
step 4925/400000: train loss: 0.201557
epoch:107, batch5, load frames use: 4.51571106911s
step 4926/400000: train loss: 0.206122
epoch:107, batch6, load frames use: 3.40900993347s
step 4927/400000: train loss: 0.199881
epoch:107, batch7, load frames use: 3.57205796242s
step 4928/400000: train loss: 0.200782
epoch:107, batch8, load frames use: 3.36390805244s
step 4929/400000: train loss: 0.183140
epoch:107, batch9, load frames use: 4.01183700562s
step 4930/400000: train loss: 0.187728
epoch:107, batch10, load frames use: 3.62865900993s
step 4931/400000: train loss: 0.193966
epoch:107, batch11, load frames use: 3.41552495956s
step 4932/400000: train loss: 0.217491
epoch:107, batch12, load frames use: 3.4579668045s
step 4933/400000: train loss: 0.207795
epoch:107, batch13, load frames use: 3.72049713135s
step 4934/400000: train loss: 0.188218
epoch:107, batch14, load frames use: 3.32251310349s
step 4935/400000: train loss: 0.206938
epoch:107, batch15, load frames use: 3.91221094131s
step 4936/400000: train loss: 0.184073
epoch:107, batch16, load frames use: 3.48290586472s
step 4937/400000: train loss: 0.222604
epoch:107, batch17, load frames use: 3.44963598251s
step 4938/400000: train loss: 0.196415
epoch:107, batch18, load frames use: 3.28491091728s
step 4939/400000: train loss: 0.192391
epoch:107, batch19, load frames use: 4.12785005569s
step 4940/400000: train loss: 0.190823
epoch:107, batch20, load frames use: 3.26559901237s
step 4941/400000: train loss: 0.199851
epoch:107, batch21, load frames use: 3.69946789742s
step 4942/400000: train loss: 0.204170
epoch:107, batch22, load frames use: 3.8884408474s
step 4943/400000: train loss: 0.201822
epoch:107, batch23, load frames use: 3.5038921833s
step 4944/400000: train loss: 0.196703
epoch:107, batch24, load frames use: 3.781498909s
step 4945/400000: train loss: 0.191441
epoch:107, batch25, load frames use: 3.21026778221s
step 4946/400000: train loss: 0.187029
epoch:107, batch26, load frames use: 3.97893404961s
step 4947/400000: train loss: 0.201323
epoch:107, batch27, load frames use: 4.10661411285s
step 4948/400000: train loss: 0.203295
epoch:107, batch28, load frames use: 3.65246295929s
step 4949/400000: train loss: 0.194655
epoch:107, batch29, load frames use: 4.08346295357s
step 4950/400000: train loss: 0.198812
epoch:107, batch30, load frames use: 4.26217508316s
step 4951/400000: train loss: 0.207847
epoch:107, batch31, load frames use: 3.881772995s
step 4952/400000: train loss: 0.163833
epoch:107, batch32, load frames use: 3.56074404716s
step 4953/400000: train loss: 0.205598
epoch:107, batch33, load frames use: 2.9983189106s
step 4954/400000: train loss: 0.208731
epoch:107, batch34, load frames use: 3.40920114517s
step 4955/400000: train loss: 0.193201
epoch:107, batch35, load frames use: 4.07721400261s
step 4956/400000: train loss: 0.192419
epoch:107, batch36, load frames use: 4.08551502228s
step 4957/400000: train loss: 0.206124
epoch:107, batch37, load frames use: 4.37905883789s
step 4958/400000: train loss: 0.195867
epoch:107, batch38, load frames use: 4.41609501839s
step 4959/400000: train loss: 0.200859
epoch:107, batch39, load frames use: 3.8404738903s
step 4960/400000: train loss: 0.213612
epoch:107, batch40, load frames use: 3.2408721447s
step 4961/400000: train loss: 0.195252
epoch:107, batch41, load frames use: 3.12302994728s
step 4962/400000: train loss: 0.198984
epoch:107, batch42, load frames use: 3.74858593941s
step 4963/400000: train loss: 0.208338
epoch:107, batch43, load frames use: 4.3827111721s
step 4964/400000: train loss: 0.201088
epoch:107, batch44, load frames use: 4.21298885345s
step 4965/400000: train loss: 0.186642
epoch:107, batch45, load frames use: 4.10231423378s
step 4966/400000: train loss: 0.232939
epoch:107, batch46, load frames use: 4.61872696877s
step 4967/400000: train loss: 0.217081
epoch:108, batch1, load frames use: 3.5154941082s
step 4968/400000: train loss: 0.198013
epoch:108, batch2, load frames use: 4.52760219574s
step 4969/400000: train loss: 0.199183
epoch:108, batch3, load frames use: 3.88880705833s
step 4970/400000: train loss: 0.187211
epoch:108, batch4, load frames use: 4.10929489136s
step 4971/400000: train loss: 0.180713
epoch:108, batch5, load frames use: 3.8886680603s
step 4972/400000: train loss: 0.201805
epoch:108, batch6, load frames use: 3.56442499161s
step 4973/400000: train loss: 0.200559
epoch:108, batch7, load frames use: 3.62605190277s
step 4974/400000: train loss: 0.183290
epoch:108, batch8, load frames use: 3.63270878792s
step 4975/400000: train loss: 0.187961
epoch:108, batch9, load frames use: 3.40472197533s
step 4976/400000: train loss: 0.214327
epoch:108, batch10, load frames use: 4.48920798302s
step 4977/400000: train loss: 0.194789
epoch:108, batch11, load frames use: 3.50262594223s
step 4978/400000: train loss: 0.186674
epoch:108, batch12, load frames use: 3.2134680748s
step 4979/400000: train loss: 0.193538
epoch:108, batch13, load frames use: 3.68054509163s
step 4980/400000: train loss: 0.200562
epoch:108, batch14, load frames use: 3.61832308769s
step 4981/400000: train loss: 0.207559
epoch:108, batch15, load frames use: 4.20646595955s
step 4982/400000: train loss: 0.224967
epoch:108, batch16, load frames use: 3.04923605919s
step 4983/400000: train loss: 0.209918
epoch:108, batch17, load frames use: 4.93623781204s
step 4984/400000: train loss: 0.174704
epoch:108, batch18, load frames use: 4.11548614502s
step 4985/400000: train loss: 0.206594
epoch:108, batch19, load frames use: 3.20157694817s
step 4986/400000: train loss: 0.199258
epoch:108, batch20, load frames use: 4.13225197792s
step 4987/400000: train loss: 0.211923
epoch:108, batch21, load frames use: 3.82481002808s
step 4988/400000: train loss: 0.193348
epoch:108, batch22, load frames use: 3.83252811432s
step 4989/400000: train loss: 0.202871
epoch:108, batch23, load frames use: 7.44871211052s
step 4990/400000: train loss: 0.201103
epoch:108, batch24, load frames use: 3.90921807289s
step 4991/400000: train loss: 0.181023
epoch:108, batch25, load frames use: 6.41860294342s
step 4992/400000: train loss: 0.177905
epoch:108, batch26, load frames use: 3.1700360775s
step 4993/400000: train loss: 0.200543
epoch:108, batch27, load frames use: 6.05729794502s
step 4994/400000: train loss: 0.169138
epoch:108, batch28, load frames use: 4.33141303062s
step 4995/400000: train loss: 0.191886
epoch:108, batch29, load frames use: 3.53311014175s
step 4996/400000: train loss: 0.192388
epoch:108, batch30, load frames use: 5.45748591423s
step 4997/400000: train loss: 0.223729
epoch:108, batch31, load frames use: 4.27979707718s
step 4998/400000: train loss: 0.206620
epoch:108, batch32, load frames use: 5.74683213234s
step 4999/400000: train loss: 0.206922
epoch:108, batch33, load frames use: 3.87659597397s
step 5000/400000: train loss: 0.199875
step 5000/400000: validation loss: 0.188752
epoch:108, batch34, load frames use: 4.29561591148s
step 5001/400000: train loss: 0.175106
epoch:108, batch35, load frames use: 3.83295202255s
step 5002/400000: train loss: 0.225662
epoch:108, batch36, load frames use: 4.95114588737s
step 5003/400000: train loss: 0.229972
epoch:108, batch37, load frames use: 7.21432995796s
step 5004/400000: train loss: 0.189366
epoch:108, batch38, load frames use: 4.15830302238s
step 5005/400000: train loss: 0.215965
epoch:108, batch39, load frames use: 3.97086501122s
step 5006/400000: train loss: 0.199586
epoch:108, batch40, load frames use: 4.08870100975s
step 5007/400000: train loss: 0.184799
epoch:108, batch41, load frames use: 5.28383994102s
step 5008/400000: train loss: 0.199419
epoch:108, batch42, load frames use: 4.3922150135s
step 5009/400000: train loss: 0.195020
epoch:108, batch43, load frames use: 5.23801779747s
step 5010/400000: train loss: 0.176125
epoch:108, batch44, load frames use: 4.77290511131s
step 5011/400000: train loss: 0.205205
epoch:108, batch45, load frames use: 5.51343297958s
step 5012/400000: train loss: 0.228532
epoch:108, batch46, load frames use: 4.21026206017s
step 5013/400000: train loss: 0.229563
epoch:109, batch1, load frames use: 4.67952990532s
step 5014/400000: train loss: 0.194746
epoch:109, batch2, load frames use: 6.66805410385s
step 5015/400000: train loss: 0.185186
epoch:109, batch3, load frames use: 4.14700222015s
step 5016/400000: train loss: 0.189958
epoch:109, batch4, load frames use: 5.6461648941s
step 5017/400000: train loss: 0.202373
epoch:109, batch5, load frames use: 4.94351911545s
step 5018/400000: train loss: 0.193952
epoch:109, batch6, load frames use: 3.95621109009s
step 5019/400000: train loss: 0.187262
epoch:109, batch7, load frames use: 4.378428936s
step 5020/400000: train loss: 0.216172
epoch:109, batch8, load frames use: 3.60899305344s
step 5021/400000: train loss: 0.192446
epoch:109, batch9, load frames use: 3.72776699066s
step 5022/400000: train loss: 0.184650
epoch:109, batch10, load frames use: 3.42698597908s
step 5023/400000: train loss: 0.207234
epoch:109, batch11, load frames use: 4.29284000397s
step 5024/400000: train loss: 0.189584
epoch:109, batch12, load frames use: 3.80182313919s
step 5025/400000: train loss: 0.211917
epoch:109, batch13, load frames use: 4.1601600647s
step 5026/400000: train loss: 0.196837
epoch:109, batch14, load frames use: 3.88745212555s
step 5027/400000: train loss: 0.186869
epoch:109, batch15, load frames use: 3.6307990551s
step 5028/400000: train loss: 0.202952
epoch:109, batch16, load frames use: 3.97315907478s
step 5029/400000: train loss: 0.206051
epoch:109, batch17, load frames use: 3.768034935s
step 5030/400000: train loss: 0.191625
epoch:109, batch18, load frames use: 3.75165104866s
step 5031/400000: train loss: 0.190799
epoch:109, batch19, load frames use: 3.66759395599s
step 5032/400000: train loss: 0.202850
epoch:109, batch20, load frames use: 3.64280390739s
step 5033/400000: train loss: 0.184240
epoch:109, batch21, load frames use: 4.10799884796s
step 5034/400000: train loss: 0.197037
epoch:109, batch22, load frames use: 3.83571195602s
step 5035/400000: train loss: 0.203228
epoch:109, batch23, load frames use: 4.05362796783s
step 5036/400000: train loss: 0.190033
epoch:109, batch24, load frames use: 3.84604501724s
step 5037/400000: train loss: 0.186656
epoch:109, batch25, load frames use: 3.63742589951s
step 5038/400000: train loss: 0.203549
epoch:109, batch26, load frames use: 3.4401538372s
step 5039/400000: train loss: 0.171557
epoch:109, batch27, load frames use: 3.7382671833s
step 5040/400000: train loss: 0.196452
epoch:109, batch28, load frames use: 3.68074417114s
step 5041/400000: train loss: 0.217340
epoch:109, batch29, load frames use: 3.7580780983s
step 5042/400000: train loss: 0.225699
epoch:109, batch30, load frames use: 3.61538100243s
step 5043/400000: train loss: 0.177060
epoch:109, batch31, load frames use: 3.47376298904s
step 5044/400000: train loss: 0.192678
epoch:109, batch32, load frames use: 4.15699982643s
step 5045/400000: train loss: 0.201966
epoch:109, batch33, load frames use: 3.68318891525s
step 5046/400000: train loss: 0.186060
epoch:109, batch34, load frames use: 3.83825802803s
step 5047/400000: train loss: 0.179636
epoch:109, batch35, load frames use: 3.50077795982s
step 5048/400000: train loss: 0.183523
epoch:109, batch36, load frames use: 3.53137016296s
step 5049/400000: train loss: 0.213209
epoch:109, batch37, load frames use: 3.78677296638s
step 5050/400000: train loss: 0.197437
epoch:109, batch38, load frames use: 3.90145587921s
step 5051/400000: train loss: 0.207508
epoch:109, batch39, load frames use: 3.6826748848s
step 5052/400000: train loss: 0.223805
epoch:109, batch40, load frames use: 4.28064417839s
step 5053/400000: train loss: 0.212428
epoch:109, batch41, load frames use: 3.69677805901s
step 5054/400000: train loss: 0.189074
epoch:109, batch42, load frames use: 4.1619579792s
step 5055/400000: train loss: 0.200088
epoch:109, batch43, load frames use: 3.48400616646s
step 5056/400000: train loss: 0.183384
epoch:109, batch44, load frames use: 3.63787698746s
step 5057/400000: train loss: 0.203947
epoch:109, batch45, load frames use: 4.56310677528s
step 5058/400000: train loss: 0.206282
epoch:109, batch46, load frames use: 3.55566096306s
step 5059/400000: train loss: 0.204781
epoch:110, batch1, load frames use: 3.68343305588s
step 5060/400000: train loss: 0.201592
epoch:110, batch2, load frames use: 3.53379702568s
step 5061/400000: train loss: 0.207992
epoch:110, batch3, load frames use: 3.70397400856s
step 5062/400000: train loss: 0.187424
epoch:110, batch4, load frames use: 3.58427000046s
step 5063/400000: train loss: 0.181365
epoch:110, batch5, load frames use: 4.62280702591s
step 5064/400000: train loss: 0.183516
epoch:110, batch6, load frames use: 4.34801006317s
step 5065/400000: train loss: 0.211941
epoch:110, batch7, load frames use: 4.13109207153s
step 5066/400000: train loss: 0.188561
epoch:110, batch8, load frames use: 5.92043304443s
step 5067/400000: train loss: 0.184598
epoch:110, batch9, load frames use: 6.30873203278s
step 5068/400000: train loss: 0.179905
epoch:110, batch10, load frames use: 4.33524417877s
step 5069/400000: train loss: 0.222806
epoch:110, batch11, load frames use: 7.58868908882s
step 5070/400000: train loss: 0.179191
epoch:110, batch12, load frames use: 4.44116306305s
step 5071/400000: train loss: 0.201671
epoch:110, batch13, load frames use: 4.16194391251s
step 5072/400000: train loss: 0.215086
epoch:110, batch14, load frames use: 6.12674617767s
step 5073/400000: train loss: 0.189582
epoch:110, batch15, load frames use: 5.62343001366s
step 5074/400000: train loss: 0.193627
epoch:110, batch16, load frames use: 6.78864693642s
step 5075/400000: train loss: 0.193942
epoch:110, batch17, load frames use: 5.67864513397s
step 5076/400000: train loss: 0.192850
epoch:110, batch18, load frames use: 5.33074212074s
step 5077/400000: train loss: 0.189908
epoch:110, batch19, load frames use: 4.33598899841s
step 5078/400000: train loss: 0.228165
epoch:110, batch20, load frames use: 6.71217799187s
step 5079/400000: train loss: 0.190858
epoch:110, batch21, load frames use: 4.89114999771s
step 5080/400000: train loss: 0.181029
epoch:110, batch22, load frames use: 4.9326069355s
step 5081/400000: train loss: 0.221397
epoch:110, batch23, load frames use: 4.28235411644s
step 5082/400000: train loss: 0.196205
epoch:110, batch24, load frames use: 4.58873605728s
step 5083/400000: train loss: 0.185844
epoch:110, batch25, load frames use: 5.4262201786s
step 5084/400000: train loss: 0.227468
epoch:110, batch26, load frames use: 4.0568780899s
step 5085/400000: train loss: 0.211371
epoch:110, batch27, load frames use: 4.91323494911s
step 5086/400000: train loss: 0.185832
epoch:110, batch28, load frames use: 4.81042599678s
step 5087/400000: train loss: 0.189970
epoch:110, batch29, load frames use: 4.67123103142s
step 5088/400000: train loss: 0.182859
epoch:110, batch30, load frames use: 4.36380600929s
step 5089/400000: train loss: 0.211097
epoch:110, batch31, load frames use: 4.73079800606s
step 5090/400000: train loss: 0.175648
epoch:110, batch32, load frames use: 4.62933802605s
step 5091/400000: train loss: 0.185412
epoch:110, batch33, load frames use: 4.04682302475s
step 5092/400000: train loss: 0.187897
epoch:110, batch34, load frames use: 3.84945988655s
step 5093/400000: train loss: 0.181493
epoch:110, batch35, load frames use: 4.17499399185s
step 5094/400000: train loss: 0.195402
epoch:110, batch36, load frames use: 4.1182808876s
step 5095/400000: train loss: 0.195265
epoch:110, batch37, load frames use: 3.67189288139s
step 5096/400000: train loss: 0.203130
epoch:110, batch38, load frames use: 3.81926703453s
step 5097/400000: train loss: 0.187950
epoch:110, batch39, load frames use: 4.04366016388s
step 5098/400000: train loss: 0.181835
epoch:110, batch40, load frames use: 4.19864583015s
step 5099/400000: train loss: 0.184099
epoch:110, batch41, load frames use: 4.09130001068s
step 5100/400000: train loss: 0.196618
step 5100/400000: validation loss: 0.171734
epoch:110, batch42, load frames use: 4.0625500679s
step 5101/400000: train loss: 0.213056
epoch:110, batch43, load frames use: 3.62957000732s
step 5102/400000: train loss: 0.197930
epoch:110, batch44, load frames use: 3.44557595253s
step 5103/400000: train loss: 0.207849
epoch:110, batch45, load frames use: 3.61213588715s
step 5104/400000: train loss: 0.210746
epoch:110, batch46, load frames use: 3.88793516159s
step 5105/400000: train loss: 0.188800
epoch:111, batch1, load frames use: 3.5713031292s
step 5106/400000: train loss: 0.209449
epoch:111, batch2, load frames use: 4.09411096573s
step 5107/400000: train loss: 0.214997
epoch:111, batch3, load frames use: 3.79112195969s
step 5108/400000: train loss: 0.212690
epoch:111, batch4, load frames use: 3.09512114525s
step 5109/400000: train loss: 0.199458
epoch:111, batch5, load frames use: 3.32357501984s
step 5110/400000: train loss: 0.183142
epoch:111, batch6, load frames use: 3.43581700325s
step 5111/400000: train loss: 0.180848
epoch:111, batch7, load frames use: 4.1464009285s
step 5112/400000: train loss: 0.192141
epoch:111, batch8, load frames use: 3.33918595314s
step 5113/400000: train loss: 0.197871
epoch:111, batch9, load frames use: 4.32296991348s
step 5114/400000: train loss: 0.191809
epoch:111, batch10, load frames use: 3.93145489693s
step 5115/400000: train loss: 0.214076
epoch:111, batch11, load frames use: 3.38152885437s
step 5116/400000: train loss: 0.217524
epoch:111, batch12, load frames use: 3.77741789818s
step 5117/400000: train loss: 0.180707
epoch:111, batch13, load frames use: 3.32941317558s
step 5118/400000: train loss: 0.190834
epoch:111, batch14, load frames use: 3.611079216s
step 5119/400000: train loss: 0.196729
epoch:111, batch15, load frames use: 3.57095003128s
step 5120/400000: train loss: 0.210936
epoch:111, batch16, load frames use: 3.63731789589s
step 5121/400000: train loss: 0.221129
epoch:111, batch17, load frames use: 3.3859539032s
step 5122/400000: train loss: 0.214372
epoch:111, batch18, load frames use: 3.58552789688s
step 5123/400000: train loss: 0.193984
epoch:111, batch19, load frames use: 3.27760195732s
step 5124/400000: train loss: 0.188652
epoch:111, batch20, load frames use: 3.22825193405s
step 5125/400000: train loss: 0.177525
epoch:111, batch21, load frames use: 3.79530501366s
step 5126/400000: train loss: 0.193097
epoch:111, batch22, load frames use: 3.31746602058s
step 5127/400000: train loss: 0.209174
epoch:111, batch23, load frames use: 3.68070197105s
step 5128/400000: train loss: 0.173319
epoch:111, batch24, load frames use: 3.34126091003s
step 5129/400000: train loss: 0.195501
epoch:111, batch25, load frames use: 3.2852408886s
step 5130/400000: train loss: 0.185762
epoch:111, batch26, load frames use: 3.47099614143s
step 5131/400000: train loss: 0.176841
epoch:111, batch27, load frames use: 3.35067009926s
step 5132/400000: train loss: 0.202956
epoch:111, batch28, load frames use: 3.31772708893s
step 5133/400000: train loss: 0.197390
epoch:111, batch29, load frames use: 3.4709880352s
step 5134/400000: train loss: 0.189825
epoch:111, batch30, load frames use: 3.84752893448s
step 5135/400000: train loss: 0.228208
epoch:111, batch31, load frames use: 3.2736260891s
step 5136/400000: train loss: 0.210794
epoch:111, batch32, load frames use: 3.3796210289s
step 5137/400000: train loss: 0.189151
epoch:111, batch33, load frames use: 3.31963586807s
step 5138/400000: train loss: 0.184039
epoch:111, batch34, load frames use: 3.41335296631s
step 5139/400000: train loss: 0.190017
epoch:111, batch35, load frames use: 3.38518404961s
step 5140/400000: train loss: 0.197286
epoch:111, batch36, load frames use: 3.44640803337s
step 5141/400000: train loss: 0.199752
epoch:111, batch37, load frames use: 3.38694691658s
step 5142/400000: train loss: 0.179761
epoch:111, batch38, load frames use: 3.7872979641s
step 5143/400000: train loss: 0.212568
epoch:111, batch39, load frames use: 3.76497507095s
step 5144/400000: train loss: 0.189060
epoch:111, batch40, load frames use: 3.4729590416s
step 5145/400000: train loss: 0.209055
epoch:111, batch41, load frames use: 3.33505702019s
step 5146/400000: train loss: 0.191129
epoch:111, batch42, load frames use: 3.40172481537s
step 5147/400000: train loss: 0.202642
epoch:111, batch43, load frames use: 3.41800093651s
step 5148/400000: train loss: 0.188746
epoch:111, batch44, load frames use: 3.5010099411s
step 5149/400000: train loss: 0.198041
epoch:111, batch45, load frames use: 3.71103286743s
step 5150/400000: train loss: 0.196946
epoch:111, batch46, load frames use: 3.30692410469s
step 5151/400000: train loss: 0.210687
epoch:112, batch1, load frames use: 3.75072407722s
step 5152/400000: train loss: 0.188145
epoch:112, batch2, load frames use: 3.74312901497s
step 5153/400000: train loss: 0.192213
epoch:112, batch3, load frames use: 3.7971599102s
step 5154/400000: train loss: 0.187127
epoch:112, batch4, load frames use: 3.33773899078s
step 5155/400000: train loss: 0.172702
epoch:112, batch5, load frames use: 3.68436598778s
step 5156/400000: train loss: 0.185071
epoch:112, batch6, load frames use: 3.59805393219s
step 5157/400000: train loss: 0.177462
epoch:112, batch7, load frames use: 3.3518140316s
step 5158/400000: train loss: 0.158003
epoch:112, batch8, load frames use: 3.37028098106s
step 5159/400000: train loss: 0.180798
epoch:112, batch9, load frames use: 4.34772801399s
step 5160/400000: train loss: 0.209364
epoch:112, batch10, load frames use: 3.80434298515s
step 5161/400000: train loss: 0.193111
epoch:112, batch11, load frames use: 4.34885501862s
step 5162/400000: train loss: 0.167327
epoch:112, batch12, load frames use: 4.59606790543s
step 5163/400000: train loss: 0.187828
epoch:112, batch13, load frames use: 3.3322429657s
step 5164/400000: train loss: 0.185838
epoch:112, batch14, load frames use: 4.78457188606s
step 5165/400000: train loss: 0.183258
epoch:112, batch15, load frames use: 3.88214707375s
step 5166/400000: train loss: 0.204063
epoch:112, batch16, load frames use: 3.51943492889s
step 5167/400000: train loss: 0.222481
epoch:112, batch17, load frames use: 3.26373291016s
step 5168/400000: train loss: 0.176183
epoch:112, batch18, load frames use: 4.82032108307s
step 5169/400000: train loss: 0.178001
epoch:112, batch19, load frames use: 3.7864689827s
step 5170/400000: train loss: 0.195521
epoch:112, batch20, load frames use: 3.77118396759s
step 5171/400000: train loss: 0.192873
epoch:112, batch21, load frames use: 3.86784005165s
step 5172/400000: train loss: 0.203934
epoch:112, batch22, load frames use: 4.5070707798s
step 5173/400000: train loss: 0.196427
epoch:112, batch23, load frames use: 3.46607899666s
step 5174/400000: train loss: 0.183746
epoch:112, batch24, load frames use: 4.49653196335s
step 5175/400000: train loss: 0.204527
epoch:112, batch25, load frames use: 3.2618560791s
step 5176/400000: train loss: 0.184329
epoch:112, batch26, load frames use: 2.80157494545s
step 5177/400000: train loss: 0.188103
epoch:112, batch27, load frames use: 3.28103399277s
step 5178/400000: train loss: 0.204430
epoch:112, batch28, load frames use: 4.17451095581s
step 5179/400000: train loss: 0.195105
epoch:112, batch29, load frames use: 3.98906517029s
step 5180/400000: train loss: 0.200737
epoch:112, batch30, load frames use: 3.33938694s
step 5181/400000: train loss: 0.182034
epoch:112, batch31, load frames use: 3.98696494102s
step 5182/400000: train loss: 0.211333
epoch:112, batch32, load frames use: 4.507117033s
step 5183/400000: train loss: 0.179697
epoch:112, batch33, load frames use: 3.36017203331s
step 5184/400000: train loss: 0.187243
epoch:112, batch34, load frames use: 3.23874783516s
step 5185/400000: train loss: 0.185422
epoch:112, batch35, load frames use: 3.81429696083s
step 5186/400000: train loss: 0.184147
epoch:112, batch36, load frames use: 3.14495205879s
step 5187/400000: train loss: 0.202716
epoch:112, batch37, load frames use: 3.98866701126s
step 5188/400000: train loss: 0.171479
epoch:112, batch38, load frames use: 3.96490192413s
step 5189/400000: train loss: 0.214820
epoch:112, batch39, load frames use: 3.93489098549s
step 5190/400000: train loss: 0.185862
epoch:112, batch40, load frames use: 3.83446383476s
step 5191/400000: train loss: 0.206784
epoch:112, batch41, load frames use: 3.38482809067s
step 5192/400000: train loss: 0.198148
epoch:112, batch42, load frames use: 3.28866100311s
step 5193/400000: train loss: 0.178161
epoch:112, batch43, load frames use: 3.29385113716s
step 5194/400000: train loss: 0.195372
epoch:112, batch44, load frames use: 3.70819497108s
step 5195/400000: train loss: 0.191860
epoch:112, batch45, load frames use: 3.6895108223s
step 5196/400000: train loss: 0.186737
epoch:112, batch46, load frames use: 3.24545001984s
step 5197/400000: train loss: 0.193425
epoch:113, batch1, load frames use: 3.59065294266s
step 5198/400000: train loss: 0.183126
epoch:113, batch2, load frames use: 6.63199710846s
step 5199/400000: train loss: 0.188738
epoch:113, batch3, load frames use: 3.45225214958s
step 5200/400000: train loss: 0.203321
step 5200/400000: validation loss: 0.190643
epoch:113, batch4, load frames use: 4.89632415771s
step 5201/400000: train loss: 0.187814
epoch:113, batch5, load frames use: 3.95368909836s
step 5202/400000: train loss: 0.157027
epoch:113, batch6, load frames use: 3.55526614189s
step 5203/400000: train loss: 0.182766
epoch:113, batch7, load frames use: 3.42234420776s
step 5204/400000: train loss: 0.193819
epoch:113, batch8, load frames use: 3.88046097755s
step 5205/400000: train loss: 0.184163
epoch:113, batch9, load frames use: 3.97737503052s
step 5206/400000: train loss: 0.192717
epoch:113, batch10, load frames use: 3.85554909706s
step 5207/400000: train loss: 0.178987
epoch:113, batch11, load frames use: 4.07388901711s
step 5208/400000: train loss: 0.188733
epoch:113, batch12, load frames use: 4.78934693336s
step 5209/400000: train loss: 0.177801
epoch:113, batch13, load frames use: 4.41781806946s
step 5210/400000: train loss: 0.185132
epoch:113, batch14, load frames use: 3.90729784966s
step 5211/400000: train loss: 0.166486
epoch:113, batch15, load frames use: 3.79222202301s
step 5212/400000: train loss: 0.183797
epoch:113, batch16, load frames use: 4.51331186295s
step 5213/400000: train loss: 0.200861
epoch:113, batch17, load frames use: 3.76667094231s
step 5214/400000: train loss: 0.196830
epoch:113, batch18, load frames use: 3.64825105667s
step 5215/400000: train loss: 0.190199
epoch:113, batch19, load frames use: 3.68453502655s
step 5216/400000: train loss: 0.193875
epoch:113, batch20, load frames use: 3.50136113167s
step 5217/400000: train loss: 0.192350
epoch:113, batch21, load frames use: 3.56730318069s
step 5218/400000: train loss: 0.174095
epoch:113, batch22, load frames use: 3.49542593956s
step 5219/400000: train loss: 0.183194
epoch:113, batch23, load frames use: 3.53861284256s
step 5220/400000: train loss: 0.171225
epoch:113, batch24, load frames use: 3.64299917221s
step 5221/400000: train loss: 0.173524
epoch:113, batch25, load frames use: 3.5755469799s
step 5222/400000: train loss: 0.217719
epoch:113, batch26, load frames use: 3.5588080883s
step 5223/400000: train loss: 0.207212
epoch:113, batch27, load frames use: 3.5955529213s
step 5224/400000: train loss: 0.202380
epoch:113, batch28, load frames use: 3.75410795212s
step 5225/400000: train loss: 0.190620
epoch:113, batch29, load frames use: 3.52085494995s
step 5226/400000: train loss: 0.208057
epoch:113, batch30, load frames use: 3.28764200211s
step 5227/400000: train loss: 0.180594
epoch:113, batch31, load frames use: 3.52061700821s
step 5228/400000: train loss: 0.179140
epoch:113, batch32, load frames use: 4.00742101669s
step 5229/400000: train loss: 0.200049
epoch:113, batch33, load frames use: 3.64637804031s
step 5230/400000: train loss: 0.173291
epoch:113, batch34, load frames use: 3.47811198235s
step 5231/400000: train loss: 0.173947
epoch:113, batch35, load frames use: 3.60916614532s
step 5232/400000: train loss: 0.184917
epoch:113, batch36, load frames use: 3.63166093826s
step 5233/400000: train loss: 0.194278
epoch:113, batch37, load frames use: 3.39911317825s
step 5234/400000: train loss: 0.198526
epoch:113, batch38, load frames use: 3.75361990929s
step 5235/400000: train loss: 0.175328
epoch:113, batch39, load frames use: 3.6334168911s
step 5236/400000: train loss: 0.196278
epoch:113, batch40, load frames use: 3.83698105812s
step 5237/400000: train loss: 0.176100
epoch:113, batch41, load frames use: 3.6630461216s
step 5238/400000: train loss: 0.181772
epoch:113, batch42, load frames use: 3.43015789986s
step 5239/400000: train loss: 0.202934
epoch:113, batch43, load frames use: 3.52943301201s
step 5240/400000: train loss: 0.191756
epoch:113, batch44, load frames use: 3.57862591743s
step 5241/400000: train loss: 0.205749
epoch:113, batch45, load frames use: 3.54673409462s
step 5242/400000: train loss: 0.169637
epoch:113, batch46, load frames use: 4.42339110374s
step 5243/400000: train loss: 0.193780
epoch:114, batch1, load frames use: 3.44585800171s
step 5244/400000: train loss: 0.176203
epoch:114, batch2, load frames use: 3.37023305893s
step 5245/400000: train loss: 0.177560
epoch:114, batch3, load frames use: 3.62392306328s
step 5246/400000: train loss: 0.179910
epoch:114, batch4, load frames use: 3.32707905769s
step 5247/400000: train loss: 0.174645
epoch:114, batch5, load frames use: 4.11645412445s
step 5248/400000: train loss: 0.173428
epoch:114, batch6, load frames use: 3.28667092323s
step 5249/400000: train loss: 0.178672
epoch:114, batch7, load frames use: 3.87356591225s
step 5250/400000: train loss: 0.195520
epoch:114, batch8, load frames use: 3.39054298401s
step 5251/400000: train loss: 0.182974
epoch:114, batch9, load frames use: 3.91293501854s
step 5252/400000: train loss: 0.190835
epoch:114, batch10, load frames use: 3.73732113838s
step 5253/400000: train loss: 0.167340
epoch:114, batch11, load frames use: 3.66669392586s
step 5254/400000: train loss: 0.189281
epoch:114, batch12, load frames use: 3.59549283981s
step 5255/400000: train loss: 0.220570
epoch:114, batch13, load frames use: 3.44871401787s
step 5256/400000: train loss: 0.185482
epoch:114, batch14, load frames use: 3.58433294296s
step 5257/400000: train loss: 0.192819
epoch:114, batch15, load frames use: 3.78642296791s
step 5258/400000: train loss: 0.176790
epoch:114, batch16, load frames use: 3.4813439846s
step 5259/400000: train loss: 0.184794
epoch:114, batch17, load frames use: 3.55054998398s
step 5260/400000: train loss: 0.182901
epoch:114, batch18, load frames use: 3.26656103134s
step 5261/400000: train loss: 0.179874
epoch:114, batch19, load frames use: 2.93600201607s
step 5262/400000: train loss: 0.196831
epoch:114, batch20, load frames use: 7.29288601875s
step 5263/400000: train loss: 0.183995
epoch:114, batch21, load frames use: 3.59318208694s
step 5264/400000: train loss: 0.190146
epoch:114, batch22, load frames use: 3.50277113914s
step 5265/400000: train loss: 0.192873
epoch:114, batch23, load frames use: 3.83835601807s
step 5266/400000: train loss: 0.179958
epoch:114, batch24, load frames use: 4.21824789047s
step 5267/400000: train loss: 0.178396
epoch:114, batch25, load frames use: 3.57103896141s
step 5268/400000: train loss: 0.194255
epoch:114, batch26, load frames use: 3.89896702766s
step 5269/400000: train loss: 0.190306
epoch:114, batch27, load frames use: 4.51472592354s
step 5270/400000: train loss: 0.212351
epoch:114, batch28, load frames use: 4.40020895004s
step 5271/400000: train loss: 0.182639
epoch:114, batch29, load frames use: 3.38459300995s
step 5272/400000: train loss: 0.193951
epoch:114, batch30, load frames use: 5.09547591209s
step 5273/400000: train loss: 0.207255
epoch:114, batch31, load frames use: 3.74149489403s
step 5274/400000: train loss: 0.199063
epoch:114, batch32, load frames use: 3.89886689186s
step 5275/400000: train loss: 0.187860
epoch:114, batch33, load frames use: 4.02589893341s
step 5276/400000: train loss: 0.197165
epoch:114, batch34, load frames use: 3.55249595642s
step 5277/400000: train loss: 0.205074
epoch:114, batch35, load frames use: 3.31349396706s
step 5278/400000: train loss: 0.229411
epoch:114, batch36, load frames use: 4.35905098915s
step 5279/400000: train loss: 0.187005
epoch:114, batch37, load frames use: 4.28613090515s
step 5280/400000: train loss: 0.195974
epoch:114, batch38, load frames use: 3.72291517258s
step 5281/400000: train loss: 0.184413
epoch:114, batch39, load frames use: 4.26217484474s
step 5282/400000: train loss: 0.178200
epoch:114, batch40, load frames use: 5.15136599541s
step 5283/400000: train loss: 0.182164
epoch:114, batch41, load frames use: 3.4248239994s
step 5284/400000: train loss: 0.175804
epoch:114, batch42, load frames use: 3.36783099174s
step 5285/400000: train loss: 0.206623
epoch:114, batch43, load frames use: 4.44729399681s
step 5286/400000: train loss: 0.190655
epoch:114, batch44, load frames use: 3.65856790543s
step 5287/400000: train loss: 0.214632
epoch:114, batch45, load frames use: 3.7639272213s
step 5288/400000: train loss: 0.209298
epoch:114, batch46, load frames use: 5.89961791039s
step 5289/400000: train loss: 0.203231
epoch:115, batch1, load frames use: 3.25515413284s
step 5290/400000: train loss: 0.191443
epoch:115, batch2, load frames use: 3.86684894562s
step 5291/400000: train loss: 0.177527
epoch:115, batch3, load frames use: 3.79829001427s
step 5292/400000: train loss: 0.167689
epoch:115, batch4, load frames use: 3.36501002312s
step 5293/400000: train loss: 0.180905
epoch:115, batch5, load frames use: 3.44827103615s
step 5294/400000: train loss: 0.177068
epoch:115, batch6, load frames use: 4.31809091568s
step 5295/400000: train loss: 0.191272
epoch:115, batch7, load frames use: 3.58393287659s
step 5296/400000: train loss: 0.190280
epoch:115, batch8, load frames use: 6.3680961132s
step 5297/400000: train loss: 0.198095
epoch:115, batch9, load frames use: 4.57849001884s
step 5298/400000: train loss: 0.188858
epoch:115, batch10, load frames use: 3.42878818512s
step 5299/400000: train loss: 0.172171
epoch:115, batch11, load frames use: 3.62108778954s
step 5300/400000: train loss: 0.193273
step 5300/400000: validation loss: 0.173731
epoch:115, batch12, load frames use: 4.55752491951s
step 5301/400000: train loss: 0.177764
epoch:115, batch13, load frames use: 3.50501084328s
step 5302/400000: train loss: 0.179315
epoch:115, batch14, load frames use: 3.73433685303s
step 5303/400000: train loss: 0.207379
epoch:115, batch15, load frames use: 4.85913896561s
step 5304/400000: train loss: 0.183297
epoch:115, batch16, load frames use: 3.94390892982s
step 5305/400000: train loss: 0.167814
epoch:115, batch17, load frames use: 5.03195786476s
step 5306/400000: train loss: 0.201080
epoch:115, batch18, load frames use: 3.36053490639s
step 5307/400000: train loss: 0.167063
epoch:115, batch19, load frames use: 5.00985217094s
step 5308/400000: train loss: 0.176592
epoch:115, batch20, load frames use: 3.73771190643s
step 5309/400000: train loss: 0.195788
epoch:115, batch21, load frames use: 3.40072607994s
step 5310/400000: train loss: 0.181523
epoch:115, batch22, load frames use: 3.26694297791s
step 5311/400000: train loss: 0.191364
epoch:115, batch23, load frames use: 3.2436671257s
step 5312/400000: train loss: 0.191938
epoch:115, batch24, load frames use: 4.05503511429s
step 5313/400000: train loss: 0.165357
epoch:115, batch25, load frames use: 3.52178502083s
step 5314/400000: train loss: 0.191761
epoch:115, batch26, load frames use: 3.36947703362s
step 5315/400000: train loss: 0.207117
epoch:115, batch27, load frames use: 4.53039312363s
step 5316/400000: train loss: 0.204805
epoch:115, batch28, load frames use: 3.59985589981s
step 5317/400000: train loss: 0.180349
epoch:115, batch29, load frames use: 4.45848107338s
step 5318/400000: train loss: 0.185730
epoch:115, batch30, load frames use: 3.77130794525s
step 5319/400000: train loss: 0.188780
epoch:115, batch31, load frames use: 4.44756603241s
step 5320/400000: train loss: 0.191845
epoch:115, batch32, load frames use: 4.32014012337s
step 5321/400000: train loss: 0.203710
epoch:115, batch33, load frames use: 3.86621117592s
step 5322/400000: train loss: 0.180999
epoch:115, batch34, load frames use: 3.24831819534s
step 5323/400000: train loss: 0.187706
epoch:115, batch35, load frames use: 3.93738293648s
step 5324/400000: train loss: 0.177744
epoch:115, batch36, load frames use: 3.46205687523s
step 5325/400000: train loss: 0.178087
epoch:115, batch37, load frames use: 3.40692520142s
step 5326/400000: train loss: 0.189970
epoch:115, batch38, load frames use: 2.96029996872s
step 5327/400000: train loss: 0.209468
epoch:115, batch39, load frames use: 3.25951886177s
step 5328/400000: train loss: 0.200565
epoch:115, batch40, load frames use: 3.39932179451s
step 5329/400000: train loss: 0.192200
epoch:115, batch41, load frames use: 5.38857483864s
step 5330/400000: train loss: 0.195102
epoch:115, batch42, load frames use: 5.95887303352s
step 5331/400000: train loss: 0.220834
epoch:115, batch43, load frames use: 4.29813218117s
step 5332/400000: train loss: 0.182758
epoch:115, batch44, load frames use: 4.25914716721s
step 5333/400000: train loss: 0.172262
epoch:115, batch45, load frames use: 4.04999899864s
step 5334/400000: train loss: 0.190014
epoch:115, batch46, load frames use: 5.08473396301s
step 5335/400000: train loss: 0.181338
epoch:116, batch1, load frames use: 3.95859289169s
step 5336/400000: train loss: 0.186976
epoch:116, batch2, load frames use: 3.4061729908s
step 5337/400000: train loss: 0.218704
epoch:116, batch3, load frames use: 4.71934008598s
step 5338/400000: train loss: 0.173489
epoch:116, batch4, load frames use: 4.12146902084s
step 5339/400000: train loss: 0.189233
epoch:116, batch5, load frames use: 3.36045098305s
step 5340/400000: train loss: 0.183118
epoch:116, batch6, load frames use: 3.12687301636s
step 5341/400000: train loss: 0.174189
epoch:116, batch7, load frames use: 3.55113005638s
step 5342/400000: train loss: 0.176390
epoch:116, batch8, load frames use: 3.14270687103s
step 5343/400000: train loss: 0.174852
epoch:116, batch9, load frames use: 3.51546382904s
step 5344/400000: train loss: 0.188731
epoch:116, batch10, load frames use: 5.34931087494s
step 5345/400000: train loss: 0.183380
epoch:116, batch11, load frames use: 3.28699493408s
step 5346/400000: train loss: 0.167983
epoch:116, batch12, load frames use: 3.73951792717s
step 5347/400000: train loss: 0.188018
epoch:116, batch13, load frames use: 3.81779098511s
step 5348/400000: train loss: 0.215346
epoch:116, batch14, load frames use: 3.53401994705s
step 5349/400000: train loss: 0.170477
epoch:116, batch15, load frames use: 3.74787902832s
step 5350/400000: train loss: 0.197974
epoch:116, batch16, load frames use: 3.61062908173s
step 5351/400000: train loss: 0.183209
epoch:116, batch17, load frames use: 3.37054896355s
step 5352/400000: train loss: 0.181556
epoch:116, batch18, load frames use: 3.5209479332s
step 5353/400000: train loss: 0.191904
epoch:116, batch19, load frames use: 3.20850896835s
step 5354/400000: train loss: 0.169674
epoch:116, batch20, load frames use: 3.37201786041s
step 5355/400000: train loss: 0.190754
epoch:116, batch21, load frames use: 3.4722931385s
step 5356/400000: train loss: 0.178644
epoch:116, batch22, load frames use: 3.67985296249s
step 5357/400000: train loss: 0.182556
epoch:116, batch23, load frames use: 3.34358215332s
step 5358/400000: train loss: 0.184688
epoch:116, batch24, load frames use: 3.32250094414s
step 5359/400000: train loss: 0.184284
epoch:116, batch25, load frames use: 3.89541292191s
step 5360/400000: train loss: 0.188978
epoch:116, batch26, load frames use: 4.54970812798s
step 5361/400000: train loss: 0.214279
epoch:116, batch27, load frames use: 4.08220982552s
step 5362/400000: train loss: 0.180917
epoch:116, batch28, load frames use: 4.0625s
step 5363/400000: train loss: 0.182743
epoch:116, batch29, load frames use: 3.54414200783s
step 5364/400000: train loss: 0.170895
epoch:116, batch30, load frames use: 4.09170603752s
step 5365/400000: train loss: 0.182039
epoch:116, batch31, load frames use: 4.89451694489s
step 5366/400000: train loss: 0.190342
epoch:116, batch32, load frames use: 3.58490204811s
step 5367/400000: train loss: 0.175535
epoch:116, batch33, load frames use: 3.39307403564s
step 5368/400000: train loss: 0.173423
epoch:116, batch34, load frames use: 3.8537838459s
step 5369/400000: train loss: 0.193533
epoch:116, batch35, load frames use: 3.86204099655s
step 5370/400000: train loss: 0.194632
epoch:116, batch36, load frames use: 4.03062486649s
step 5371/400000: train loss: 0.187589
epoch:116, batch37, load frames use: 3.66287899017s
step 5372/400000: train loss: 0.179797
epoch:116, batch38, load frames use: 3.38016486168s
step 5373/400000: train loss: 0.223439
epoch:116, batch39, load frames use: 3.77896809578s
step 5374/400000: train loss: 0.178826
epoch:116, batch40, load frames use: 3.28542995453s
step 5375/400000: train loss: 0.195568
epoch:116, batch41, load frames use: 3.10010910034s
step 5376/400000: train loss: 0.184084
epoch:116, batch42, load frames use: 3.98015809059s
step 5377/400000: train loss: 0.194253
epoch:116, batch43, load frames use: 3.52915716171s
step 5378/400000: train loss: 0.206752
epoch:116, batch44, load frames use: 3.43733501434s
step 5379/400000: train loss: 0.186423
epoch:116, batch45, load frames use: 3.50758099556s
step 5380/400000: train loss: 0.178678
epoch:116, batch46, load frames use: 4.15657997131s
step 5381/400000: train loss: 0.205209
epoch:117, batch1, load frames use: 3.52207517624s
step 5382/400000: train loss: 0.177327
epoch:117, batch2, load frames use: 3.54383111s
step 5383/400000: train loss: 0.167574
epoch:117, batch3, load frames use: 3.56499195099s
step 5384/400000: train loss: 0.200570
epoch:117, batch4, load frames use: 3.45795798302s
step 5385/400000: train loss: 0.196629
epoch:117, batch5, load frames use: 4.62447500229s
step 5386/400000: train loss: 0.183312
epoch:117, batch6, load frames use: 3.66016697884s
step 5387/400000: train loss: 0.166850
epoch:117, batch7, load frames use: 5.6254529953s
step 5388/400000: train loss: 0.174473
epoch:117, batch8, load frames use: 3.15755987167s
step 5389/400000: train loss: 0.177692
epoch:117, batch9, load frames use: 4.12718200684s
step 5390/400000: train loss: 0.181156
epoch:117, batch10, load frames use: 3.99182200432s
step 5391/400000: train loss: 0.181365
epoch:117, batch11, load frames use: 3.95382094383s
step 5392/400000: train loss: 0.210919
epoch:117, batch12, load frames use: 3.54891395569s
step 5393/400000: train loss: 0.176261
epoch:117, batch13, load frames use: 3.63097310066s
step 5394/400000: train loss: 0.174875
epoch:117, batch14, load frames use: 3.29917907715s
step 5395/400000: train loss: 0.216153
epoch:117, batch15, load frames use: 4.23783183098s
step 5396/400000: train loss: 0.163171
epoch:117, batch16, load frames use: 3.93851900101s
step 5397/400000: train loss: 0.165622
epoch:117, batch17, load frames use: 3.25152397156s
step 5398/400000: train loss: 0.182950
epoch:117, batch18, load frames use: 3.23325586319s
step 5399/400000: train loss: 0.209047
epoch:117, batch19, load frames use: 3.70055198669s
step 5400/400000: train loss: 0.184846
step 5400/400000: validation loss: 0.172734
epoch:117, batch20, load frames use: 4.26201701164s
step 5401/400000: train loss: 0.194399
epoch:117, batch21, load frames use: 3.66566300392s
step 5402/400000: train loss: 0.192575
epoch:117, batch22, load frames use: 3.89143204689s
step 5403/400000: train loss: 0.178570
epoch:117, batch23, load frames use: 4.38859701157s
step 5404/400000: train loss: 0.173454
epoch:117, batch24, load frames use: 3.83800911903s
step 5405/400000: train loss: 0.205106
epoch:117, batch25, load frames use: 3.88749980927s
step 5406/400000: train loss: 0.193525
epoch:117, batch26, load frames use: 3.31207299232s
step 5407/400000: train loss: 0.187930
epoch:117, batch27, load frames use: 3.64791703224s
step 5408/400000: train loss: 0.172089
epoch:117, batch28, load frames use: 3.56400299072s
step 5409/400000: train loss: 0.193663
epoch:117, batch29, load frames use: 3.44845604897s
step 5410/400000: train loss: 0.175243
epoch:117, batch30, load frames use: 3.28541111946s
step 5411/400000: train loss: 0.182508
epoch:117, batch31, load frames use: 3.2684109211s
step 5412/400000: train loss: 0.166335
epoch:117, batch32, load frames use: 3.25013995171s
step 5413/400000: train loss: 0.190888
epoch:117, batch33, load frames use: 3.44628000259s
step 5414/400000: train loss: 0.181982
epoch:117, batch34, load frames use: 3.90646505356s
step 5415/400000: train loss: 0.200980
epoch:117, batch35, load frames use: 4.13735389709s
step 5416/400000: train loss: 0.181924
epoch:117, batch36, load frames use: 3.37190294266s
step 5417/400000: train loss: 0.184617
epoch:117, batch37, load frames use: 5.0994849205s
step 5418/400000: train loss: 0.201097
epoch:117, batch38, load frames use: 3.97011089325s
step 5419/400000: train loss: 0.191379
epoch:117, batch39, load frames use: 4.26887512207s
step 5420/400000: train loss: 0.208906
epoch:117, batch40, load frames use: 4.48916888237s
step 5421/400000: train loss: 0.213707
epoch:117, batch41, load frames use: 3.77853298187s
step 5422/400000: train loss: 0.195218
epoch:117, batch42, load frames use: 3.27588200569s
step 5423/400000: train loss: 0.195634
epoch:117, batch43, load frames use: 4.36097192764s
step 5424/400000: train loss: 0.189143
epoch:117, batch44, load frames use: 3.48122000694s
step 5425/400000: train loss: 0.181864
epoch:117, batch45, load frames use: 3.26143598557s
step 5426/400000: train loss: 0.191797
epoch:117, batch46, load frames use: 3.08327102661s
step 5427/400000: train loss: 0.187356
epoch:118, batch1, load frames use: 3.6246380806s
step 5428/400000: train loss: 0.186687
epoch:118, batch2, load frames use: 4.29048299789s
step 5429/400000: train loss: 0.200695
epoch:118, batch3, load frames use: 3.95084404945s
step 5430/400000: train loss: 0.195011
epoch:118, batch4, load frames use: 3.48073506355s
step 5431/400000: train loss: 0.187186
epoch:118, batch5, load frames use: 3.81839108467s
step 5432/400000: train loss: 0.175431
epoch:118, batch6, load frames use: 3.72605013847s
step 5433/400000: train loss: 0.160123
epoch:118, batch7, load frames use: 5.90660214424s
step 5434/400000: train loss: 0.183870
epoch:118, batch8, load frames use: 3.90788197517s
step 5435/400000: train loss: 0.187107
epoch:118, batch9, load frames use: 4.44602489471s
step 5436/400000: train loss: 0.192591
epoch:118, batch10, load frames use: 3.56075596809s
step 5437/400000: train loss: 0.173053
epoch:118, batch11, load frames use: 3.59521102905s
step 5438/400000: train loss: 0.196507
epoch:118, batch12, load frames use: 3.33310389519s
step 5439/400000: train loss: 0.186846
epoch:118, batch13, load frames use: 3.56960105896s
step 5440/400000: train loss: 0.204177
epoch:118, batch14, load frames use: 3.21577215195s
step 5441/400000: train loss: 0.178840
epoch:118, batch15, load frames use: 3.88782787323s
step 5442/400000: train loss: 0.183337
epoch:118, batch16, load frames use: 3.43168306351s
step 5443/400000: train loss: 0.198175
epoch:118, batch17, load frames use: 3.56474995613s
step 5444/400000: train loss: 0.193848
epoch:118, batch18, load frames use: 5.01388907433s
step 5445/400000: train loss: 0.192421
epoch:118, batch19, load frames use: 3.5951128006s
step 5446/400000: train loss: 0.180574
epoch:118, batch20, load frames use: 5.56780791283s
step 5447/400000: train loss: 0.169778
epoch:118, batch21, load frames use: 3.4902639389s
step 5448/400000: train loss: 0.178459
epoch:118, batch22, load frames use: 3.46982502937s
step 5449/400000: train loss: 0.187506
epoch:118, batch23, load frames use: 3.2631509304s
step 5450/400000: train loss: 0.171429
epoch:118, batch24, load frames use: 4.37105917931s
step 5451/400000: train loss: 0.175291
epoch:118, batch25, load frames use: 5.52074790001s
step 5452/400000: train loss: 0.183972
epoch:118, batch26, load frames use: 4.18590903282s
step 5453/400000: train loss: 0.208943
epoch:118, batch27, load frames use: 3.66864109039s
step 5454/400000: train loss: 0.188803
epoch:118, batch28, load frames use: 3.21644997597s
step 5455/400000: train loss: 0.214493
epoch:118, batch29, load frames use: 3.53494215012s
step 5456/400000: train loss: 0.181410
epoch:118, batch30, load frames use: 5.17201709747s
step 5457/400000: train loss: 0.206029
epoch:118, batch31, load frames use: 5.24740386009s
step 5458/400000: train loss: 0.185600
epoch:118, batch32, load frames use: 4.76028013229s
step 5459/400000: train loss: 0.183059
epoch:118, batch33, load frames use: 4.14558410645s
step 5460/400000: train loss: 0.205373
epoch:118, batch34, load frames use: 3.4654250145s
step 5461/400000: train loss: 0.194275
epoch:118, batch35, load frames use: 4.19656085968s
step 5462/400000: train loss: 0.219121
epoch:118, batch36, load frames use: 3.28909015656s
step 5463/400000: train loss: 0.198006
epoch:118, batch37, load frames use: 4.35643482208s
step 5464/400000: train loss: 0.213088
epoch:118, batch38, load frames use: 3.67397880554s
step 5465/400000: train loss: 0.191150
epoch:118, batch39, load frames use: 4.51903986931s
step 5466/400000: train loss: 0.198227
epoch:118, batch40, load frames use: 5.73080301285s
step 5467/400000: train loss: 0.210660
epoch:118, batch41, load frames use: 4.67260503769s
step 5468/400000: train loss: 0.199491
epoch:118, batch42, load frames use: 3.47434282303s
step 5469/400000: train loss: 0.180049
epoch:118, batch43, load frames use: 3.42411899567s
step 5470/400000: train loss: 0.193076
epoch:118, batch44, load frames use: 3.43342709541s
step 5471/400000: train loss: 0.190491
epoch:118, batch45, load frames use: 3.58347201347s
step 5472/400000: train loss: 0.188769
epoch:118, batch46, load frames use: 3.35911393166s
step 5473/400000: train loss: 0.197926
epoch:119, batch1, load frames use: 3.03840994835s
step 5474/400000: train loss: 0.200208
epoch:119, batch2, load frames use: 5.43221187592s
step 5475/400000: train loss: 0.204067
epoch:119, batch3, load frames use: 3.44047999382s
step 5476/400000: train loss: 0.174445
epoch:119, batch4, load frames use: 3.18169808388s
step 5477/400000: train loss: 0.222880
epoch:119, batch5, load frames use: 4.49061203003s
step 5478/400000: train loss: 0.172611
epoch:119, batch6, load frames use: 3.3813290596s
step 5479/400000: train loss: 0.180046
epoch:119, batch7, load frames use: 3.40082406998s
step 5480/400000: train loss: 0.166470
epoch:119, batch8, load frames use: 3.83990716934s
step 5481/400000: train loss: 0.208436
epoch:119, batch9, load frames use: 3.33425402641s
step 5482/400000: train loss: 0.180643
epoch:119, batch10, load frames use: 3.2735619545s
step 5483/400000: train loss: 0.182464
epoch:119, batch11, load frames use: 5.61239600182s
step 5484/400000: train loss: 0.179571
epoch:119, batch12, load frames use: 5.22198009491s
step 5485/400000: train loss: 0.176114
epoch:119, batch13, load frames use: 3.59610295296s
step 5486/400000: train loss: 0.174493
epoch:119, batch14, load frames use: 3.81177783012s
step 5487/400000: train loss: 0.195115
epoch:119, batch15, load frames use: 3.82946109772s
step 5488/400000: train loss: 0.190868
epoch:119, batch16, load frames use: 3.37018895149s
step 5489/400000: train loss: 0.172218
epoch:119, batch17, load frames use: 4.20845508575s
step 5490/400000: train loss: 0.207274
epoch:119, batch18, load frames use: 3.31376600266s
step 5491/400000: train loss: 0.206450
epoch:119, batch19, load frames use: 3.61831688881s
step 5492/400000: train loss: 0.204946
epoch:119, batch20, load frames use: 3.42725396156s
step 5493/400000: train loss: 0.207972
epoch:119, batch21, load frames use: 4.88180494308s
step 5494/400000: train loss: 0.190282
epoch:119, batch22, load frames use: 4.14225006104s
step 5495/400000: train loss: 0.196513
epoch:119, batch23, load frames use: 3.73357009888s
step 5496/400000: train loss: 0.193679
epoch:119, batch24, load frames use: 4.17281103134s
step 5497/400000: train loss: 0.176756
epoch:119, batch25, load frames use: 3.38569307327s
step 5498/400000: train loss: 0.181861
epoch:119, batch26, load frames use: 4.63863992691s
step 5499/400000: train loss: 0.189919
epoch:119, batch27, load frames use: 5.33157587051s
step 5500/400000: train loss: 0.188731
step 5500/400000: validation loss: 0.187351
epoch:119, batch28, load frames use: 5.12512588501s
step 5501/400000: train loss: 0.222203
epoch:119, batch29, load frames use: 4.0259680748s
step 5502/400000: train loss: 0.198185
epoch:119, batch30, load frames use: 3.86951208115s
step 5503/400000: train loss: 0.196858
epoch:119, batch31, load frames use: 5.66094899178s
step 5504/400000: train loss: 0.194208
epoch:119, batch32, load frames use: 5.68516588211s
step 5505/400000: train loss: 0.188014
epoch:119, batch33, load frames use: 3.29541516304s
step 5506/400000: train loss: 0.185696
epoch:119, batch34, load frames use: 4.0850610733s
step 5507/400000: train loss: 0.176260
epoch:119, batch35, load frames use: 3.33950901031s
step 5508/400000: train loss: 0.184860
epoch:119, batch36, load frames use: 4.28063201904s
step 5509/400000: train loss: 0.175600
epoch:119, batch37, load frames use: 3.28242588043s
step 5510/400000: train loss: 0.186249
epoch:119, batch38, load frames use: 3.58876895905s
step 5511/400000: train loss: 0.181359
epoch:119, batch39, load frames use: 3.41765284538s
step 5512/400000: train loss: 0.191104
epoch:119, batch40, load frames use: 4.24908804893s
step 5513/400000: train loss: 0.185921
epoch:119, batch41, load frames use: 3.12488603592s
step 5514/400000: train loss: 0.196883
epoch:119, batch42, load frames use: 3.47652602196s
step 5515/400000: train loss: 0.188459
epoch:119, batch43, load frames use: 3.91866087914s
step 5516/400000: train loss: 0.201784
epoch:119, batch44, load frames use: 3.23483896255s
step 5517/400000: train loss: 0.187712
epoch:119, batch45, load frames use: 5.0331799984s
step 5518/400000: train loss: 0.206717
epoch:119, batch46, load frames use: 5.33562016487s
step 5519/400000: train loss: 0.197822
epoch:120, batch1, load frames use: 5.28069210052s
step 5520/400000: train loss: 0.189497
epoch:120, batch2, load frames use: 3.59529209137s
step 5521/400000: train loss: 0.167456
epoch:120, batch3, load frames use: 4.71451997757s
step 5522/400000: train loss: 0.171084
epoch:120, batch4, load frames use: 3.4972140789s
step 5523/400000: train loss: 0.211119
epoch:120, batch5, load frames use: 3.6136341095s
step 5524/400000: train loss: 0.205276
epoch:120, batch6, load frames use: 3.43008708954s
step 5525/400000: train loss: 0.181214
epoch:120, batch7, load frames use: 3.71947312355s
step 5526/400000: train loss: 0.165640
epoch:120, batch8, load frames use: 3.36927199364s
step 5527/400000: train loss: 0.177733
epoch:120, batch9, load frames use: 5.58611798286s
step 5528/400000: train loss: 0.194264
epoch:120, batch10, load frames use: 3.96566319466s
step 5529/400000: train loss: 0.188235
epoch:120, batch11, load frames use: 3.33659100533s
step 5530/400000: train loss: 0.192357
epoch:120, batch12, load frames use: 3.83367085457s
step 5531/400000: train loss: 0.183958
epoch:120, batch13, load frames use: 4.53519797325s
step 5532/400000: train loss: 0.175326
epoch:120, batch14, load frames use: 3.52073597908s
step 5533/400000: train loss: 0.188965
epoch:120, batch15, load frames use: 3.77901411057s
step 5534/400000: train loss: 0.202495
epoch:120, batch16, load frames use: 4.95974302292s
step 5535/400000: train loss: 0.196800
epoch:120, batch17, load frames use: 3.50584602356s
step 5536/400000: train loss: 0.187783
epoch:120, batch18, load frames use: 4.68944501877s
step 5537/400000: train loss: 0.192289
epoch:120, batch19, load frames use: 4.09476113319s
step 5538/400000: train loss: 0.195916
epoch:120, batch20, load frames use: 5.61208987236s
step 5539/400000: train loss: 0.209358
epoch:120, batch21, load frames use: 4.08961606026s
step 5540/400000: train loss: 0.218298
epoch:120, batch22, load frames use: 3.73056602478s
step 5541/400000: train loss: 0.181917
epoch:120, batch23, load frames use: 5.39544916153s
step 5542/400000: train loss: 0.177794
epoch:120, batch24, load frames use: 3.93664193153s
step 5543/400000: train loss: 0.173911
epoch:120, batch25, load frames use: 5.12337398529s
step 5544/400000: train loss: 0.188091
epoch:120, batch26, load frames use: 3.60038614273s
step 5545/400000: train loss: 0.178937
epoch:120, batch27, load frames use: 3.93522000313s
step 5546/400000: train loss: 0.174806
epoch:120, batch28, load frames use: 4.36016607285s
step 5547/400000: train loss: 0.200134
epoch:120, batch29, load frames use: 3.46093010902s
step 5548/400000: train loss: 0.189871
epoch:120, batch30, load frames use: 4.29166793823s
step 5549/400000: train loss: 0.207157
epoch:120, batch31, load frames use: 3.44288992882s
step 5550/400000: train loss: 0.177882
epoch:120, batch32, load frames use: 3.73054003716s
step 5551/400000: train loss: 0.186149
epoch:120, batch33, load frames use: 3.62657594681s
step 5552/400000: train loss: 0.193034
epoch:120, batch34, load frames use: 4.05259799957s
step 5553/400000: train loss: 0.189169
epoch:120, batch35, load frames use: 3.28663492203s
step 5554/400000: train loss: 0.199455
epoch:120, batch36, load frames use: 3.32733988762s
step 5555/400000: train loss: 0.194801
epoch:120, batch37, load frames use: 3.34179520607s
step 5556/400000: train loss: 0.190620
epoch:120, batch38, load frames use: 3.8295841217s
step 5557/400000: train loss: 0.199553
epoch:120, batch39, load frames use: 3.4504802227s
step 5558/400000: train loss: 0.187829
epoch:120, batch40, load frames use: 4.23835396767s
step 5559/400000: train loss: 0.201499
epoch:120, batch41, load frames use: 6.79908800125s
step 5560/400000: train loss: 0.200162
epoch:120, batch42, load frames use: 3.65117096901s
step 5561/400000: train loss: 0.215595
epoch:120, batch43, load frames use: 3.73601102829s
step 5562/400000: train loss: 0.185631
epoch:120, batch44, load frames use: 3.39337491989s
step 5563/400000: train loss: 0.216656
epoch:120, batch45, load frames use: 3.51486206055s
step 5564/400000: train loss: 0.179672
epoch:120, batch46, load frames use: 3.30903792381s
step 5565/400000: train loss: 0.201862
epoch:121, batch1, load frames use: 3.9076859951s
step 5566/400000: train loss: 0.171679
epoch:121, batch2, load frames use: 3.67363381386s
step 5567/400000: train loss: 0.197680
epoch:121, batch3, load frames use: 3.31298899651s
step 5568/400000: train loss: 0.200718
epoch:121, batch4, load frames use: 3.85765504837s
step 5569/400000: train loss: 0.186901
epoch:121, batch5, load frames use: 4.47200608253s
step 5570/400000: train loss: 0.186245
epoch:121, batch6, load frames use: 3.23320102692s
step 5571/400000: train loss: 0.192750
epoch:121, batch7, load frames use: 3.25182890892s
step 5572/400000: train loss: 0.176078
epoch:121, batch8, load frames use: 3.74666213989s
step 5573/400000: train loss: 0.196733
epoch:121, batch9, load frames use: 3.46407294273s
step 5574/400000: train loss: 0.177806
epoch:121, batch10, load frames use: 3.67502498627s
step 5575/400000: train loss: 0.160615
epoch:121, batch11, load frames use: 4.05456495285s
step 5576/400000: train loss: 0.182217
epoch:121, batch12, load frames use: 5.04507708549s
step 5577/400000: train loss: 0.204164
epoch:121, batch13, load frames use: 3.38579487801s
step 5578/400000: train loss: 0.203905
epoch:121, batch14, load frames use: 3.99473595619s
step 5579/400000: train loss: 0.193347
epoch:121, batch15, load frames use: 3.85999011993s
step 5580/400000: train loss: 0.200691
epoch:121, batch16, load frames use: 5.75348091125s
step 5581/400000: train loss: 0.169651
epoch:121, batch17, load frames use: 3.88735985756s
step 5582/400000: train loss: 0.191807
epoch:121, batch18, load frames use: 4.12865900993s
step 5583/400000: train loss: 0.184487
epoch:121, batch19, load frames use: 3.44775509834s
step 5584/400000: train loss: 0.192372
epoch:121, batch20, load frames use: 4.80824780464s
step 5585/400000: train loss: 0.186847
epoch:121, batch21, load frames use: 3.79547190666s
step 5586/400000: train loss: 0.194424
epoch:121, batch22, load frames use: 3.34076189995s
step 5587/400000: train loss: 0.171558
epoch:121, batch23, load frames use: 3.5825779438s
step 5588/400000: train loss: 0.190946
epoch:121, batch24, load frames use: 3.25739598274s
step 5589/400000: train loss: 0.214749
epoch:121, batch25, load frames use: 3.99252986908s
step 5590/400000: train loss: 0.194438
epoch:121, batch26, load frames use: 3.98059916496s
step 5591/400000: train loss: 0.200976
epoch:121, batch27, load frames use: 3.53862309456s
step 5592/400000: train loss: 0.192811
epoch:121, batch28, load frames use: 3.56739902496s
step 5593/400000: train loss: 0.187975
epoch:121, batch29, load frames use: 6.26523113251s
step 5594/400000: train loss: 0.198777
epoch:121, batch30, load frames use: 3.42158794403s
step 5595/400000: train loss: 0.163551
epoch:121, batch31, load frames use: 4.07868814468s
step 5596/400000: train loss: 0.183669
epoch:121, batch32, load frames use: 3.37246394157s
step 5597/400000: train loss: 0.217773
epoch:121, batch33, load frames use: 3.89428687096s
step 5598/400000: train loss: 0.183094
epoch:121, batch34, load frames use: 4.47316408157s
step 5599/400000: train loss: 0.187719
epoch:121, batch35, load frames use: 4.39793610573s
step 5600/400000: train loss: 0.178770
step 5600/400000: validation loss: 0.176286
epoch:121, batch36, load frames use: 3.7906730175s
step 5601/400000: train loss: 0.200551
epoch:121, batch37, load frames use: 4.01305699348s
step 5602/400000: train loss: 0.184858
epoch:121, batch38, load frames use: 3.97652888298s
step 5603/400000: train loss: 0.201623
epoch:121, batch39, load frames use: 3.99238801003s
step 5604/400000: train loss: 0.200884
epoch:121, batch40, load frames use: 3.09907388687s
step 5605/400000: train loss: 0.188770
epoch:121, batch41, load frames use: 3.42137813568s
step 5606/400000: train loss: 0.177523
epoch:121, batch42, load frames use: 3.7758910656s
step 5607/400000: train loss: 0.174123
epoch:121, batch43, load frames use: 3.3285779953s
step 5608/400000: train loss: 0.191546
epoch:121, batch44, load frames use: 4.00109195709s
step 5609/400000: train loss: 0.187780
epoch:121, batch45, load frames use: 3.33689117432s
step 5610/400000: train loss: 0.172625
epoch:121, batch46, load frames use: 4.74650096893s
step 5611/400000: train loss: 0.193573
epoch:122, batch1, load frames use: 3.5332698822s
step 5612/400000: train loss: 0.199610
epoch:122, batch2, load frames use: 3.45415687561s
step 5613/400000: train loss: 0.193957
epoch:122, batch3, load frames use: 3.77744889259s
step 5614/400000: train loss: 0.194246
epoch:122, batch4, load frames use: 4.03826999664s
step 5615/400000: train loss: 0.173105
epoch:122, batch5, load frames use: 4.3862760067s
step 5616/400000: train loss: 0.186267
epoch:122, batch6, load frames use: 3.60061001778s
step 5617/400000: train loss: 0.161209
epoch:122, batch7, load frames use: 4.67255306244s
step 5618/400000: train loss: 0.177821
epoch:122, batch8, load frames use: 3.32836008072s
step 5619/400000: train loss: 0.199937
epoch:122, batch9, load frames use: 3.14344716072s
step 5620/400000: train loss: 0.210654
epoch:122, batch10, load frames use: 3.33736205101s
step 5621/400000: train loss: 0.179376
epoch:122, batch11, load frames use: 3.21895503998s
step 5622/400000: train loss: 0.171040
epoch:122, batch12, load frames use: 3.70169305801s
step 5623/400000: train loss: 0.159128
epoch:122, batch13, load frames use: 4.55031204224s
step 5624/400000: train loss: 0.159815
epoch:122, batch14, load frames use: 3.4283349514s
step 5625/400000: train loss: 0.193355
epoch:122, batch15, load frames use: 3.62171697617s
step 5626/400000: train loss: 0.168627
epoch:122, batch16, load frames use: 3.8958940506s
step 5627/400000: train loss: 0.171994
epoch:122, batch17, load frames use: 3.23828697205s
step 5628/400000: train loss: 0.185530
epoch:122, batch18, load frames use: 4.34882688522s
step 5629/400000: train loss: 0.182415
epoch:122, batch19, load frames use: 3.95093393326s
step 5630/400000: train loss: 0.170062
epoch:122, batch20, load frames use: 3.92059993744s
step 5631/400000: train loss: 0.179991
epoch:122, batch21, load frames use: 4.96987009048s
step 5632/400000: train loss: 0.192533
epoch:122, batch22, load frames use: 4.38371300697s
step 5633/400000: train loss: 0.199405
epoch:122, batch23, load frames use: 3.38339304924s
step 5634/400000: train loss: 0.197066
epoch:122, batch24, load frames use: 3.7403049469s
step 5635/400000: train loss: 0.207253
epoch:122, batch25, load frames use: 3.69531106949s
step 5636/400000: train loss: 0.207120
epoch:122, batch26, load frames use: 4.16709017754s
step 5637/400000: train loss: 0.186920
epoch:122, batch27, load frames use: 3.32648205757s
step 5638/400000: train loss: 0.169765
epoch:122, batch28, load frames use: 3.46346116066s
step 5639/400000: train loss: 0.170682
epoch:122, batch29, load frames use: 4.05886793137s
step 5640/400000: train loss: 0.194761
epoch:122, batch30, load frames use: 3.14148283005s
step 5641/400000: train loss: 0.183546
epoch:122, batch31, load frames use: 5.10285282135s
step 5642/400000: train loss: 0.182900
epoch:122, batch32, load frames use: 3.42747998238s
step 5643/400000: train loss: 0.201717
epoch:122, batch33, load frames use: 3.97378396988s
step 5644/400000: train loss: 0.183419
epoch:122, batch34, load frames use: 3.93829607964s
step 5645/400000: train loss: 0.190890
epoch:122, batch35, load frames use: 3.14645504951s
step 5646/400000: train loss: 0.180705
epoch:122, batch36, load frames use: 3.30277991295s
step 5647/400000: train loss: 0.167009
epoch:122, batch37, load frames use: 3.30462813377s
step 5648/400000: train loss: 0.187007
epoch:122, batch38, load frames use: 4.7608499527s
step 5649/400000: train loss: 0.192822
epoch:122, batch39, load frames use: 3.75657510757s
step 5650/400000: train loss: 0.210330
epoch:122, batch40, load frames use: 3.49367904663s
step 5651/400000: train loss: 0.188791
epoch:122, batch41, load frames use: 5.35644698143s
step 5652/400000: train loss: 0.182003
epoch:122, batch42, load frames use: 3.57034015656s
step 5653/400000: train loss: 0.194292
epoch:122, batch43, load frames use: 3.48522496223s
step 5654/400000: train loss: 0.197639
epoch:122, batch44, load frames use: 3.80936002731s
step 5655/400000: train loss: 0.172447
epoch:122, batch45, load frames use: 3.56285691261s
step 5656/400000: train loss: 0.196158
epoch:122, batch46, load frames use: 3.31150889397s
step 5657/400000: train loss: 0.175861
epoch:123, batch1, load frames use: 3.28468203545s
step 5658/400000: train loss: 0.176922
epoch:123, batch2, load frames use: 3.20513606071s
step 5659/400000: train loss: 0.171960
epoch:123, batch3, load frames use: 3.49479198456s
step 5660/400000: train loss: 0.169830
epoch:123, batch4, load frames use: 3.83661699295s
step 5661/400000: train loss: 0.196308
epoch:123, batch5, load frames use: 3.29122281075s
step 5662/400000: train loss: 0.202892
epoch:123, batch6, load frames use: 3.43519306183s
step 5663/400000: train loss: 0.173977
epoch:123, batch7, load frames use: 3.28880786896s
step 5664/400000: train loss: 0.194466
epoch:123, batch8, load frames use: 3.8540520668s
step 5665/400000: train loss: 0.214393
epoch:123, batch9, load frames use: 3.98501205444s
step 5666/400000: train loss: 0.180204
epoch:123, batch10, load frames use: 3.79034519196s
step 5667/400000: train loss: 0.194169
epoch:123, batch11, load frames use: 3.5337369442s
step 5668/400000: train loss: 0.177086
epoch:123, batch12, load frames use: 3.91725897789s
step 5669/400000: train loss: 0.182160
epoch:123, batch13, load frames use: 4.07093906403s
step 5670/400000: train loss: 0.187037
epoch:123, batch14, load frames use: 3.36063194275s
step 5671/400000: train loss: 0.185855
epoch:123, batch15, load frames use: 3.11732792854s
step 5672/400000: train loss: 0.168417
epoch:123, batch16, load frames use: 3.22930002213s
step 5673/400000: train loss: 0.161042
epoch:123, batch17, load frames use: 4.11127376556s
step 5674/400000: train loss: 0.186096
epoch:123, batch18, load frames use: 3.29920196533s
step 5675/400000: train loss: 0.179512
epoch:123, batch19, load frames use: 3.30474185944s
step 5676/400000: train loss: 0.199833
epoch:123, batch20, load frames use: 3.2698059082s
step 5677/400000: train loss: 0.173072
epoch:123, batch21, load frames use: 3.28693389893s
step 5678/400000: train loss: 0.186053
epoch:123, batch22, load frames use: 3.64289903641s
step 5679/400000: train loss: 0.187239
epoch:123, batch23, load frames use: 3.86141300201s
step 5680/400000: train loss: 0.188873
epoch:123, batch24, load frames use: 5.32773685455s
step 5681/400000: train loss: 0.186072
epoch:123, batch25, load frames use: 4.00805401802s
step 5682/400000: train loss: 0.176498
epoch:123, batch26, load frames use: 4.90100598335s
step 5683/400000: train loss: 0.180904
epoch:123, batch27, load frames use: 3.78038692474s
step 5684/400000: train loss: 0.192418
epoch:123, batch28, load frames use: 3.37276291847s
step 5685/400000: train loss: 0.189855
epoch:123, batch29, load frames use: 3.34394407272s
step 5686/400000: train loss: 0.191550
epoch:123, batch30, load frames use: 6.84077811241s
step 5687/400000: train loss: 0.191911
epoch:123, batch31, load frames use: 3.33537793159s
step 5688/400000: train loss: 0.191516
epoch:123, batch32, load frames use: 3.79447698593s
step 5689/400000: train loss: 0.171834
epoch:123, batch33, load frames use: 3.76808905602s
step 5690/400000: train loss: 0.182375
epoch:123, batch34, load frames use: 3.17876982689s
step 5691/400000: train loss: 0.180378
epoch:123, batch35, load frames use: 3.39409685135s
step 5692/400000: train loss: 0.190532
epoch:123, batch36, load frames use: 4.03602695465s
step 5693/400000: train loss: 0.173166
epoch:123, batch37, load frames use: 4.31053996086s
step 5694/400000: train loss: 0.182429
epoch:123, batch38, load frames use: 3.30582380295s
step 5695/400000: train loss: 0.188644
epoch:123, batch39, load frames use: 4.15032505989s
step 5696/400000: train loss: 0.179108
epoch:123, batch40, load frames use: 3.33948493004s
step 5697/400000: train loss: 0.193006
epoch:123, batch41, load frames use: 5.53081703186s
step 5698/400000: train loss: 0.180917
epoch:123, batch42, load frames use: 6.86205482483s
step 5699/400000: train loss: 0.191994
epoch:123, batch43, load frames use: 4.24563694s
step 5700/400000: train loss: 0.200651
step 5700/400000: validation loss: 0.194872
epoch:123, batch44, load frames use: 4.55051898956s
step 5701/400000: train loss: 0.195423
epoch:123, batch45, load frames use: 3.73934006691s
step 5702/400000: train loss: 0.184363
epoch:123, batch46, load frames use: 3.47747683525s
step 5703/400000: train loss: 0.172268
epoch:124, batch1, load frames use: 3.17092490196s
step 5704/400000: train loss: 0.207787
epoch:124, batch2, load frames use: 4.91857290268s
step 5705/400000: train loss: 0.166239
epoch:124, batch3, load frames use: 3.33207988739s
step 5706/400000: train loss: 0.185561
epoch:124, batch4, load frames use: 3.38805103302s
step 5707/400000: train loss: 0.193639
epoch:124, batch5, load frames use: 3.47659993172s
step 5708/400000: train loss: 0.176464
epoch:124, batch6, load frames use: 3.04871582985s
step 5709/400000: train loss: 0.201070
epoch:124, batch7, load frames use: 3.43481087685s
step 5710/400000: train loss: 0.195929
epoch:124, batch8, load frames use: 3.417096138s
step 5711/400000: train loss: 0.190068
epoch:124, batch9, load frames use: 3.14764308929s
step 5712/400000: train loss: 0.197105
epoch:124, batch10, load frames use: 3.31789207458s
step 5713/400000: train loss: 0.168188
epoch:124, batch11, load frames use: 3.92256593704s
step 5714/400000: train loss: 0.166184
epoch:124, batch12, load frames use: 3.16914081573s
step 5715/400000: train loss: 0.191746
epoch:124, batch13, load frames use: 3.66221404076s
step 5716/400000: train loss: 0.209832
epoch:124, batch14, load frames use: 3.30044603348s
step 5717/400000: train loss: 0.209455
epoch:124, batch15, load frames use: 3.55634999275s
step 5718/400000: train loss: 0.202731
epoch:124, batch16, load frames use: 6.33417701721s
step 5719/400000: train loss: 0.194412
epoch:124, batch17, load frames use: 3.8064930439s
step 5720/400000: train loss: 0.170557
epoch:124, batch18, load frames use: 3.63527989388s
step 5721/400000: train loss: 0.220665
epoch:124, batch19, load frames use: 2.76584887505s
step 5722/400000: train loss: 0.183421
epoch:124, batch20, load frames use: 3.66629290581s
step 5723/400000: train loss: 0.175401
epoch:124, batch21, load frames use: 4.3131248951s
step 5724/400000: train loss: 0.178459
epoch:124, batch22, load frames use: 3.63032698631s
step 5725/400000: train loss: 0.158518
epoch:124, batch23, load frames use: 3.47462415695s
step 5726/400000: train loss: 0.191695
epoch:124, batch24, load frames use: 3.182970047s
step 5727/400000: train loss: 0.191359
epoch:124, batch25, load frames use: 3.98925614357s
step 5728/400000: train loss: 0.173354
epoch:124, batch26, load frames use: 4.05871295929s
step 5729/400000: train loss: 0.209344
epoch:124, batch27, load frames use: 5.54338407516s
step 5730/400000: train loss: 0.169101
epoch:124, batch28, load frames use: 3.70811200142s
step 5731/400000: train loss: 0.196994
epoch:124, batch29, load frames use: 3.25124311447s
step 5732/400000: train loss: 0.189833
epoch:124, batch30, load frames use: 5.01741790771s
step 5733/400000: train loss: 0.185714
epoch:124, batch31, load frames use: 3.62909293175s
step 5734/400000: train loss: 0.185639
epoch:124, batch32, load frames use: 5.5812318325s
step 5735/400000: train loss: 0.189298
epoch:124, batch33, load frames use: 3.53693604469s
step 5736/400000: train loss: 0.182542
epoch:124, batch34, load frames use: 4.57381081581s
step 5737/400000: train loss: 0.206038
epoch:124, batch35, load frames use: 3.91738700867s
step 5738/400000: train loss: 0.170327
epoch:124, batch36, load frames use: 3.20030593872s
step 5739/400000: train loss: 0.187016
epoch:124, batch37, load frames use: 2.91792488098s
step 5740/400000: train loss: 0.176649
epoch:124, batch38, load frames use: 3.90580105782s
step 5741/400000: train loss: 0.202348
epoch:124, batch39, load frames use: 3.57607102394s
step 5742/400000: train loss: 0.181485
epoch:124, batch40, load frames use: 3.58332109451s
step 5743/400000: train loss: 0.159965
epoch:124, batch41, load frames use: 3.92420005798s
step 5744/400000: train loss: 0.188572
epoch:124, batch42, load frames use: 5.82157802582s
step 5745/400000: train loss: 0.177851
epoch:124, batch43, load frames use: 3.42849493027s
step 5746/400000: train loss: 0.210909
epoch:124, batch44, load frames use: 3.19075107574s
step 5747/400000: train loss: 0.195551
epoch:124, batch45, load frames use: 3.88555812836s
step 5748/400000: train loss: 0.204109
epoch:124, batch46, load frames use: 3.65286302567s
step 5749/400000: train loss: 0.186429
epoch:125, batch1, load frames use: 3.3020401001s
step 5750/400000: train loss: 0.150546
epoch:125, batch2, load frames use: 4.36216497421s
step 5751/400000: train loss: 0.187662
epoch:125, batch3, load frames use: 3.27115106583s
step 5752/400000: train loss: 0.200227
epoch:125, batch4, load frames use: 4.81053590775s
step 5753/400000: train loss: 0.177251
epoch:125, batch5, load frames use: 3.87345099449s
step 5754/400000: train loss: 0.197782
epoch:125, batch6, load frames use: 3.24219584465s
step 5755/400000: train loss: 0.170337
epoch:125, batch7, load frames use: 2.9276638031s
step 5756/400000: train loss: 0.191294
epoch:125, batch8, load frames use: 3.79150795937s
step 5757/400000: train loss: 0.195300
epoch:125, batch9, load frames use: 3.26257205009s
step 5758/400000: train loss: 0.186272
epoch:125, batch10, load frames use: 3.77378606796s
step 5759/400000: train loss: 0.182189
epoch:125, batch11, load frames use: 3.61277294159s
step 5760/400000: train loss: 0.168502
epoch:125, batch12, load frames use: 3.92914509773s
step 5761/400000: train loss: 0.170281
epoch:125, batch13, load frames use: 5.65840506554s
step 5762/400000: train loss: 0.180284
epoch:125, batch14, load frames use: 3.3540930748s
step 5763/400000: train loss: 0.186797
epoch:125, batch15, load frames use: 3.37901902199s
step 5764/400000: train loss: 0.190784
epoch:125, batch16, load frames use: 3.44737887383s
step 5765/400000: train loss: 0.183850
epoch:125, batch17, load frames use: 3.95995593071s
step 5766/400000: train loss: 0.181811
epoch:125, batch18, load frames use: 3.27690911293s
step 5767/400000: train loss: 0.190869
epoch:125, batch19, load frames use: 4.28781104088s
step 5768/400000: train loss: 0.192371
epoch:125, batch20, load frames use: 5.16008090973s
step 5769/400000: train loss: 0.184598
epoch:125, batch21, load frames use: 3.72996783257s
step 5770/400000: train loss: 0.170157
epoch:125, batch22, load frames use: 4.27730894089s
step 5771/400000: train loss: 0.193126
epoch:125, batch23, load frames use: 5.24364304543s
step 5772/400000: train loss: 0.189995
epoch:125, batch24, load frames use: 3.41432404518s
step 5773/400000: train loss: 0.204503
epoch:125, batch25, load frames use: 5.56319713593s
step 5774/400000: train loss: 0.192101
epoch:125, batch26, load frames use: 4.44379115105s
step 5775/400000: train loss: 0.176625
epoch:125, batch27, load frames use: 3.6501121521s
step 5776/400000: train loss: 0.161378
epoch:125, batch28, load frames use: 3.40874290466s
step 5777/400000: train loss: 0.184552
epoch:125, batch29, load frames use: 3.21473503113s
step 5778/400000: train loss: 0.200743
epoch:125, batch30, load frames use: 3.96651601791s
step 5779/400000: train loss: 0.171456
epoch:125, batch31, load frames use: 3.79342198372s
step 5780/400000: train loss: 0.173706
epoch:125, batch32, load frames use: 3.60403084755s
step 5781/400000: train loss: 0.204494
epoch:125, batch33, load frames use: 3.36611509323s
step 5782/400000: train loss: 0.210786
epoch:125, batch34, load frames use: 3.55961489677s
step 5783/400000: train loss: 0.173254
epoch:125, batch35, load frames use: 3.23079800606s
step 5784/400000: train loss: 0.186575
epoch:125, batch36, load frames use: 3.87584400177s
step 5785/400000: train loss: 0.173195
epoch:125, batch37, load frames use: 3.61198806763s
step 5786/400000: train loss: 0.187747
epoch:125, batch38, load frames use: 4.98053908348s
step 5787/400000: train loss: 0.246670
epoch:125, batch39, load frames use: 3.41065788269s
step 5788/400000: train loss: 0.196606
epoch:125, batch40, load frames use: 3.46182990074s
step 5789/400000: train loss: 0.184427
epoch:125, batch41, load frames use: 3.35213518143s
step 5790/400000: train loss: 0.193136
epoch:125, batch42, load frames use: 3.37466907501s
step 5791/400000: train loss: 0.178434
epoch:125, batch43, load frames use: 3.35634708405s
step 5792/400000: train loss: 0.214436
epoch:125, batch44, load frames use: 3.72168684006s
step 5793/400000: train loss: 0.195323
epoch:125, batch45, load frames use: 3.83831596375s
step 5794/400000: train loss: 0.197112
epoch:125, batch46, load frames use: 5.19414591789s
step 5795/400000: train loss: 0.190282
epoch:126, batch1, load frames use: 3.48091101646s
step 5796/400000: train loss: 0.172263
epoch:126, batch2, load frames use: 3.35483598709s
step 5797/400000: train loss: 0.193198
epoch:126, batch3, load frames use: 3.28148889542s
step 5798/400000: train loss: 0.191455
epoch:126, batch4, load frames use: 3.65157103539s
step 5799/400000: train loss: 0.178897
epoch:126, batch5, load frames use: 3.34683489799s
step 5800/400000: train loss: 0.177217
step 5800/400000: validation loss: 0.186811
epoch:126, batch6, load frames use: 3.4969727993s
step 5801/400000: train loss: 0.222701
epoch:126, batch7, load frames use: 3.43727898598s
step 5802/400000: train loss: 0.197072
epoch:126, batch8, load frames use: 4.66041517258s
step 5803/400000: train loss: 0.182983
epoch:126, batch9, load frames use: 4.208370924s
step 5804/400000: train loss: 0.193223
epoch:126, batch10, load frames use: 4.0278980732s
step 5805/400000: train loss: 0.176275
epoch:126, batch11, load frames use: 3.61418485641s
step 5806/400000: train loss: 0.195210
epoch:126, batch12, load frames use: 3.26581501961s
step 5807/400000: train loss: 0.167724
epoch:126, batch13, load frames use: 3.4495279789s
step 5808/400000: train loss: 0.186970
epoch:126, batch14, load frames use: 3.40071487427s
step 5809/400000: train loss: 0.190284
epoch:126, batch15, load frames use: 3.71326184273s
step 5810/400000: train loss: 0.152449
epoch:126, batch16, load frames use: 5.23396921158s
step 5811/400000: train loss: 0.170041
epoch:126, batch17, load frames use: 3.17797493935s
step 5812/400000: train loss: 0.171737
epoch:126, batch18, load frames use: 3.58297586441s
step 5813/400000: train loss: 0.181046
epoch:126, batch19, load frames use: 3.50478816032s
step 5814/400000: train loss: 0.193259
epoch:126, batch20, load frames use: 3.61175704002s
step 5815/400000: train loss: 0.184929
epoch:126, batch21, load frames use: 3.16512489319s
step 5816/400000: train loss: 0.192296
epoch:126, batch22, load frames use: 4.64511203766s
step 5817/400000: train loss: 0.196781
epoch:126, batch23, load frames use: 3.52501106262s
step 5818/400000: train loss: 0.212740
epoch:126, batch24, load frames use: 4.92240595818s
step 5819/400000: train loss: 0.201387
epoch:126, batch25, load frames use: 4.82643580437s
step 5820/400000: train loss: 0.186880
epoch:126, batch26, load frames use: 4.66967201233s
step 5821/400000: train loss: 0.180988
epoch:126, batch27, load frames use: 3.84219288826s
step 5822/400000: train loss: 0.192778
epoch:126, batch28, load frames use: 3.2168200016s
step 5823/400000: train loss: 0.187428
epoch:126, batch29, load frames use: 3.15690112114s
step 5824/400000: train loss: 0.173252
epoch:126, batch30, load frames use: 3.56005501747s
step 5825/400000: train loss: 0.192820
epoch:126, batch31, load frames use: 5.15318107605s
step 5826/400000: train loss: 0.232735
epoch:126, batch32, load frames use: 3.9786260128s
step 5827/400000: train loss: 0.188856
epoch:126, batch33, load frames use: 6.80555486679s
step 5828/400000: train loss: 0.179929
epoch:126, batch34, load frames use: 3.87170600891s
step 5829/400000: train loss: 0.215253
epoch:126, batch35, load frames use: 4.04080104828s
step 5830/400000: train loss: 0.182556
epoch:126, batch36, load frames use: 3.66566205025s
step 5831/400000: train loss: 0.177711
epoch:126, batch37, load frames use: 6.84449887276s
step 5832/400000: train loss: 0.178938
epoch:126, batch38, load frames use: 3.62525606155s
step 5833/400000: train loss: 0.224938
epoch:126, batch39, load frames use: 3.67750310898s
step 5834/400000: train loss: 0.193630
epoch:126, batch40, load frames use: 3.29491400719s
step 5835/400000: train loss: 0.183426
epoch:126, batch41, load frames use: 3.44146895409s
step 5836/400000: train loss: 0.184885
epoch:126, batch42, load frames use: 3.30579900742s
step 5837/400000: train loss: 0.185034
epoch:126, batch43, load frames use: 4.87020277977s
step 5838/400000: train loss: 0.200381
epoch:126, batch44, load frames use: 4.37282013893s
step 5839/400000: train loss: 0.180241
epoch:126, batch45, load frames use: 3.58654093742s
step 5840/400000: train loss: 0.195043
epoch:126, batch46, load frames use: 4.08814096451s
step 5841/400000: train loss: 0.175966
epoch:127, batch1, load frames use: 3.17079615593s
step 5842/400000: train loss: 0.193297
epoch:127, batch2, load frames use: 4.81298279762s
step 5843/400000: train loss: 0.166302
epoch:127, batch3, load frames use: 4.21298885345s
step 5844/400000: train loss: 0.181914
epoch:127, batch4, load frames use: 5.89595508575s
step 5845/400000: train loss: 0.178664
epoch:127, batch5, load frames use: 3.49303412437s
step 5846/400000: train loss: 0.187644
epoch:127, batch6, load frames use: 3.75715017319s
step 5847/400000: train loss: 0.149236
epoch:127, batch7, load frames use: 3.6861410141s
step 5848/400000: train loss: 0.180818
epoch:127, batch8, load frames use: 3.72459602356s
step 5849/400000: train loss: 0.166351
epoch:127, batch9, load frames use: 3.79181194305s
step 5850/400000: train loss: 0.173079
epoch:127, batch10, load frames use: 5.61731791496s
step 5851/400000: train loss: 0.176591
epoch:127, batch11, load frames use: 3.79898810387s
step 5852/400000: train loss: 0.196453
epoch:127, batch12, load frames use: 3.04631519318s
step 5853/400000: train loss: 0.178630
epoch:127, batch13, load frames use: 4.44229412079s
step 5854/400000: train loss: 0.187120
epoch:127, batch14, load frames use: 3.34660482407s
step 5855/400000: train loss: 0.168758
epoch:127, batch15, load frames use: 3.77483510971s
step 5856/400000: train loss: 0.180337
epoch:127, batch16, load frames use: 4.39025712013s
step 5857/400000: train loss: 0.199112
epoch:127, batch17, load frames use: 4.37459206581s
step 5858/400000: train loss: 0.175062
epoch:127, batch18, load frames use: 3.77800893784s
step 5859/400000: train loss: 0.167367
epoch:127, batch19, load frames use: 3.83888792992s
step 5860/400000: train loss: 0.182368
epoch:127, batch20, load frames use: 5.94179320335s
step 5861/400000: train loss: 0.158201
epoch:127, batch21, load frames use: 3.27313113213s
step 5862/400000: train loss: 0.175193
epoch:127, batch22, load frames use: 3.55397796631s
step 5863/400000: train loss: 0.175499
epoch:127, batch23, load frames use: 4.79407906532s
step 5864/400000: train loss: 0.197455
epoch:127, batch24, load frames use: 5.36586689949s
step 5865/400000: train loss: 0.191294
epoch:127, batch25, load frames use: 4.19198703766s
step 5866/400000: train loss: 0.173822
epoch:127, batch26, load frames use: 3.30312991142s
step 5867/400000: train loss: 0.180108
epoch:127, batch27, load frames use: 3.54854297638s
step 5868/400000: train loss: 0.169922
epoch:127, batch28, load frames use: 3.42027306557s
step 5869/400000: train loss: 0.180932
epoch:127, batch29, load frames use: 3.60754108429s
step 5870/400000: train loss: 0.185525
epoch:127, batch30, load frames use: 3.12252116203s
step 5871/400000: train loss: 0.212128
epoch:127, batch31, load frames use: 3.40921592712s
step 5872/400000: train loss: 0.182770
epoch:127, batch32, load frames use: 3.95194196701s
step 5873/400000: train loss: 0.196268
epoch:127, batch33, load frames use: 4.61415410042s
step 5874/400000: train loss: 0.183163
epoch:127, batch34, load frames use: 4.08312797546s
step 5875/400000: train loss: 0.164182
epoch:127, batch35, load frames use: 3.97332000732s
step 5876/400000: train loss: 0.162020
epoch:127, batch36, load frames use: 3.2886030674s
step 5877/400000: train loss: 0.194818
epoch:127, batch37, load frames use: 3.33895206451s
step 5878/400000: train loss: 0.195478
epoch:127, batch38, load frames use: 3.87822008133s
step 5879/400000: train loss: 0.187827
epoch:127, batch39, load frames use: 3.80919003487s
step 5880/400000: train loss: 0.220512
epoch:127, batch40, load frames use: 3.92172503471s
step 5881/400000: train loss: 0.197668
epoch:127, batch41, load frames use: 3.98841118813s
step 5882/400000: train loss: 0.186354
epoch:127, batch42, load frames use: 3.2427778244s
step 5883/400000: train loss: 0.179350
epoch:127, batch43, load frames use: 5.18073797226s
step 5884/400000: train loss: 0.210224
epoch:127, batch44, load frames use: 3.26668810844s
step 5885/400000: train loss: 0.184425
epoch:127, batch45, load frames use: 3.33187603951s
step 5886/400000: train loss: 0.204109
epoch:127, batch46, load frames use: 3.22374916077s
step 5887/400000: train loss: 0.180801
epoch:128, batch1, load frames use: 3.82225608826s
step 5888/400000: train loss: 0.180604
epoch:128, batch2, load frames use: 3.19869208336s
step 5889/400000: train loss: 0.180950
epoch:128, batch3, load frames use: 3.30524492264s
step 5890/400000: train loss: 0.178288
epoch:128, batch4, load frames use: 3.32746505737s
step 5891/400000: train loss: 0.168745
epoch:128, batch5, load frames use: 4.16044712067s
step 5892/400000: train loss: 0.191891
epoch:128, batch6, load frames use: 4.0188100338s
step 5893/400000: train loss: 0.182598
epoch:128, batch7, load frames use: 3.86799192429s
step 5894/400000: train loss: 0.218979
epoch:128, batch8, load frames use: 3.46745800972s
step 5895/400000: train loss: 0.177523
epoch:128, batch9, load frames use: 3.92553210258s
step 5896/400000: train loss: 0.153062
epoch:128, batch10, load frames use: 5.02148795128s
step 5897/400000: train loss: 0.169829
epoch:128, batch11, load frames use: 4.03947114944s
step 5898/400000: train loss: 0.169837
epoch:128, batch12, load frames use: 4.01923799515s
step 5899/400000: train loss: 0.176957
epoch:128, batch13, load frames use: 3.15113806725s
step 5900/400000: train loss: 0.187861
step 5900/400000: validation loss: 0.172223
epoch:128, batch14, load frames use: 5.65011096001s
step 5901/400000: train loss: 0.188001
epoch:128, batch15, load frames use: 3.64883303642s
step 5902/400000: train loss: 0.187172
epoch:128, batch16, load frames use: 3.23539400101s
step 5903/400000: train loss: 0.178308
epoch:128, batch17, load frames use: 3.42872691154s
step 5904/400000: train loss: 0.188227
epoch:128, batch18, load frames use: 3.49330210686s
step 5905/400000: train loss: 0.189690
epoch:128, batch19, load frames use: 3.943764925s
step 5906/400000: train loss: 0.183364
epoch:128, batch20, load frames use: 3.76062512398s
step 5907/400000: train loss: 0.172353
epoch:128, batch21, load frames use: 4.77474999428s
step 5908/400000: train loss: 0.183220
epoch:128, batch22, load frames use: 4.13437390327s
step 5909/400000: train loss: 0.189158
epoch:128, batch23, load frames use: 3.49618005753s
step 5910/400000: train loss: 0.170998
epoch:128, batch24, load frames use: 4.38301706314s
step 5911/400000: train loss: 0.159614
epoch:128, batch25, load frames use: 3.64632177353s
step 5912/400000: train loss: 0.171705
epoch:128, batch26, load frames use: 3.24044418335s
step 5913/400000: train loss: 0.190635
epoch:128, batch27, load frames use: 4.03066682816s
step 5914/400000: train loss: 0.171531
epoch:128, batch28, load frames use: 3.8774831295s
step 5915/400000: train loss: 0.194767
epoch:128, batch29, load frames use: 3.51490998268s
step 5916/400000: train loss: 0.194194
epoch:128, batch30, load frames use: 3.39066290855s
step 5917/400000: train loss: 0.194996
epoch:128, batch31, load frames use: 4.05752897263s
step 5918/400000: train loss: 0.191203
epoch:128, batch32, load frames use: 3.31909513474s
step 5919/400000: train loss: 0.186560
epoch:128, batch33, load frames use: 3.46460008621s
step 5920/400000: train loss: 0.179626
epoch:128, batch34, load frames use: 3.49295401573s
step 5921/400000: train loss: 0.182467
epoch:128, batch35, load frames use: 3.44130086899s
step 5922/400000: train loss: 0.190139
epoch:128, batch36, load frames use: 3.66693210602s
step 5923/400000: train loss: 0.196502
epoch:128, batch37, load frames use: 5.36403608322s
step 5924/400000: train loss: 0.191665
epoch:128, batch38, load frames use: 3.3049390316s
step 5925/400000: train loss: 0.176026
epoch:128, batch39, load frames use: 3.78577113152s
step 5926/400000: train loss: 0.166690
epoch:128, batch40, load frames use: 3.82588601112s
step 5927/400000: train loss: 0.208014
epoch:128, batch41, load frames use: 4.1822719574s
step 5928/400000: train loss: 0.187334
epoch:128, batch42, load frames use: 3.74552607536s
step 5929/400000: train loss: 0.181834
epoch:128, batch43, load frames use: 3.34895086288s
step 5930/400000: train loss: 0.180441
epoch:128, batch44, load frames use: 5.74217796326s
step 5931/400000: train loss: 0.198387
epoch:128, batch45, load frames use: 3.31367993355s
step 5932/400000: train loss: 0.174886
epoch:128, batch46, load frames use: 4.47114706039s
step 5933/400000: train loss: 0.182669
epoch:129, batch1, load frames use: 3.79474401474s
step 5934/400000: train loss: 0.176388
epoch:129, batch2, load frames use: 4.1906940937s
step 5935/400000: train loss: 0.165268
epoch:129, batch3, load frames use: 3.68076586723s
step 5936/400000: train loss: 0.190618
epoch:129, batch4, load frames use: 5.54375696182s
step 5937/400000: train loss: 0.178625
epoch:129, batch5, load frames use: 3.38987207413s
step 5938/400000: train loss: 0.192538
epoch:129, batch6, load frames use: 3.30733990669s
step 5939/400000: train loss: 0.175653
epoch:129, batch7, load frames use: 6.08339190483s
step 5940/400000: train loss: 0.172486
epoch:129, batch8, load frames use: 4.68387818336s
step 5941/400000: train loss: 0.177804
epoch:129, batch9, load frames use: 5.8412129879s
step 5942/400000: train loss: 0.179083
epoch:129, batch10, load frames use: 4.14450478554s
step 5943/400000: train loss: 0.166807
epoch:129, batch11, load frames use: 3.96516704559s
step 5944/400000: train loss: 0.164786
epoch:129, batch12, load frames use: 5.38266706467s
step 5945/400000: train loss: 0.176979
epoch:129, batch13, load frames use: 3.68864917755s
step 5946/400000: train loss: 0.160080
epoch:129, batch14, load frames use: 4.94627189636s
step 5947/400000: train loss: 0.194597
epoch:129, batch15, load frames use: 5.187458992s
step 5948/400000: train loss: 0.182560
epoch:129, batch16, load frames use: 3.76415991783s
step 5949/400000: train loss: 0.188385
epoch:129, batch17, load frames use: 3.75900697708s
step 5950/400000: train loss: 0.173167
epoch:129, batch18, load frames use: 3.15229988098s
step 5951/400000: train loss: 0.158701
epoch:129, batch19, load frames use: 3.62589001656s
step 5952/400000: train loss: 0.187887
epoch:129, batch20, load frames use: 4.63433289528s
step 5953/400000: train loss: 0.166336
epoch:129, batch21, load frames use: 4.17258119583s
step 5954/400000: train loss: 0.173005
epoch:129, batch22, load frames use: 4.17597699165s
step 5955/400000: train loss: 0.181241
epoch:129, batch23, load frames use: 4.56393504143s
step 5956/400000: train loss: 0.176745
epoch:129, batch24, load frames use: 4.3091301918s
step 5957/400000: train loss: 0.155784
epoch:129, batch25, load frames use: 3.47822904587s
step 5958/400000: train loss: 0.176711
epoch:129, batch26, load frames use: 6.83906888962s
step 5959/400000: train loss: 0.159954
epoch:129, batch27, load frames use: 4.32596993446s
step 5960/400000: train loss: 0.191005
epoch:129, batch28, load frames use: 3.16007304192s
step 5961/400000: train loss: 0.192752
epoch:129, batch29, load frames use: 3.22768521309s
step 5962/400000: train loss: 0.175949
epoch:129, batch30, load frames use: 3.15404391289s
step 5963/400000: train loss: 0.169299
epoch:129, batch31, load frames use: 3.20084404945s
step 5964/400000: train loss: 0.163516
epoch:129, batch32, load frames use: 3.64012694359s
step 5965/400000: train loss: 0.198179
epoch:129, batch33, load frames use: 4.65752696991s
step 5966/400000: train loss: 0.196590
epoch:129, batch34, load frames use: 3.21786785126s
step 5967/400000: train loss: 0.159995
epoch:129, batch35, load frames use: 3.81822109222s
step 5968/400000: train loss: 0.182833
epoch:129, batch36, load frames use: 3.33298897743s
step 5969/400000: train loss: 0.173951
epoch:129, batch37, load frames use: 3.33345508575s
step 5970/400000: train loss: 0.166383
epoch:129, batch38, load frames use: 3.79597687721s
step 5971/400000: train loss: 0.176814
epoch:129, batch39, load frames use: 5.38423204422s
step 5972/400000: train loss: 0.170587
epoch:129, batch40, load frames use: 3.45130681992s
step 5973/400000: train loss: 0.197880
epoch:129, batch41, load frames use: 3.37241721153s
step 5974/400000: train loss: 0.186829
epoch:129, batch42, load frames use: 4.04780006409s
step 5975/400000: train loss: 0.182923
epoch:129, batch43, load frames use: 3.15559411049s
step 5976/400000: train loss: 0.191250
epoch:129, batch44, load frames use: 3.59098696709s
step 5977/400000: train loss: 0.178991
epoch:129, batch45, load frames use: 4.56936717033s
step 5978/400000: train loss: 0.186920
epoch:129, batch46, load frames use: 3.3032720089s
step 5979/400000: train loss: 0.174880
epoch:130, batch1, load frames use: 6.88336920738s
step 5980/400000: train loss: 0.153317
epoch:130, batch2, load frames use: 3.64618992805s
step 5981/400000: train loss: 0.160446
epoch:130, batch3, load frames use: 3.83328008652s
step 5982/400000: train loss: 0.201144
epoch:130, batch4, load frames use: 3.18361997604s
step 5983/400000: train loss: 0.166026
epoch:130, batch5, load frames use: 3.5266058445s
step 5984/400000: train loss: 0.169177
epoch:130, batch6, load frames use: 4.3356628418s
step 5985/400000: train loss: 0.173798
epoch:130, batch7, load frames use: 3.46244692802s
step 5986/400000: train loss: 0.173923
epoch:130, batch8, load frames use: 3.29974985123s
step 5987/400000: train loss: 0.173891
epoch:130, batch9, load frames use: 4.13402986526s
step 5988/400000: train loss: 0.178463
epoch:130, batch10, load frames use: 3.42997503281s
step 5989/400000: train loss: 0.188879
epoch:130, batch11, load frames use: 3.63783693314s
step 5990/400000: train loss: 0.180734
epoch:130, batch12, load frames use: 4.73396587372s
step 5991/400000: train loss: 0.179509
epoch:130, batch13, load frames use: 4.71661090851s
step 5992/400000: train loss: 0.192200
epoch:130, batch14, load frames use: 3.7903740406s
step 5993/400000: train loss: 0.165000
epoch:130, batch15, load frames use: 4.28131699562s
step 5994/400000: train loss: 0.193629
epoch:130, batch16, load frames use: 3.12212181091s
step 5995/400000: train loss: 0.175614
epoch:130, batch17, load frames use: 4.39994311333s
step 5996/400000: train loss: 0.172942
epoch:130, batch18, load frames use: 5.26955294609s
step 5997/400000: train loss: 0.177388
epoch:130, batch19, load frames use: 4.01611804962s
step 5998/400000: train loss: 0.182087
epoch:130, batch20, load frames use: 3.42091703415s
step 5999/400000: train loss: 0.172354
epoch:130, batch21, load frames use: 4.03675985336s
step 6000/400000: train loss: 0.188959
step 6000/400000: validation loss: 0.169702
epoch:130, batch22, load frames use: 3.71741580963s
step 6001/400000: train loss: 0.185171
epoch:130, batch23, load frames use: 3.83957505226s
step 6002/400000: train loss: 0.176658
epoch:130, batch24, load frames use: 3.73638391495s
step 6003/400000: train loss: 0.183665
epoch:130, batch25, load frames use: 4.6801970005s
step 6004/400000: train loss: 0.174305
epoch:130, batch26, load frames use: 4.42928099632s
step 6005/400000: train loss: 0.175861
epoch:130, batch27, load frames use: 5.53840184212s
step 6006/400000: train loss: 0.160535
epoch:130, batch28, load frames use: 5.63122606277s
step 6007/400000: train loss: 0.173378
epoch:130, batch29, load frames use: 3.45502400398s
step 6008/400000: train loss: 0.166251
epoch:130, batch30, load frames use: 3.48376297951s
step 6009/400000: train loss: 0.161285
epoch:130, batch31, load frames use: 3.70177102089s
step 6010/400000: train loss: 0.202281
epoch:130, batch32, load frames use: 3.66399693489s
step 6011/400000: train loss: 0.193780
epoch:130, batch33, load frames use: 4.27283716202s
step 6012/400000: train loss: 0.198349
epoch:130, batch34, load frames use: 3.49327206612s
step 6013/400000: train loss: 0.187428
epoch:130, batch35, load frames use: 3.64478707314s
step 6014/400000: train loss: 0.195272
epoch:130, batch36, load frames use: 4.29188013077s
step 6015/400000: train loss: 0.183047
epoch:130, batch37, load frames use: 3.69730687141s
step 6016/400000: train loss: 0.191269
epoch:130, batch38, load frames use: 3.74983596802s
step 6017/400000: train loss: 0.206362
epoch:130, batch39, load frames use: 3.41529798508s
step 6018/400000: train loss: 0.181803
epoch:130, batch40, load frames use: 3.50021100044s
step 6019/400000: train loss: 0.211124
epoch:130, batch41, load frames use: 3.50971007347s
step 6020/400000: train loss: 0.174040
epoch:130, batch42, load frames use: 3.90398097038s
step 6021/400000: train loss: 0.187958
epoch:130, batch43, load frames use: 4.37158989906s
step 6022/400000: train loss: 0.171665
epoch:130, batch44, load frames use: 4.12242698669s
step 6023/400000: train loss: 0.185941
epoch:130, batch45, load frames use: 4.39030218124s
step 6024/400000: train loss: 0.184286
epoch:130, batch46, load frames use: 4.23676800728s
step 6025/400000: train loss: 0.202512
epoch:131, batch1, load frames use: 3.28455114365s
step 6026/400000: train loss: 0.168294
epoch:131, batch2, load frames use: 4.03583598137s
step 6027/400000: train loss: 0.193059
epoch:131, batch3, load frames use: 4.21344995499s
step 6028/400000: train loss: 0.171919
epoch:131, batch4, load frames use: 3.28204894066s
step 6029/400000: train loss: 0.178269
epoch:131, batch5, load frames use: 3.97483897209s
step 6030/400000: train loss: 0.181875
epoch:131, batch6, load frames use: 3.65810203552s
step 6031/400000: train loss: 0.173742
epoch:131, batch7, load frames use: 3.25643491745s
step 6032/400000: train loss: 0.166447
epoch:131, batch8, load frames use: 3.49303507805s
step 6033/400000: train loss: 0.177818
epoch:131, batch9, load frames use: 3.49575591087s
step 6034/400000: train loss: 0.165810
epoch:131, batch10, load frames use: 3.48431777954s
step 6035/400000: train loss: 0.159960
epoch:131, batch11, load frames use: 3.3897600174s
step 6036/400000: train loss: 0.194704
epoch:131, batch12, load frames use: 3.32942008972s
step 6037/400000: train loss: 0.174774
epoch:131, batch13, load frames use: 3.34351110458s
step 6038/400000: train loss: 0.191706
epoch:131, batch14, load frames use: 3.91101098061s
step 6039/400000: train loss: 0.145935
epoch:131, batch15, load frames use: 3.88871407509s
step 6040/400000: train loss: 0.182118
epoch:131, batch16, load frames use: 4.77094697952s
step 6041/400000: train loss: 0.184868
epoch:131, batch17, load frames use: 3.42703104019s
step 6042/400000: train loss: 0.181647
epoch:131, batch18, load frames use: 3.30699300766s
step 6043/400000: train loss: 0.185746
epoch:131, batch19, load frames use: 4.23025798798s
step 6044/400000: train loss: 0.170502
epoch:131, batch20, load frames use: 3.32915186882s
step 6045/400000: train loss: 0.179649
epoch:131, batch21, load frames use: 3.63066482544s
step 6046/400000: train loss: 0.165331
epoch:131, batch22, load frames use: 3.35591292381s
step 6047/400000: train loss: 0.179599
epoch:131, batch23, load frames use: 4.9918358326s
step 6048/400000: train loss: 0.179078
epoch:131, batch24, load frames use: 3.84483885765s
step 6049/400000: train loss: 0.175491
epoch:131, batch25, load frames use: 3.91802287102s
step 6050/400000: train loss: 0.176988
epoch:131, batch26, load frames use: 4.20823812485s
step 6051/400000: train loss: 0.168510
epoch:131, batch27, load frames use: 3.52561903s
step 6052/400000: train loss: 0.186524
epoch:131, batch28, load frames use: 3.93501806259s
step 6053/400000: train loss: 0.174704
epoch:131, batch29, load frames use: 3.53032803535s
step 6054/400000: train loss: 0.173412
epoch:131, batch30, load frames use: 3.52939009666s
step 6055/400000: train loss: 0.186578
epoch:131, batch31, load frames use: 3.92529702187s
step 6056/400000: train loss: 0.184182
epoch:131, batch32, load frames use: 4.3064789772s
step 6057/400000: train loss: 0.190819
epoch:131, batch33, load frames use: 3.79096794128s
step 6058/400000: train loss: 0.179458
epoch:131, batch34, load frames use: 3.68632411957s
step 6059/400000: train loss: 0.193129
epoch:131, batch35, load frames use: 3.66810917854s
step 6060/400000: train loss: 0.190915
epoch:131, batch36, load frames use: 3.46156692505s
step 6061/400000: train loss: 0.181669
epoch:131, batch37, load frames use: 3.99605989456s
step 6062/400000: train loss: 0.184000
epoch:131, batch38, load frames use: 4.30544996262s
step 6063/400000: train loss: 0.190009
epoch:131, batch39, load frames use: 4.10500812531s
step 6064/400000: train loss: 0.184290
epoch:131, batch40, load frames use: 3.43846797943s
step 6065/400000: train loss: 0.179428
epoch:131, batch41, load frames use: 3.37982797623s
step 6066/400000: train loss: 0.202439
epoch:131, batch42, load frames use: 3.52686786652s
step 6067/400000: train loss: 0.173602
epoch:131, batch43, load frames use: 3.26733493805s
step 6068/400000: train loss: 0.173518
epoch:131, batch44, load frames use: 3.59854006767s
step 6069/400000: train loss: 0.168968
epoch:131, batch45, load frames use: 3.59012293816s
step 6070/400000: train loss: 0.201230
epoch:131, batch46, load frames use: 4.48375582695s
step 6071/400000: train loss: 0.182987
epoch:132, batch1, load frames use: 3.75084614754s
step 6072/400000: train loss: 0.184898
epoch:132, batch2, load frames use: 4.22194910049s
step 6073/400000: train loss: 0.179460
epoch:132, batch3, load frames use: 3.3441529274s
step 6074/400000: train loss: 0.178594
epoch:132, batch4, load frames use: 3.42651200294s
step 6075/400000: train loss: 0.151268
epoch:132, batch5, load frames use: 3.51509308815s
step 6076/400000: train loss: 0.168140
epoch:132, batch6, load frames use: 3.5917840004s
step 6077/400000: train loss: 0.157421
epoch:132, batch7, load frames use: 4.33068704605s
step 6078/400000: train loss: 0.175745
epoch:132, batch8, load frames use: 4.39657592773s
step 6079/400000: train loss: 0.163988
epoch:132, batch9, load frames use: 4.25138521194s
step 6080/400000: train loss: 0.188232
epoch:132, batch10, load frames use: 4.08268594742s
step 6081/400000: train loss: 0.162405
epoch:132, batch11, load frames use: 3.92950296402s
step 6082/400000: train loss: 0.163612
epoch:132, batch12, load frames use: 3.62598705292s
step 6083/400000: train loss: 0.157075
epoch:132, batch13, load frames use: 4.02270698547s
step 6084/400000: train loss: 0.171974
epoch:132, batch14, load frames use: 3.50735592842s
step 6085/400000: train loss: 0.171700
epoch:132, batch15, load frames use: 3.05761694908s
step 6086/400000: train loss: 0.187005
epoch:132, batch16, load frames use: 4.13466191292s
step 6087/400000: train loss: 0.159516
epoch:132, batch17, load frames use: 5.37645792961s
step 6088/400000: train loss: 0.178403
epoch:132, batch18, load frames use: 3.61132621765s
step 6089/400000: train loss: 0.188930
epoch:132, batch19, load frames use: 5.64216399193s
step 6090/400000: train loss: 0.170054
epoch:132, batch20, load frames use: 4.60421895981s
step 6091/400000: train loss: 0.155923
epoch:132, batch21, load frames use: 4.69581699371s
step 6092/400000: train loss: 0.159398
epoch:132, batch22, load frames use: 4.04704403877s
step 6093/400000: train loss: 0.201483
epoch:132, batch23, load frames use: 3.57877397537s
step 6094/400000: train loss: 0.171894
epoch:132, batch24, load frames use: 3.57712507248s
step 6095/400000: train loss: 0.158697
epoch:132, batch25, load frames use: 3.96963095665s
step 6096/400000: train loss: 0.186281
epoch:132, batch26, load frames use: 3.79779005051s
step 6097/400000: train loss: 0.183943
epoch:132, batch27, load frames use: 3.27828001976s
step 6098/400000: train loss: 0.191811
epoch:132, batch28, load frames use: 3.57696413994s
step 6099/400000: train loss: 0.175282
epoch:132, batch29, load frames use: 3.39110994339s
step 6100/400000: train loss: 0.165525
step 6100/400000: validation loss: 0.156803
epoch:132, batch30, load frames use: 3.50774979591s
step 6101/400000: train loss: 0.180504
epoch:132, batch31, load frames use: 4.46686911583s
step 6102/400000: train loss: 0.166240
epoch:132, batch32, load frames use: 3.58428311348s
step 6103/400000: train loss: 0.189397
epoch:132, batch33, load frames use: 4.44001698494s
step 6104/400000: train loss: 0.194898
epoch:132, batch34, load frames use: 5.96310019493s
step 6105/400000: train loss: 0.176236
epoch:132, batch35, load frames use: 4.30166697502s
step 6106/400000: train loss: 0.188903
epoch:132, batch36, load frames use: 3.65271687508s
step 6107/400000: train loss: 0.174116
epoch:132, batch37, load frames use: 3.53452396393s
step 6108/400000: train loss: 0.160658
epoch:132, batch38, load frames use: 3.83450198174s
step 6109/400000: train loss: 0.159235
epoch:132, batch39, load frames use: 4.32005691528s
step 6110/400000: train loss: 0.192343
epoch:132, batch40, load frames use: 3.49711489677s
step 6111/400000: train loss: 0.178845
epoch:132, batch41, load frames use: 4.1978738308s
step 6112/400000: train loss: 0.183931
epoch:132, batch42, load frames use: 3.32305002213s
step 6113/400000: train loss: 0.183195
epoch:132, batch43, load frames use: 3.72094798088s
step 6114/400000: train loss: 0.181482
epoch:132, batch44, load frames use: 3.66357898712s
step 6115/400000: train loss: 0.178051
epoch:132, batch45, load frames use: 4.08964800835s
step 6116/400000: train loss: 0.181789
epoch:132, batch46, load frames use: 4.04244995117s
step 6117/400000: train loss: 0.190001
epoch:133, batch1, load frames use: 3.53497695923s
step 6118/400000: train loss: 0.159927
epoch:133, batch2, load frames use: 5.80668210983s
step 6119/400000: train loss: 0.173922
epoch:133, batch3, load frames use: 3.47185301781s
step 6120/400000: train loss: 0.155603
epoch:133, batch4, load frames use: 5.10253596306s
step 6121/400000: train loss: 0.174731
epoch:133, batch5, load frames use: 3.83500599861s
step 6122/400000: train loss: 0.155890
epoch:133, batch6, load frames use: 3.82428908348s
step 6123/400000: train loss: 0.178805
epoch:133, batch7, load frames use: 3.63599801064s
step 6124/400000: train loss: 0.164849
epoch:133, batch8, load frames use: 6.18804478645s
step 6125/400000: train loss: 0.170375
epoch:133, batch9, load frames use: 3.45167517662s
step 6126/400000: train loss: 0.187441
epoch:133, batch10, load frames use: 4.17707419395s
step 6127/400000: train loss: 0.185009
epoch:133, batch11, load frames use: 4.03475594521s
step 6128/400000: train loss: 0.172303
epoch:133, batch12, load frames use: 3.6222178936s
step 6129/400000: train loss: 0.203787
epoch:133, batch13, load frames use: 3.45780706406s
step 6130/400000: train loss: 0.178860
epoch:133, batch14, load frames use: 3.70580887794s
step 6131/400000: train loss: 0.182359
epoch:133, batch15, load frames use: 3.94466495514s
step 6132/400000: train loss: 0.181903
epoch:133, batch16, load frames use: 3.37025403976s
step 6133/400000: train loss: 0.180613
epoch:133, batch17, load frames use: 3.31788802147s
step 6134/400000: train loss: 0.185312
epoch:133, batch18, load frames use: 3.43577504158s
step 6135/400000: train loss: 0.186988
epoch:133, batch19, load frames use: 4.1800699234s
step 6136/400000: train loss: 0.189028
epoch:133, batch20, load frames use: 3.38094210625s
step 6137/400000: train loss: 0.188831
epoch:133, batch21, load frames use: 3.40228295326s
step 6138/400000: train loss: 0.163076
epoch:133, batch22, load frames use: 3.56469798088s
step 6139/400000: train loss: 0.176343
epoch:133, batch23, load frames use: 3.83656191826s
step 6140/400000: train loss: 0.192943
epoch:133, batch24, load frames use: 3.66744995117s
step 6141/400000: train loss: 0.174655
epoch:133, batch25, load frames use: 3.48447299004s
step 6142/400000: train loss: 0.178446
epoch:133, batch26, load frames use: 3.40386009216s
step 6143/400000: train loss: 0.180250
epoch:133, batch27, load frames use: 3.673828125s
step 6144/400000: train loss: 0.176756
epoch:133, batch28, load frames use: 3.43119978905s
step 6145/400000: train loss: 0.178837
epoch:133, batch29, load frames use: 4.5703690052s
step 6146/400000: train loss: 0.162558
epoch:133, batch30, load frames use: 4.32266783714s
step 6147/400000: train loss: 0.162247
epoch:133, batch31, load frames use: 3.4394671917s
step 6148/400000: train loss: 0.189121
epoch:133, batch32, load frames use: 4.14830183983s
step 6149/400000: train loss: 0.173239
epoch:133, batch33, load frames use: 4.1631770134s
step 6150/400000: train loss: 0.185164
epoch:133, batch34, load frames use: 3.68896603584s
step 6151/400000: train loss: 0.199572
epoch:133, batch35, load frames use: 3.63703298569s
step 6152/400000: train loss: 0.187889
epoch:133, batch36, load frames use: 5.42257404327s
step 6153/400000: train loss: 0.171845
epoch:133, batch37, load frames use: 3.34156298637s
step 6154/400000: train loss: 0.173177
epoch:133, batch38, load frames use: 5.21377301216s
step 6155/400000: train loss: 0.183258
epoch:133, batch39, load frames use: 4.24731492996s
step 6156/400000: train loss: 0.164161
epoch:133, batch40, load frames use: 4.41308188438s
step 6157/400000: train loss: 0.190042
epoch:133, batch41, load frames use: 3.20626282692s
step 6158/400000: train loss: 0.194605
epoch:133, batch42, load frames use: 4.34085392952s
step 6159/400000: train loss: 0.162751
epoch:133, batch43, load frames use: 3.64467096329s
step 6160/400000: train loss: 0.203611
epoch:133, batch44, load frames use: 4.29152011871s
step 6161/400000: train loss: 0.204631
epoch:133, batch45, load frames use: 3.87363004684s
step 6162/400000: train loss: 0.187339
epoch:133, batch46, load frames use: 3.51536393166s
step 6163/400000: train loss: 0.170281
epoch:134, batch1, load frames use: 3.43873500824s
step 6164/400000: train loss: 0.152676
epoch:134, batch2, load frames use: 3.63272595406s
step 6165/400000: train loss: 0.161436
epoch:134, batch3, load frames use: 3.38816189766s
step 6166/400000: train loss: 0.180384
epoch:134, batch4, load frames use: 3.4556479454s
step 6167/400000: train loss: 0.170039
epoch:134, batch5, load frames use: 3.5199739933s
step 6168/400000: train loss: 0.169212
epoch:134, batch6, load frames use: 3.38625097275s
step 6169/400000: train loss: 0.194619
epoch:134, batch7, load frames use: 3.56520104408s
step 6170/400000: train loss: 0.172891
epoch:134, batch8, load frames use: 3.95069789886s
step 6171/400000: train loss: 0.165388
epoch:134, batch9, load frames use: 3.43507313728s
step 6172/400000: train loss: 0.163139
epoch:134, batch10, load frames use: 4.88465809822s
step 6173/400000: train loss: 0.173929
epoch:134, batch11, load frames use: 5.75951099396s
step 6174/400000: train loss: 0.180521
epoch:134, batch12, load frames use: 3.58800816536s
step 6175/400000: train loss: 0.170771
epoch:134, batch13, load frames use: 3.96148300171s
step 6176/400000: train loss: 0.183330
epoch:134, batch14, load frames use: 3.53340387344s
step 6177/400000: train loss: 0.198341
epoch:134, batch15, load frames use: 3.71104288101s
step 6178/400000: train loss: 0.167456
epoch:134, batch16, load frames use: 3.23824191093s
step 6179/400000: train loss: 0.169896
epoch:134, batch17, load frames use: 5.10047578812s
step 6180/400000: train loss: 0.178695
epoch:134, batch18, load frames use: 3.81094908714s
step 6181/400000: train loss: 0.179432
epoch:134, batch19, load frames use: 3.9840400219s
step 6182/400000: train loss: 0.183136
epoch:134, batch20, load frames use: 3.94978690147s
step 6183/400000: train loss: 0.188451
epoch:134, batch21, load frames use: 3.91596603394s
step 6184/400000: train loss: 0.180787
epoch:134, batch22, load frames use: 3.61091709137s
step 6185/400000: train loss: 0.167545
epoch:134, batch23, load frames use: 4.16137909889s
step 6186/400000: train loss: 0.175955
epoch:134, batch24, load frames use: 4.1337249279s
step 6187/400000: train loss: 0.186960
epoch:134, batch25, load frames use: 4.40608501434s
step 6188/400000: train loss: 0.179572
epoch:134, batch26, load frames use: 4.42974591255s
step 6189/400000: train loss: 0.186027
epoch:134, batch27, load frames use: 4.31619095802s
step 6190/400000: train loss: 0.189236
epoch:134, batch28, load frames use: 4.15000915527s
step 6191/400000: train loss: 0.190289
epoch:134, batch29, load frames use: 3.78775596619s
step 6192/400000: train loss: 0.183062
epoch:134, batch30, load frames use: 3.38203597069s
step 6193/400000: train loss: 0.173150
epoch:134, batch31, load frames use: 3.91152000427s
step 6194/400000: train loss: 0.168406
epoch:134, batch32, load frames use: 4.61305499077s
step 6195/400000: train loss: 0.179981
epoch:134, batch33, load frames use: 3.84406995773s
step 6196/400000: train loss: 0.172382
epoch:134, batch34, load frames use: 3.2457690239s
step 6197/400000: train loss: 0.166457
epoch:134, batch35, load frames use: 4.6928191185s
step 6198/400000: train loss: 0.177117
epoch:134, batch36, load frames use: 3.46138000488s
step 6199/400000: train loss: 0.205381
epoch:134, batch37, load frames use: 3.60482215881s
step 6200/400000: train loss: 0.200277
step 6200/400000: validation loss: 0.201059
epoch:134, batch38, load frames use: 3.51130890846s
step 6201/400000: train loss: 0.179294
epoch:134, batch39, load frames use: 3.8130440712s
step 6202/400000: train loss: 0.190415
epoch:134, batch40, load frames use: 5.26889491081s
step 6203/400000: train loss: 0.164070
epoch:134, batch41, load frames use: 3.69432806969s
step 6204/400000: train loss: 0.169884
epoch:134, batch42, load frames use: 3.38461089134s
step 6205/400000: train loss: 0.190981
epoch:134, batch43, load frames use: 5.1848320961s
step 6206/400000: train loss: 0.175519
epoch:134, batch44, load frames use: 3.49652385712s
step 6207/400000: train loss: 0.182536
epoch:134, batch45, load frames use: 3.96577692032s
step 6208/400000: train loss: 0.212435
epoch:134, batch46, load frames use: 3.52050995827s
step 6209/400000: train loss: 0.184811
epoch:135, batch1, load frames use: 4.20398807526s
step 6210/400000: train loss: 0.160731
epoch:135, batch2, load frames use: 4.8036699295s
step 6211/400000: train loss: 0.162832
epoch:135, batch3, load frames use: 3.47922897339s
step 6212/400000: train loss: 0.161280
epoch:135, batch4, load frames use: 3.13367891312s
step 6213/400000: train loss: 0.174928
epoch:135, batch5, load frames use: 3.84607195854s
step 6214/400000: train loss: 0.196966
epoch:135, batch6, load frames use: 5.19543790817s
step 6215/400000: train loss: 0.171578
epoch:135, batch7, load frames use: 3.34213399887s
step 6216/400000: train loss: 0.180226
epoch:135, batch8, load frames use: 5.8661570549s
step 6217/400000: train loss: 0.183486
epoch:135, batch9, load frames use: 4.72600698471s
step 6218/400000: train loss: 0.195095
epoch:135, batch10, load frames use: 4.11743497849s
step 6219/400000: train loss: 0.162482
epoch:135, batch11, load frames use: 5.29912900925s
step 6220/400000: train loss: 0.175447
epoch:135, batch12, load frames use: 3.90677404404s
step 6221/400000: train loss: 0.181566
epoch:135, batch13, load frames use: 3.98483300209s
step 6222/400000: train loss: 0.183785
epoch:135, batch14, load frames use: 4.18754196167s
step 6223/400000: train loss: 0.176719
epoch:135, batch15, load frames use: 3.36464715004s
step 6224/400000: train loss: 0.163432
epoch:135, batch16, load frames use: 4.60535407066s
step 6225/400000: train loss: 0.186269
epoch:135, batch17, load frames use: 6.4443731308s
step 6226/400000: train loss: 0.181357
epoch:135, batch18, load frames use: 3.79449486732s
step 6227/400000: train loss: 0.173432
epoch:135, batch19, load frames use: 4.74538516998s
step 6228/400000: train loss: 0.187915
epoch:135, batch20, load frames use: 3.49885702133s
step 6229/400000: train loss: 0.174842
epoch:135, batch21, load frames use: 3.55914998055s
step 6230/400000: train loss: 0.166252
epoch:135, batch22, load frames use: 3.29084801674s
step 6231/400000: train loss: 0.178933
epoch:135, batch23, load frames use: 3.84951400757s
step 6232/400000: train loss: 0.176604
epoch:135, batch24, load frames use: 3.61301898956s
step 6233/400000: train loss: 0.163434
epoch:135, batch25, load frames use: 3.28129005432s
step 6234/400000: train loss: 0.168798
epoch:135, batch26, load frames use: 3.45161104202s
step 6235/400000: train loss: 0.184037
epoch:135, batch27, load frames use: 4.65610194206s
step 6236/400000: train loss: 0.191461
epoch:135, batch28, load frames use: 3.72167205811s
step 6237/400000: train loss: 0.188108
epoch:135, batch29, load frames use: 3.9992620945s
step 6238/400000: train loss: 0.179281
epoch:135, batch30, load frames use: 4.30064105988s
step 6239/400000: train loss: 0.160621
epoch:135, batch31, load frames use: 4.60205698013s
step 6240/400000: train loss: 0.205108
epoch:135, batch32, load frames use: 3.86048412323s
step 6241/400000: train loss: 0.164489
epoch:135, batch33, load frames use: 3.48934102058s
step 6242/400000: train loss: 0.183118
epoch:135, batch34, load frames use: 4.12315487862s
step 6243/400000: train loss: 0.175510
epoch:135, batch35, load frames use: 4.16019892693s
step 6244/400000: train loss: 0.192049
epoch:135, batch36, load frames use: 3.35148501396s
step 6245/400000: train loss: 0.179005
epoch:135, batch37, load frames use: 4.41978406906s
step 6246/400000: train loss: 0.174534
epoch:135, batch38, load frames use: 3.68513894081s
step 6247/400000: train loss: 0.177555
epoch:135, batch39, load frames use: 3.51019287109s
step 6248/400000: train loss: 0.179006
epoch:135, batch40, load frames use: 4.60503697395s
step 6249/400000: train loss: 0.164152
epoch:135, batch41, load frames use: 5.04889893532s
step 6250/400000: train loss: 0.168663
epoch:135, batch42, load frames use: 4.28921198845s
step 6251/400000: train loss: 0.196285
epoch:135, batch43, load frames use: 5.36075806618s
step 6252/400000: train loss: 0.172966
epoch:135, batch44, load frames use: 4.48289108276s
step 6253/400000: train loss: 0.176954
epoch:135, batch45, load frames use: 3.41529297829s
step 6254/400000: train loss: 0.165824
epoch:135, batch46, load frames use: 3.73140287399s
step 6255/400000: train loss: 0.182988
epoch:136, batch1, load frames use: 5.26593017578s
step 6256/400000: train loss: 0.144800
epoch:136, batch2, load frames use: 3.56700205803s
step 6257/400000: train loss: 0.163639
epoch:136, batch3, load frames use: 5.07154798508s
step 6258/400000: train loss: 0.205810
epoch:136, batch4, load frames use: 4.40905404091s
step 6259/400000: train loss: 0.180129
epoch:136, batch5, load frames use: 4.34767103195s
step 6260/400000: train loss: 0.168120
epoch:136, batch6, load frames use: 3.16697406769s
step 6261/400000: train loss: 0.177840
epoch:136, batch7, load frames use: 3.33218288422s
step 6262/400000: train loss: 0.179771
epoch:136, batch8, load frames use: 3.34336400032s
step 6263/400000: train loss: 0.191785
epoch:136, batch9, load frames use: 3.62735915184s
step 6264/400000: train loss: 0.152798
epoch:136, batch10, load frames use: 4.7735888958s
step 6265/400000: train loss: 0.169203
epoch:136, batch11, load frames use: 3.29492497444s
step 6266/400000: train loss: 0.181999
epoch:136, batch12, load frames use: 4.02721786499s
step 6267/400000: train loss: 0.161892
epoch:136, batch13, load frames use: 3.81566286087s
step 6268/400000: train loss: 0.185152
epoch:136, batch14, load frames use: 3.42593693733s
step 6269/400000: train loss: 0.165854
epoch:136, batch15, load frames use: 3.91490387917s
step 6270/400000: train loss: 0.180958
epoch:136, batch16, load frames use: 4.39710497856s
step 6271/400000: train loss: 0.174690
epoch:136, batch17, load frames use: 3.58236098289s
step 6272/400000: train loss: 0.165746
epoch:136, batch18, load frames use: 4.66408205032s
step 6273/400000: train loss: 0.189963
epoch:136, batch19, load frames use: 4.3119931221s
step 6274/400000: train loss: 0.179499
epoch:136, batch20, load frames use: 4.33640313148s
step 6275/400000: train loss: 0.190393
epoch:136, batch21, load frames use: 3.63789010048s
step 6276/400000: train loss: 0.176097
epoch:136, batch22, load frames use: 3.56918287277s
step 6277/400000: train loss: 0.202141
epoch:136, batch23, load frames use: 3.76409220695s
step 6278/400000: train loss: 0.185370
epoch:136, batch24, load frames use: 4.02481198311s
step 6279/400000: train loss: 0.182100
epoch:136, batch25, load frames use: 6.77534103394s
step 6280/400000: train loss: 0.192566
epoch:136, batch26, load frames use: 3.31781888008s
step 6281/400000: train loss: 0.204234
epoch:136, batch27, load frames use: 3.90833711624s
step 6282/400000: train loss: 0.186351
epoch:136, batch28, load frames use: 3.97783493996s
step 6283/400000: train loss: 0.180364
epoch:136, batch29, load frames use: 3.66102194786s
step 6284/400000: train loss: 0.191503
epoch:136, batch30, load frames use: 3.46883106232s
step 6285/400000: train loss: 0.185507
epoch:136, batch31, load frames use: 3.76493406296s
step 6286/400000: train loss: 0.167146
epoch:136, batch32, load frames use: 3.46141004562s
step 6287/400000: train loss: 0.168700
epoch:136, batch33, load frames use: 4.04314994812s
step 6288/400000: train loss: 0.186373
epoch:136, batch34, load frames use: 4.17574715614s
step 6289/400000: train loss: 0.176888
epoch:136, batch35, load frames use: 3.92697811127s
step 6290/400000: train loss: 0.169979
epoch:136, batch36, load frames use: 6.1662709713s
step 6291/400000: train loss: 0.177295
epoch:136, batch37, load frames use: 3.42036104202s
step 6292/400000: train loss: 0.177836
epoch:136, batch38, load frames use: 3.21082782745s
step 6293/400000: train loss: 0.179689
epoch:136, batch39, load frames use: 3.83504796028s
step 6294/400000: train loss: 0.190851
epoch:136, batch40, load frames use: 3.58898687363s
step 6295/400000: train loss: 0.157859
epoch:136, batch41, load frames use: 3.99796915054s
step 6296/400000: train loss: 0.182927
epoch:136, batch42, load frames use: 3.46736311913s
step 6297/400000: train loss: 0.166719
epoch:136, batch43, load frames use: 4.09782218933s
step 6298/400000: train loss: 0.174156
epoch:136, batch44, load frames use: 4.41866397858s
step 6299/400000: train loss: 0.169770
epoch:136, batch45, load frames use: 3.91566491127s
step 6300/400000: train loss: 0.183557
step 6300/400000: validation loss: 0.177922
epoch:136, batch46, load frames use: 4.63947701454s
step 6301/400000: train loss: 0.179800
epoch:137, batch1, load frames use: 4.12040281296s
step 6302/400000: train loss: 0.172403
epoch:137, batch2, load frames use: 3.87770390511s
step 6303/400000: train loss: 0.165892
epoch:137, batch3, load frames use: 3.82148504257s
step 6304/400000: train loss: 0.161346
epoch:137, batch4, load frames use: 4.21348810196s
step 6305/400000: train loss: 0.175863
epoch:137, batch5, load frames use: 4.77103710175s
step 6306/400000: train loss: 0.163468
epoch:137, batch6, load frames use: 3.87446713448s
step 6307/400000: train loss: 0.163805
epoch:137, batch7, load frames use: 3.57436299324s
step 6308/400000: train loss: 0.175954
epoch:137, batch8, load frames use: 3.55135512352s
step 6309/400000: train loss: 0.174304
epoch:137, batch9, load frames use: 4.77666091919s
step 6310/400000: train loss: 0.183844
epoch:137, batch10, load frames use: 3.99591708183s
step 6311/400000: train loss: 0.177449
epoch:137, batch11, load frames use: 4.09421515465s
step 6312/400000: train loss: 0.149316
epoch:137, batch12, load frames use: 4.08464694023s
step 6313/400000: train loss: 0.164548
epoch:137, batch13, load frames use: 3.41572284698s
step 6314/400000: train loss: 0.146249
epoch:137, batch14, load frames use: 4.2232940197s
step 6315/400000: train loss: 0.176084
epoch:137, batch15, load frames use: 3.8426721096s
step 6316/400000: train loss: 0.166828
epoch:137, batch16, load frames use: 4.3427760601s
step 6317/400000: train loss: 0.168683
epoch:137, batch17, load frames use: 3.37928891182s
step 6318/400000: train loss: 0.170839
epoch:137, batch18, load frames use: 3.74012589455s
step 6319/400000: train loss: 0.162928
epoch:137, batch19, load frames use: 5.24515604973s
step 6320/400000: train loss: 0.184300
epoch:137, batch20, load frames use: 3.82991099358s
step 6321/400000: train loss: 0.172886
epoch:137, batch21, load frames use: 6.23401093483s
step 6322/400000: train loss: 0.168665
epoch:137, batch22, load frames use: 3.84542608261s
step 6323/400000: train loss: 0.169242
epoch:137, batch23, load frames use: 3.86431288719s
step 6324/400000: train loss: 0.175783
epoch:137, batch24, load frames use: 4.90250992775s
step 6325/400000: train loss: 0.166732
epoch:137, batch25, load frames use: 4.15557813644s
step 6326/400000: train loss: 0.180203
epoch:137, batch26, load frames use: 3.30025577545s
step 6327/400000: train loss: 0.172408
epoch:137, batch27, load frames use: 4.32974100113s
step 6328/400000: train loss: 0.175847
epoch:137, batch28, load frames use: 3.33146905899s
step 6329/400000: train loss: 0.186549
epoch:137, batch29, load frames use: 4.09874510765s
step 6330/400000: train loss: 0.173762
epoch:137, batch30, load frames use: 4.08953905106s
step 6331/400000: train loss: 0.181940
epoch:137, batch31, load frames use: 4.19866704941s
step 6332/400000: train loss: 0.186347
epoch:137, batch32, load frames use: 3.90952205658s
step 6333/400000: train loss: 0.167185
epoch:137, batch33, load frames use: 5.91420698166s
step 6334/400000: train loss: 0.189054
epoch:137, batch34, load frames use: 4.05275797844s
step 6335/400000: train loss: 0.170226
epoch:137, batch35, load frames use: 5.35257387161s
step 6336/400000: train loss: 0.197270
epoch:137, batch36, load frames use: 3.57788181305s
step 6337/400000: train loss: 0.177371
epoch:137, batch37, load frames use: 5.77703285217s
step 6338/400000: train loss: 0.164547
epoch:137, batch38, load frames use: 4.18168306351s
step 6339/400000: train loss: 0.200947
epoch:137, batch39, load frames use: 3.95389199257s
step 6340/400000: train loss: 0.179911
epoch:137, batch40, load frames use: 3.46399188042s
step 6341/400000: train loss: 0.169483
epoch:137, batch41, load frames use: 4.22767686844s
step 6342/400000: train loss: 0.180040
epoch:137, batch42, load frames use: 4.1005821228s
step 6343/400000: train loss: 0.176430
epoch:137, batch43, load frames use: 3.73299980164s
step 6344/400000: train loss: 0.172259
epoch:137, batch44, load frames use: 3.89347910881s
step 6345/400000: train loss: 0.177101
epoch:137, batch45, load frames use: 3.82371997833s
step 6346/400000: train loss: 0.180406
epoch:137, batch46, load frames use: 5.86085605621s
step 6347/400000: train loss: 0.167143
epoch:138, batch1, load frames use: 4.14083600044s
step 6348/400000: train loss: 0.181443
epoch:138, batch2, load frames use: 3.52487492561s
step 6349/400000: train loss: 0.172094
epoch:138, batch3, load frames use: 5.07251787186s
step 6350/400000: train loss: 0.175734
epoch:138, batch4, load frames use: 4.43425583839s
step 6351/400000: train loss: 0.163119
epoch:138, batch5, load frames use: 3.72431087494s
step 6352/400000: train loss: 0.168957
epoch:138, batch6, load frames use: 3.56055188179s
step 6353/400000: train loss: 0.148142
epoch:138, batch7, load frames use: 3.44053697586s
step 6354/400000: train loss: 0.164704
epoch:138, batch8, load frames use: 3.37582111359s
step 6355/400000: train loss: 0.152203
epoch:138, batch9, load frames use: 3.25805592537s
step 6356/400000: train loss: 0.162831
epoch:138, batch10, load frames use: 3.08868908882s
step 6357/400000: train loss: 0.172159
epoch:138, batch11, load frames use: 3.73056507111s
step 6358/400000: train loss: 0.171592
epoch:138, batch12, load frames use: 3.59472107887s
step 6359/400000: train loss: 0.159819
epoch:138, batch13, load frames use: 3.96326303482s
step 6360/400000: train loss: 0.180591
epoch:138, batch14, load frames use: 3.9507420063s
step 6361/400000: train loss: 0.171238
epoch:138, batch15, load frames use: 4.75309991837s
step 6362/400000: train loss: 0.168687
epoch:138, batch16, load frames use: 4.22813105583s
step 6363/400000: train loss: 0.173103
epoch:138, batch17, load frames use: 3.72070217133s
step 6364/400000: train loss: 0.168538
epoch:138, batch18, load frames use: 3.58195781708s
step 6365/400000: train loss: 0.167236
epoch:138, batch19, load frames use: 4.60170507431s
step 6366/400000: train loss: 0.186141
epoch:138, batch20, load frames use: 4.02769899368s
step 6367/400000: train loss: 0.167934
epoch:138, batch21, load frames use: 4.2766289711s
step 6368/400000: train loss: 0.170702
epoch:138, batch22, load frames use: 4.80031299591s
step 6369/400000: train loss: 0.166647
epoch:138, batch23, load frames use: 5.29563212395s
step 6370/400000: train loss: 0.162425
epoch:138, batch24, load frames use: 3.4483230114s
step 6371/400000: train loss: 0.148529
epoch:138, batch25, load frames use: 3.36235809326s
step 6372/400000: train loss: 0.164737
epoch:138, batch26, load frames use: 3.31985902786s
step 6373/400000: train loss: 0.161464
epoch:138, batch27, load frames use: 3.61016201973s
step 6374/400000: train loss: 0.174021
epoch:138, batch28, load frames use: 4.67194890976s
step 6375/400000: train loss: 0.185057
epoch:138, batch29, load frames use: 4.41276693344s
step 6376/400000: train loss: 0.184576
epoch:138, batch30, load frames use: 4.46052289009s
step 6377/400000: train loss: 0.173218
epoch:138, batch31, load frames use: 3.60791707039s
step 6378/400000: train loss: 0.167482
epoch:138, batch32, load frames use: 3.90637612343s
step 6379/400000: train loss: 0.188568
epoch:138, batch33, load frames use: 5.07259297371s
step 6380/400000: train loss: 0.154119
epoch:138, batch34, load frames use: 3.91876316071s
step 6381/400000: train loss: 0.175471
epoch:138, batch35, load frames use: 3.69610905647s
step 6382/400000: train loss: 0.161006
epoch:138, batch36, load frames use: 4.13806200027s
step 6383/400000: train loss: 0.147528
epoch:138, batch37, load frames use: 6.0803899765s
step 6384/400000: train loss: 0.167729
epoch:138, batch38, load frames use: 4.3876080513s
step 6385/400000: train loss: 0.164757
epoch:138, batch39, load frames use: 3.35115695s
step 6386/400000: train loss: 0.164894
epoch:138, batch40, load frames use: 4.33848905563s
step 6387/400000: train loss: 0.178992
epoch:138, batch41, load frames use: 5.31407403946s
step 6388/400000: train loss: 0.155786
epoch:138, batch42, load frames use: 4.7674779892s
step 6389/400000: train loss: 0.172725
epoch:138, batch43, load frames use: 4.07495594025s
step 6390/400000: train loss: 0.196462
epoch:138, batch44, load frames use: 4.30849003792s
step 6391/400000: train loss: 0.171212
epoch:138, batch45, load frames use: 5.32357192039s
step 6392/400000: train loss: 0.174425
epoch:138, batch46, load frames use: 5.55903506279s
step 6393/400000: train loss: 0.178487
epoch:139, batch1, load frames use: 5.34090018272s
step 6394/400000: train loss: 0.158463
epoch:139, batch2, load frames use: 5.28907394409s
step 6395/400000: train loss: 0.182447
epoch:139, batch3, load frames use: 3.60261797905s
step 6396/400000: train loss: 0.183020
epoch:139, batch4, load frames use: 3.66934204102s
step 6397/400000: train loss: 0.187665
epoch:139, batch5, load frames use: 4.19449996948s
step 6398/400000: train loss: 0.184039
epoch:139, batch6, load frames use: 4.7436311245s
step 6399/400000: train loss: 0.174131
epoch:139, batch7, load frames use: 4.39216399193s
step 6400/400000: train loss: 0.157641
step 6400/400000: validation loss: 0.165478
epoch:139, batch8, load frames use: 3.25998401642s
step 6401/400000: train loss: 0.152103
epoch:139, batch9, load frames use: 3.79416894913s
step 6402/400000: train loss: 0.171798
epoch:139, batch10, load frames use: 3.50222492218s
step 6403/400000: train loss: 0.160956
epoch:139, batch11, load frames use: 3.69444108009s
step 6404/400000: train loss: 0.140724
epoch:139, batch12, load frames use: 3.43899798393s
step 6405/400000: train loss: 0.170858
epoch:139, batch13, load frames use: 4.92642617226s
step 6406/400000: train loss: 0.162388
epoch:139, batch14, load frames use: 3.90046691895s
step 6407/400000: train loss: 0.164668
epoch:139, batch15, load frames use: 3.52087688446s
step 6408/400000: train loss: 0.155317
epoch:139, batch16, load frames use: 4.47247815132s
step 6409/400000: train loss: 0.172074
epoch:139, batch17, load frames use: 3.60429787636s
step 6410/400000: train loss: 0.180902
epoch:139, batch18, load frames use: 3.6632809639s
step 6411/400000: train loss: 0.184294
epoch:139, batch19, load frames use: 4.84287405014s
step 6412/400000: train loss: 0.177832
epoch:139, batch20, load frames use: 4.65374994278s
step 6413/400000: train loss: 0.186916
epoch:139, batch21, load frames use: 6.29491901398s
step 6414/400000: train loss: 0.167739
epoch:139, batch22, load frames use: 3.44698095322s
step 6415/400000: train loss: 0.178014
epoch:139, batch23, load frames use: 3.87185382843s
step 6416/400000: train loss: 0.173359
epoch:139, batch24, load frames use: 3.32143902779s
step 6417/400000: train loss: 0.181709
epoch:139, batch25, load frames use: 3.40312194824s
step 6418/400000: train loss: 0.153779
epoch:139, batch26, load frames use: 3.56613087654s
step 6419/400000: train loss: 0.168315
epoch:139, batch27, load frames use: 3.72740507126s
step 6420/400000: train loss: 0.193464
epoch:139, batch28, load frames use: 3.5768558979s
step 6421/400000: train loss: 0.170307
epoch:139, batch29, load frames use: 4.00740790367s
step 6422/400000: train loss: 0.163152
epoch:139, batch30, load frames use: 3.75301790237s
step 6423/400000: train loss: 0.179643
epoch:139, batch31, load frames use: 4.44229912758s
step 6424/400000: train loss: 0.158882
epoch:139, batch32, load frames use: 4.55072999001s
step 6425/400000: train loss: 0.178357
epoch:139, batch33, load frames use: 3.79959201813s
step 6426/400000: train loss: 0.166619
epoch:139, batch34, load frames use: 4.00751614571s
step 6427/400000: train loss: 0.169001
epoch:139, batch35, load frames use: 4.97182393074s
step 6428/400000: train loss: 0.174970
epoch:139, batch36, load frames use: 3.89414095879s
step 6429/400000: train loss: 0.155032
epoch:139, batch37, load frames use: 4.20101189613s
step 6430/400000: train loss: 0.181476
epoch:139, batch38, load frames use: 3.67460083961s
step 6431/400000: train loss: 0.198998
epoch:139, batch39, load frames use: 5.23781991005s
step 6432/400000: train loss: 0.165109
epoch:139, batch40, load frames use: 6.26348686218s
step 6433/400000: train loss: 0.186564
epoch:139, batch41, load frames use: 3.37873101234s
step 6434/400000: train loss: 0.155363
epoch:139, batch42, load frames use: 3.25055122375s
step 6435/400000: train loss: 0.170504
epoch:139, batch43, load frames use: 3.39788198471s
step 6436/400000: train loss: 0.180276
epoch:139, batch44, load frames use: 4.34579515457s
step 6437/400000: train loss: 0.182154
epoch:139, batch45, load frames use: 3.89572191238s
step 6438/400000: train loss: 0.174610
epoch:139, batch46, load frames use: 3.67898702621s
step 6439/400000: train loss: 0.191284
epoch:140, batch1, load frames use: 4.787940979s
step 6440/400000: train loss: 0.159222
epoch:140, batch2, load frames use: 4.77127718925s
step 6441/400000: train loss: 0.179186
epoch:140, batch3, load frames use: 3.96546006203s
step 6442/400000: train loss: 0.157470
epoch:140, batch4, load frames use: 4.20305991173s
step 6443/400000: train loss: 0.177847
epoch:140, batch5, load frames use: 4.08686089516s
step 6444/400000: train loss: 0.173690
epoch:140, batch6, load frames use: 4.82138705254s
step 6445/400000: train loss: 0.175415
epoch:140, batch7, load frames use: 4.04208803177s
step 6446/400000: train loss: 0.160423
epoch:140, batch8, load frames use: 3.8893430233s
step 6447/400000: train loss: 0.190555
epoch:140, batch9, load frames use: 3.58104109764s
step 6448/400000: train loss: 0.183723
epoch:140, batch10, load frames use: 4.15215086937s
step 6449/400000: train loss: 0.175504
epoch:140, batch11, load frames use: 3.87059211731s
step 6450/400000: train loss: 0.168471
epoch:140, batch12, load frames use: 3.75821304321s
step 6451/400000: train loss: 0.182123
epoch:140, batch13, load frames use: 3.824862957s
step 6452/400000: train loss: 0.175851
epoch:140, batch14, load frames use: 3.78345489502s
step 6453/400000: train loss: 0.194041
epoch:140, batch15, load frames use: 3.94297599792s
step 6454/400000: train loss: 0.198166
epoch:140, batch16, load frames use: 5.03749895096s
step 6455/400000: train loss: 0.186627
epoch:140, batch17, load frames use: 3.85849189758s
step 6456/400000: train loss: 0.174821
epoch:140, batch18, load frames use: 4.22423911095s
step 6457/400000: train loss: 0.163326
epoch:140, batch19, load frames use: 3.5822660923s
step 6458/400000: train loss: 0.169024
epoch:140, batch20, load frames use: 3.93643689156s
step 6459/400000: train loss: 0.172320
epoch:140, batch21, load frames use: 5.16535592079s
step 6460/400000: train loss: 0.162280
epoch:140, batch22, load frames use: 5.11610603333s
step 6461/400000: train loss: 0.169819
epoch:140, batch23, load frames use: 3.43928289413s
step 6462/400000: train loss: 0.180611
epoch:140, batch24, load frames use: 4.25474214554s
step 6463/400000: train loss: 0.156941
epoch:140, batch25, load frames use: 4.75905203819s
step 6464/400000: train loss: 0.201778
epoch:140, batch26, load frames use: 4.91754293442s
step 6465/400000: train loss: 0.176502
epoch:140, batch27, load frames use: 3.52631998062s
step 6466/400000: train loss: 0.188064
epoch:140, batch28, load frames use: 5.6886639595s
step 6467/400000: train loss: 0.163210
epoch:140, batch29, load frames use: 5.5648419857s
step 6468/400000: train loss: 0.167611
epoch:140, batch30, load frames use: 4.13971614838s
step 6469/400000: train loss: 0.189061
epoch:140, batch31, load frames use: 4.63967704773s
step 6470/400000: train loss: 0.171206
epoch:140, batch32, load frames use: 3.57855892181s
step 6471/400000: train loss: 0.163197
epoch:140, batch33, load frames use: 3.67841291428s
step 6472/400000: train loss: 0.171351
epoch:140, batch34, load frames use: 4.22569298744s
step 6473/400000: train loss: 0.186086
epoch:140, batch35, load frames use: 4.3548989296s
step 6474/400000: train loss: 0.175562
epoch:140, batch36, load frames use: 5.01954507828s
step 6475/400000: train loss: 0.149643
epoch:140, batch37, load frames use: 4.23064923286s
step 6476/400000: train loss: 0.177772
epoch:140, batch38, load frames use: 4.46639704704s
step 6477/400000: train loss: 0.169424
epoch:140, batch39, load frames use: 3.02362203598s
step 6478/400000: train loss: 0.150121
epoch:140, batch40, load frames use: 3.99536681175s
step 6479/400000: train loss: 0.167516
epoch:140, batch41, load frames use: 3.9841709137s
step 6480/400000: train loss: 0.176443
epoch:140, batch42, load frames use: 3.97639489174s
step 6481/400000: train loss: 0.141262
epoch:140, batch43, load frames use: 3.32872915268s
step 6482/400000: train loss: 0.178699
epoch:140, batch44, load frames use: 4.35342407227s
step 6483/400000: train loss: 0.169240
epoch:140, batch45, load frames use: 3.388890028s
step 6484/400000: train loss: 0.167737
epoch:140, batch46, load frames use: 4.46363806725s
step 6485/400000: train loss: 0.179315
epoch:141, batch1, load frames use: 5.98112082481s
step 6486/400000: train loss: 0.169626
epoch:141, batch2, load frames use: 3.69531083107s
step 6487/400000: train loss: 0.177796
epoch:141, batch3, load frames use: 4.25102114677s
step 6488/400000: train loss: 0.155878
epoch:141, batch4, load frames use: 3.55620193481s
step 6489/400000: train loss: 0.153626
epoch:141, batch5, load frames use: 3.88515710831s
step 6490/400000: train loss: 0.169100
epoch:141, batch6, load frames use: 4.00607705116s
step 6491/400000: train loss: 0.170460
epoch:141, batch7, load frames use: 4.4734480381s
step 6492/400000: train loss: 0.142448
epoch:141, batch8, load frames use: 3.4771361351s
step 6493/400000: train loss: 0.148951
epoch:141, batch9, load frames use: 3.8781709671s
step 6494/400000: train loss: 0.175265
epoch:141, batch10, load frames use: 3.39732408524s
step 6495/400000: train loss: 0.172319
epoch:141, batch11, load frames use: 3.50826478004s
step 6496/400000: train loss: 0.181549
epoch:141, batch12, load frames use: 3.77777004242s
step 6497/400000: train loss: 0.167734
epoch:141, batch13, load frames use: 3.66610193253s
step 6498/400000: train loss: 0.163917
epoch:141, batch14, load frames use: 3.2841989994s
step 6499/400000: train loss: 0.160378
epoch:141, batch15, load frames use: 4.11589312553s
step 6500/400000: train loss: 0.177766
step 6500/400000: validation loss: 0.166057
epoch:141, batch16, load frames use: 3.52893805504s
step 6501/400000: train loss: 0.194236
epoch:141, batch17, load frames use: 5.35136890411s
step 6502/400000: train loss: 0.160023
epoch:141, batch18, load frames use: 3.53486394882s
step 6503/400000: train loss: 0.156873
epoch:141, batch19, load frames use: 4.53609895706s
step 6504/400000: train loss: 0.177387
epoch:141, batch20, load frames use: 3.82844305038s
step 6505/400000: train loss: 0.178106
epoch:141, batch21, load frames use: 4.05026602745s
step 6506/400000: train loss: 0.183746
epoch:141, batch22, load frames use: 4.51142001152s
step 6507/400000: train loss: 0.168543
epoch:141, batch23, load frames use: 4.16116499901s
step 6508/400000: train loss: 0.163399
epoch:141, batch24, load frames use: 3.49878001213s
step 6509/400000: train loss: 0.189336
epoch:141, batch25, load frames use: 4.18455481529s
step 6510/400000: train loss: 0.196415
epoch:141, batch26, load frames use: 3.50271892548s
step 6511/400000: train loss: 0.160842
epoch:141, batch27, load frames use: 3.45864701271s
step 6512/400000: train loss: 0.185295
epoch:141, batch28, load frames use: 3.5578379631s
step 6513/400000: train loss: 0.185093
epoch:141, batch29, load frames use: 3.17639994621s
step 6514/400000: train loss: 0.167837
epoch:141, batch30, load frames use: 3.69093513489s
step 6515/400000: train loss: 0.163982
epoch:141, batch31, load frames use: 3.90754294395s
step 6516/400000: train loss: 0.180938
epoch:141, batch32, load frames use: 3.50850296021s
step 6517/400000: train loss: 0.168827
epoch:141, batch33, load frames use: 3.96504497528s
step 6518/400000: train loss: 0.192008
epoch:141, batch34, load frames use: 3.73697519302s
step 6519/400000: train loss: 0.169167
epoch:141, batch35, load frames use: 3.49710798264s
step 6520/400000: train loss: 0.167016
epoch:141, batch36, load frames use: 4.34437203407s
step 6521/400000: train loss: 0.181123
epoch:141, batch37, load frames use: 4.45248103142s
step 6522/400000: train loss: 0.173460
epoch:141, batch38, load frames use: 3.80962705612s
step 6523/400000: train loss: 0.178895
epoch:141, batch39, load frames use: 3.72356700897s
step 6524/400000: train loss: 0.182999
epoch:141, batch40, load frames use: 4.26656484604s
step 6525/400000: train loss: 0.183641
epoch:141, batch41, load frames use: 3.89989113808s
step 6526/400000: train loss: 0.176347
epoch:141, batch42, load frames use: 3.95181202888s
step 6527/400000: train loss: 0.176359
epoch:141, batch43, load frames use: 3.63941788673s
step 6528/400000: train loss: 0.185692
epoch:141, batch44, load frames use: 3.61429715157s
step 6529/400000: train loss: 0.194285
epoch:141, batch45, load frames use: 3.47166705132s
step 6530/400000: train loss: 0.188451
epoch:141, batch46, load frames use: 4.41351699829s
step 6531/400000: train loss: 0.195525
epoch:142, batch1, load frames use: 4.13719415665s
step 6532/400000: train loss: 0.206927
epoch:142, batch2, load frames use: 4.26304101944s
step 6533/400000: train loss: 0.166905
epoch:142, batch3, load frames use: 4.12316322327s
step 6534/400000: train loss: 0.183340
epoch:142, batch4, load frames use: 3.83727908134s
step 6535/400000: train loss: 0.156561
epoch:142, batch5, load frames use: 3.9522550106s
step 6536/400000: train loss: 0.195078
epoch:142, batch6, load frames use: 3.42517995834s
step 6537/400000: train loss: 0.173901
epoch:142, batch7, load frames use: 3.66921782494s
step 6538/400000: train loss: 0.163192
epoch:142, batch8, load frames use: 3.6069419384s
step 6539/400000: train loss: 0.188436
epoch:142, batch9, load frames use: 3.36853098869s
step 6540/400000: train loss: 0.195866
epoch:142, batch10, load frames use: 3.60872411728s
step 6541/400000: train loss: 0.195911
epoch:142, batch11, load frames use: 4.29661107063s
step 6542/400000: train loss: 0.173036
epoch:142, batch12, load frames use: 3.68616700172s
step 6543/400000: train loss: 0.190161
epoch:142, batch13, load frames use: 3.70266509056s
step 6544/400000: train loss: 0.180359
epoch:142, batch14, load frames use: 3.66028094292s
step 6545/400000: train loss: 0.174900
epoch:142, batch15, load frames use: 3.35615587234s
step 6546/400000: train loss: 0.188277
epoch:142, batch16, load frames use: 4.80886602402s
step 6547/400000: train loss: 0.180889
epoch:142, batch17, load frames use: 4.00021004677s
step 6548/400000: train loss: 0.191105
epoch:142, batch18, load frames use: 3.2405629158s
step 6549/400000: train loss: 0.172443
epoch:142, batch19, load frames use: 4.46225309372s
step 6550/400000: train loss: 0.180941
epoch:142, batch20, load frames use: 3.87895703316s
step 6551/400000: train loss: 0.187278
epoch:142, batch21, load frames use: 3.54010415077s
step 6552/400000: train loss: 0.163112
epoch:142, batch22, load frames use: 4.78285098076s
step 6553/400000: train loss: 0.191444
epoch:142, batch23, load frames use: 3.50884103775s
step 6554/400000: train loss: 0.168511
epoch:142, batch24, load frames use: 3.6364338398s
step 6555/400000: train loss: 0.188047
epoch:142, batch25, load frames use: 5.46054315567s
step 6556/400000: train loss: 0.176932
epoch:142, batch26, load frames use: 4.51555299759s
step 6557/400000: train loss: 0.166629
epoch:142, batch27, load frames use: 4.20774412155s
step 6558/400000: train loss: 0.160115
epoch:142, batch28, load frames use: 3.65943717957s
step 6559/400000: train loss: 0.162677
epoch:142, batch29, load frames use: 3.46384882927s
step 6560/400000: train loss: 0.184394
epoch:142, batch30, load frames use: 3.08242797852s
step 6561/400000: train loss: 0.190664
epoch:142, batch31, load frames use: 3.69727110863s
step 6562/400000: train loss: 0.187639
epoch:142, batch32, load frames use: 3.94270992279s
step 6563/400000: train loss: 0.175830
epoch:142, batch33, load frames use: 3.76302099228s
step 6564/400000: train loss: 0.188070
epoch:142, batch34, load frames use: 5.04049277306s
step 6565/400000: train loss: 0.182514
epoch:142, batch35, load frames use: 3.99584889412s
step 6566/400000: train loss: 0.170808
epoch:142, batch36, load frames use: 4.83316922188s
step 6567/400000: train loss: 0.166603
epoch:142, batch37, load frames use: 4.41613793373s
step 6568/400000: train loss: 0.167991
epoch:142, batch38, load frames use: 4.50794386864s
step 6569/400000: train loss: 0.173601
epoch:142, batch39, load frames use: 3.92495203018s
step 6570/400000: train loss: 0.188342
epoch:142, batch40, load frames use: 5.09198880196s
step 6571/400000: train loss: 0.163777
epoch:142, batch41, load frames use: 6.1020898819s
step 6572/400000: train loss: 0.180852
epoch:142, batch42, load frames use: 3.26695013046s
step 6573/400000: train loss: 0.164656
epoch:142, batch43, load frames use: 3.64172720909s
step 6574/400000: train loss: 0.175509
epoch:142, batch44, load frames use: 3.69634509087s
step 6575/400000: train loss: 0.153392
epoch:142, batch45, load frames use: 3.19562506676s
step 6576/400000: train loss: 0.156587
epoch:142, batch46, load frames use: 3.66913509369s
step 6577/400000: train loss: 0.182070
epoch:143, batch1, load frames use: 3.31150388718s
step 6578/400000: train loss: 0.173932
epoch:143, batch2, load frames use: 3.38489103317s
step 6579/400000: train loss: 0.190033
epoch:143, batch3, load frames use: 3.66283583641s
step 6580/400000: train loss: 0.168070
epoch:143, batch4, load frames use: 3.82987189293s
step 6581/400000: train loss: 0.168752
epoch:143, batch5, load frames use: 3.76399087906s
step 6582/400000: train loss: 0.167996
epoch:143, batch6, load frames use: 3.94695115089s
step 6583/400000: train loss: 0.200703
epoch:143, batch7, load frames use: 3.44645094872s
step 6584/400000: train loss: 0.171556
epoch:143, batch8, load frames use: 3.55421495438s
step 6585/400000: train loss: 0.158558
epoch:143, batch9, load frames use: 3.69218111038s
step 6586/400000: train loss: 0.166753
epoch:143, batch10, load frames use: 3.58905291557s
step 6587/400000: train loss: 0.161984
epoch:143, batch11, load frames use: 3.87314486504s
step 6588/400000: train loss: 0.150673
epoch:143, batch12, load frames use: 4.19607806206s
step 6589/400000: train loss: 0.179301
epoch:143, batch13, load frames use: 3.74690794945s
step 6590/400000: train loss: 0.165522
epoch:143, batch14, load frames use: 3.92177510262s
step 6591/400000: train loss: 0.156188
epoch:143, batch15, load frames use: 3.42389988899s
step 6592/400000: train loss: 0.160527
epoch:143, batch16, load frames use: 2.99090790749s
step 6593/400000: train loss: 0.174898
epoch:143, batch17, load frames use: 3.30337095261s
step 6594/400000: train loss: 0.151234
epoch:143, batch18, load frames use: 3.83799695969s
step 6595/400000: train loss: 0.171260
epoch:143, batch19, load frames use: 3.36334896088s
step 6596/400000: train loss: 0.158040
epoch:143, batch20, load frames use: 3.33062982559s
step 6597/400000: train loss: 0.164804
epoch:143, batch21, load frames use: 3.41164088249s
step 6598/400000: train loss: 0.178076
epoch:143, batch22, load frames use: 3.28138279915s
step 6599/400000: train loss: 0.174962
epoch:143, batch23, load frames use: 3.39401602745s
step 6600/400000: train loss: 0.169009
step 6600/400000: validation loss: 0.160468
epoch:143, batch24, load frames use: 3.51746892929s
step 6601/400000: train loss: 0.172494
epoch:143, batch25, load frames use: 3.82732009888s
step 6602/400000: train loss: 0.156356
epoch:143, batch26, load frames use: 3.3841240406s
step 6603/400000: train loss: 0.162048
epoch:143, batch27, load frames use: 3.39707899094s
step 6604/400000: train loss: 0.180855
epoch:143, batch28, load frames use: 3.32905101776s
step 6605/400000: train loss: 0.186574
epoch:143, batch29, load frames use: 4.58172917366s
step 6606/400000: train loss: 0.159206
epoch:143, batch30, load frames use: 4.17363190651s
step 6607/400000: train loss: 0.181134
epoch:143, batch31, load frames use: 3.9657459259s
step 6608/400000: train loss: 0.155843
epoch:143, batch32, load frames use: 3.42367982864s
step 6609/400000: train loss: 0.192284
epoch:143, batch33, load frames use: 3.42162799835s
step 6610/400000: train loss: 0.188547
epoch:143, batch34, load frames use: 3.6756811142s
step 6611/400000: train loss: 0.162837
epoch:143, batch35, load frames use: 3.45445203781s
step 6612/400000: train loss: 0.165301
epoch:143, batch36, load frames use: 4.58389902115s
step 6613/400000: train loss: 0.174111
epoch:143, batch37, load frames use: 4.28388285637s
step 6614/400000: train loss: 0.174931
epoch:143, batch38, load frames use: 3.3153860569s
step 6615/400000: train loss: 0.163142
epoch:143, batch39, load frames use: 3.85680890083s
step 6616/400000: train loss: 0.186049
epoch:143, batch40, load frames use: 3.39668893814s
step 6617/400000: train loss: 0.154460
epoch:143, batch41, load frames use: 3.73697090149s
step 6618/400000: train loss: 0.170276
epoch:143, batch42, load frames use: 4.50199317932s
step 6619/400000: train loss: 0.160808
epoch:143, batch43, load frames use: 4.51086711884s
step 6620/400000: train loss: 0.173046
epoch:143, batch44, load frames use: 3.42069506645s
step 6621/400000: train loss: 0.181432
epoch:143, batch45, load frames use: 3.43039798737s
step 6622/400000: train loss: 0.187033
epoch:143, batch46, load frames use: 4.34397006035s
step 6623/400000: train loss: 0.190323
epoch:144, batch1, load frames use: 4.03074097633s
step 6624/400000: train loss: 0.154036
epoch:144, batch2, load frames use: 3.57181191444s
step 6625/400000: train loss: 0.152247
epoch:144, batch3, load frames use: 4.42000293732s
step 6626/400000: train loss: 0.159323
epoch:144, batch4, load frames use: 4.45487117767s
step 6627/400000: train loss: 0.170316
epoch:144, batch5, load frames use: 3.09864211082s
step 6628/400000: train loss: 0.178473
epoch:144, batch6, load frames use: 4.40824508667s
step 6629/400000: train loss: 0.162031
epoch:144, batch7, load frames use: 3.45984005928s
step 6630/400000: train loss: 0.161821
epoch:144, batch8, load frames use: 4.71598601341s
step 6631/400000: train loss: 0.178881
epoch:144, batch9, load frames use: 4.10276889801s
step 6632/400000: train loss: 0.160548
epoch:144, batch10, load frames use: 3.28258609772s
step 6633/400000: train loss: 0.177191
epoch:144, batch11, load frames use: 4.88103604317s
step 6634/400000: train loss: 0.164889
epoch:144, batch12, load frames use: 3.53303098679s
step 6635/400000: train loss: 0.179566
epoch:144, batch13, load frames use: 3.7871658802s
step 6636/400000: train loss: 0.167467
epoch:144, batch14, load frames use: 4.01595497131s
step 6637/400000: train loss: 0.171101
epoch:144, batch15, load frames use: 3.58779406548s
step 6638/400000: train loss: 0.138065
epoch:144, batch16, load frames use: 4.35934710503s
step 6639/400000: train loss: 0.190956
epoch:144, batch17, load frames use: 3.63363289833s
step 6640/400000: train loss: 0.173894
epoch:144, batch18, load frames use: 5.4092168808s
step 6641/400000: train loss: 0.176277
epoch:144, batch19, load frames use: 3.36087608337s
step 6642/400000: train loss: 0.145121
epoch:144, batch20, load frames use: 4.09500002861s
step 6643/400000: train loss: 0.170514
epoch:144, batch21, load frames use: 3.71456003189s
step 6644/400000: train loss: 0.162364
epoch:144, batch22, load frames use: 3.27722096443s
step 6645/400000: train loss: 0.157074
epoch:144, batch23, load frames use: 4.22765898705s
step 6646/400000: train loss: 0.161329
epoch:144, batch24, load frames use: 3.50589513779s
step 6647/400000: train loss: 0.155694
epoch:144, batch25, load frames use: 3.29136800766s
step 6648/400000: train loss: 0.167887
epoch:144, batch26, load frames use: 3.39328098297s
step 6649/400000: train loss: 0.158155
epoch:144, batch27, load frames use: 4.11870002747s
step 6650/400000: train loss: 0.168678
epoch:144, batch28, load frames use: 3.4055352211s
step 6651/400000: train loss: 0.175261
epoch:144, batch29, load frames use: 5.07716584206s
step 6652/400000: train loss: 0.175868
epoch:144, batch30, load frames use: 3.57115793228s
step 6653/400000: train loss: 0.183466
epoch:144, batch31, load frames use: 3.42159104347s
step 6654/400000: train loss: 0.173682
epoch:144, batch32, load frames use: 4.05424904823s
step 6655/400000: train loss: 0.172927
epoch:144, batch33, load frames use: 5.98263096809s
step 6656/400000: train loss: 0.187182
epoch:144, batch34, load frames use: 3.75557899475s
step 6657/400000: train loss: 0.165372
epoch:144, batch35, load frames use: 3.34521102905s
step 6658/400000: train loss: 0.182665
epoch:144, batch36, load frames use: 3.81200408936s
step 6659/400000: train loss: 0.168567
epoch:144, batch37, load frames use: 3.92961907387s
step 6660/400000: train loss: 0.179993
epoch:144, batch38, load frames use: 4.1848552227s
step 6661/400000: train loss: 0.183368
epoch:144, batch39, load frames use: 4.02584695816s
step 6662/400000: train loss: 0.160231
epoch:144, batch40, load frames use: 3.51415801048s
step 6663/400000: train loss: 0.177676
epoch:144, batch41, load frames use: 3.6968870163s
step 6664/400000: train loss: 0.187457
epoch:144, batch42, load frames use: 3.35913014412s
step 6665/400000: train loss: 0.182320
epoch:144, batch43, load frames use: 3.53358793259s
step 6666/400000: train loss: 0.178218
epoch:144, batch44, load frames use: 6.28490710258s
step 6667/400000: train loss: 0.179742
epoch:144, batch45, load frames use: 3.77767395973s
step 6668/400000: train loss: 0.172839
epoch:144, batch46, load frames use: 3.26661205292s
step 6669/400000: train loss: 0.175305
epoch:145, batch1, load frames use: 3.75015091896s
step 6670/400000: train loss: 0.171248
epoch:145, batch2, load frames use: 3.62092399597s
step 6671/400000: train loss: 0.158150
epoch:145, batch3, load frames use: 4.08405280113s
step 6672/400000: train loss: 0.155705
epoch:145, batch4, load frames use: 5.38663387299s
step 6673/400000: train loss: 0.157336
epoch:145, batch5, load frames use: 3.47533202171s
step 6674/400000: train loss: 0.170808
epoch:145, batch6, load frames use: 3.74135494232s
step 6675/400000: train loss: 0.157974
epoch:145, batch7, load frames use: 3.68459105492s
step 6676/400000: train loss: 0.152796
epoch:145, batch8, load frames use: 3.83379793167s
step 6677/400000: train loss: 0.153108
epoch:145, batch9, load frames use: 3.38103914261s
step 6678/400000: train loss: 0.163633
epoch:145, batch10, load frames use: 4.08579897881s
step 6679/400000: train loss: 0.180187
epoch:145, batch11, load frames use: 6.41006708145s
step 6680/400000: train loss: 0.170014
epoch:145, batch12, load frames use: 3.50720477104s
step 6681/400000: train loss: 0.180661
epoch:145, batch13, load frames use: 4.65739798546s
step 6682/400000: train loss: 0.159441
epoch:145, batch14, load frames use: 4.0676009655s
step 6683/400000: train loss: 0.156938
epoch:145, batch15, load frames use: 3.51100802422s
step 6684/400000: train loss: 0.194853
epoch:145, batch16, load frames use: 3.71744394302s
step 6685/400000: train loss: 0.179053
epoch:145, batch17, load frames use: 5.88662791252s
step 6686/400000: train loss: 0.187915
epoch:145, batch18, load frames use: 4.41612696648s
step 6687/400000: train loss: 0.178141
epoch:145, batch19, load frames use: 4.10676813126s
step 6688/400000: train loss: 0.177024
epoch:145, batch20, load frames use: 3.78462004662s
step 6689/400000: train loss: 0.182903
epoch:145, batch21, load frames use: 3.5958340168s
step 6690/400000: train loss: 0.153255
epoch:145, batch22, load frames use: 3.36143898964s
step 6691/400000: train loss: 0.163875
epoch:145, batch23, load frames use: 3.4270131588s
step 6692/400000: train loss: 0.150055
epoch:145, batch24, load frames use: 3.39710116386s
step 6693/400000: train loss: 0.154714
epoch:145, batch25, load frames use: 3.41754293442s
step 6694/400000: train loss: 0.187886
epoch:145, batch26, load frames use: 4.4275200367s
step 6695/400000: train loss: 0.157876
epoch:145, batch27, load frames use: 3.73411893845s
step 6696/400000: train loss: 0.190794
epoch:145, batch28, load frames use: 3.40100598335s
step 6697/400000: train loss: 0.161576
epoch:145, batch29, load frames use: 4.69230413437s
step 6698/400000: train loss: 0.174933
epoch:145, batch30, load frames use: 4.95329904556s
step 6699/400000: train loss: 0.184621
epoch:145, batch31, load frames use: 4.15193605423s
step 6700/400000: train loss: 0.194690
step 6700/400000: validation loss: 0.180008
epoch:145, batch32, load frames use: 3.81835985184s
step 6701/400000: train loss: 0.179517
epoch:145, batch33, load frames use: 3.36380887032s
step 6702/400000: train loss: 0.177687
epoch:145, batch34, load frames use: 4.02175188065s
step 6703/400000: train loss: 0.176202
epoch:145, batch35, load frames use: 4.69998812675s
step 6704/400000: train loss: 0.184386
epoch:145, batch36, load frames use: 3.84854388237s
step 6705/400000: train loss: 0.156451
epoch:145, batch37, load frames use: 3.50940179825s
step 6706/400000: train loss: 0.148619
epoch:145, batch38, load frames use: 3.33643293381s
step 6707/400000: train loss: 0.180850
epoch:145, batch39, load frames use: 3.84480381012s
step 6708/400000: train loss: 0.173619
epoch:145, batch40, load frames use: 4.23534584045s
step 6709/400000: train loss: 0.168986
epoch:145, batch41, load frames use: 4.33699083328s
step 6710/400000: train loss: 0.171661
epoch:145, batch42, load frames use: 3.27858710289s
step 6711/400000: train loss: 0.180499
epoch:145, batch43, load frames use: 3.34225296974s
step 6712/400000: train loss: 0.192726
epoch:145, batch44, load frames use: 3.46586084366s
step 6713/400000: train loss: 0.152744
epoch:145, batch45, load frames use: 4.20467686653s
step 6714/400000: train loss: 0.160379
epoch:145, batch46, load frames use: 3.6606221199s
step 6715/400000: train loss: 0.175503
epoch:146, batch1, load frames use: 3.91453099251s
step 6716/400000: train loss: 0.198595
epoch:146, batch2, load frames use: 3.58807682991s
step 6717/400000: train loss: 0.152019
epoch:146, batch3, load frames use: 3.67570495605s
step 6718/400000: train loss: 0.155824
epoch:146, batch4, load frames use: 3.51289105415s
step 6719/400000: train loss: 0.174433
epoch:146, batch5, load frames use: 3.402739048s
step 6720/400000: train loss: 0.167435
epoch:146, batch6, load frames use: 4.0835211277s
step 6721/400000: train loss: 0.152324
epoch:146, batch7, load frames use: 3.93544697762s
step 6722/400000: train loss: 0.157049
epoch:146, batch8, load frames use: 3.72937703133s
step 6723/400000: train loss: 0.171053
epoch:146, batch9, load frames use: 4.53291010857s
step 6724/400000: train loss: 0.159542
epoch:146, batch10, load frames use: 3.58103513718s
step 6725/400000: train loss: 0.181842
epoch:146, batch11, load frames use: 4.03224110603s
step 6726/400000: train loss: 0.158242
epoch:146, batch12, load frames use: 3.67566180229s
step 6727/400000: train loss: 0.169751
epoch:146, batch13, load frames use: 3.90475702286s
step 6728/400000: train loss: 0.162224
epoch:146, batch14, load frames use: 3.36918592453s
step 6729/400000: train loss: 0.182070
epoch:146, batch15, load frames use: 3.81150603294s
step 6730/400000: train loss: 0.176673
epoch:146, batch16, load frames use: 4.49489092827s
step 6731/400000: train loss: 0.145543
epoch:146, batch17, load frames use: 3.78266596794s
step 6732/400000: train loss: 0.181957
epoch:146, batch18, load frames use: 3.51794695854s
step 6733/400000: train loss: 0.164344
epoch:146, batch19, load frames use: 3.44684100151s
step 6734/400000: train loss: 0.173039
epoch:146, batch20, load frames use: 4.07553195953s
step 6735/400000: train loss: 0.193091
epoch:146, batch21, load frames use: 3.44783616066s
step 6736/400000: train loss: 0.199183
epoch:146, batch22, load frames use: 3.63456988335s
step 6737/400000: train loss: 0.184407
epoch:146, batch23, load frames use: 4.77745318413s
step 6738/400000: train loss: 0.181879
epoch:146, batch24, load frames use: 4.88003897667s
step 6739/400000: train loss: 0.157198
epoch:146, batch25, load frames use: 3.70339202881s
step 6740/400000: train loss: 0.167765
epoch:146, batch26, load frames use: 5.06101799011s
step 6741/400000: train loss: 0.179762
epoch:146, batch27, load frames use: 3.38768815994s
step 6742/400000: train loss: 0.177005
epoch:146, batch28, load frames use: 3.75945019722s
step 6743/400000: train loss: 0.181913
epoch:146, batch29, load frames use: 3.23080205917s
step 6744/400000: train loss: 0.167985
epoch:146, batch30, load frames use: 3.32330513s
step 6745/400000: train loss: 0.168687
epoch:146, batch31, load frames use: 4.0990819931s
step 6746/400000: train loss: 0.177410
epoch:146, batch32, load frames use: 3.56185984612s
step 6747/400000: train loss: 0.164689
epoch:146, batch33, load frames use: 3.73333501816s
step 6748/400000: train loss: 0.178777
epoch:146, batch34, load frames use: 3.99440312386s
step 6749/400000: train loss: 0.176657
epoch:146, batch35, load frames use: 3.85486698151s
step 6750/400000: train loss: 0.184488
epoch:146, batch36, load frames use: 4.81038188934s
step 6751/400000: train loss: 0.178556
epoch:146, batch37, load frames use: 5.44322395325s
step 6752/400000: train loss: 0.169679
epoch:146, batch38, load frames use: 3.4668200016s
step 6753/400000: train loss: 0.170916
epoch:146, batch39, load frames use: 4.07014894485s
step 6754/400000: train loss: 0.170233
epoch:146, batch40, load frames use: 3.67085504532s
step 6755/400000: train loss: 0.179452
epoch:146, batch41, load frames use: 4.66562414169s
step 6756/400000: train loss: 0.170726
epoch:146, batch42, load frames use: 4.09759783745s
step 6757/400000: train loss: 0.176864
epoch:146, batch43, load frames use: 3.13804507256s
step 6758/400000: train loss: 0.170592
epoch:146, batch44, load frames use: 3.74799609184s
step 6759/400000: train loss: 0.165575
epoch:146, batch45, load frames use: 3.48576688766s
step 6760/400000: train loss: 0.182381
epoch:146, batch46, load frames use: 4.84581708908s
step 6761/400000: train loss: 0.170808
epoch:147, batch1, load frames use: 4.30771708488s
step 6762/400000: train loss: 0.165976
epoch:147, batch2, load frames use: 3.58720993996s
step 6763/400000: train loss: 0.175114
epoch:147, batch3, load frames use: 3.82368302345s
step 6764/400000: train loss: 0.176082
epoch:147, batch4, load frames use: 4.44271612167s
step 6765/400000: train loss: 0.163366
epoch:147, batch5, load frames use: 4.14786291122s
step 6766/400000: train loss: 0.171914
epoch:147, batch6, load frames use: 4.24111294746s
step 6767/400000: train loss: 0.167753
epoch:147, batch7, load frames use: 3.37159585953s
step 6768/400000: train loss: 0.151766
epoch:147, batch8, load frames use: 3.29991793633s
step 6769/400000: train loss: 0.163088
epoch:147, batch9, load frames use: 5.32523488998s
step 6770/400000: train loss: 0.161233
epoch:147, batch10, load frames use: 3.58563613892s
step 6771/400000: train loss: 0.169266
epoch:147, batch11, load frames use: 4.10672593117s
step 6772/400000: train loss: 0.165359
epoch:147, batch12, load frames use: 3.36485481262s
step 6773/400000: train loss: 0.163742
epoch:147, batch13, load frames use: 3.47895908356s
step 6774/400000: train loss: 0.174401
epoch:147, batch14, load frames use: 3.28343200684s
step 6775/400000: train loss: 0.169431
epoch:147, batch15, load frames use: 3.25295186043s
step 6776/400000: train loss: 0.169341
epoch:147, batch16, load frames use: 4.44479894638s
step 6777/400000: train loss: 0.179778
epoch:147, batch17, load frames use: 4.86962294579s
step 6778/400000: train loss: 0.160153
epoch:147, batch18, load frames use: 3.62965893745s
step 6779/400000: train loss: 0.179575
epoch:147, batch19, load frames use: 4.28733110428s
step 6780/400000: train loss: 0.172537
epoch:147, batch20, load frames use: 4.60541796684s
step 6781/400000: train loss: 0.171731
epoch:147, batch21, load frames use: 4.12501502037s
step 6782/400000: train loss: 0.174130
epoch:147, batch22, load frames use: 3.6085050106s
step 6783/400000: train loss: 0.163348
epoch:147, batch23, load frames use: 3.41207003593s
step 6784/400000: train loss: 0.200991
epoch:147, batch24, load frames use: 3.39139699936s
step 6785/400000: train loss: 0.176894
epoch:147, batch25, load frames use: 3.490639925s
step 6786/400000: train loss: 0.159351
epoch:147, batch26, load frames use: 3.8166179657s
step 6787/400000: train loss: 0.174003
epoch:147, batch27, load frames use: 5.49584698677s
step 6788/400000: train loss: 0.163939
epoch:147, batch28, load frames use: 3.45227599144s
step 6789/400000: train loss: 0.180477
epoch:147, batch29, load frames use: 3.38627004623s
step 6790/400000: train loss: 0.166566
epoch:147, batch30, load frames use: 3.51502203941s
step 6791/400000: train loss: 0.180969
epoch:147, batch31, load frames use: 4.02392315865s
step 6792/400000: train loss: 0.176543
epoch:147, batch32, load frames use: 3.44077110291s
step 6793/400000: train loss: 0.173054
epoch:147, batch33, load frames use: 3.6838350296s
step 6794/400000: train loss: 0.168389
epoch:147, batch34, load frames use: 3.27501487732s
step 6795/400000: train loss: 0.165796
epoch:147, batch35, load frames use: 3.2693400383s
step 6796/400000: train loss: 0.177756
epoch:147, batch36, load frames use: 3.19801712036s
step 6797/400000: train loss: 0.180880
epoch:147, batch37, load frames use: 3.51922297478s
step 6798/400000: train loss: 0.156112
epoch:147, batch38, load frames use: 6.04273915291s
step 6799/400000: train loss: 0.167378
epoch:147, batch39, load frames use: 3.24292993546s
step 6800/400000: train loss: 0.168646
step 6800/400000: validation loss: 0.164211
epoch:147, batch40, load frames use: 4.79164719582s
step 6801/400000: train loss: 0.171272
epoch:147, batch41, load frames use: 3.62082695961s
step 6802/400000: train loss: 0.187044
epoch:147, batch42, load frames use: 3.44368600845s
step 6803/400000: train loss: 0.157630
epoch:147, batch43, load frames use: 3.78402709961s
step 6804/400000: train loss: 0.164888
epoch:147, batch44, load frames use: 4.47979402542s
step 6805/400000: train loss: 0.182412
epoch:147, batch45, load frames use: 4.17776799202s
step 6806/400000: train loss: 0.183220
epoch:147, batch46, load frames use: 3.60444903374s
step 6807/400000: train loss: 0.176229
epoch:148, batch1, load frames use: 4.15451002121s
step 6808/400000: train loss: 0.161365
epoch:148, batch2, load frames use: 5.41283607483s
step 6809/400000: train loss: 0.175188
epoch:148, batch3, load frames use: 3.37511110306s
step 6810/400000: train loss: 0.169332
epoch:148, batch4, load frames use: 3.3131301403s
step 6811/400000: train loss: 0.163564
epoch:148, batch5, load frames use: 3.61559987068s
step 6812/400000: train loss: 0.183498
epoch:148, batch6, load frames use: 3.34434103966s
step 6813/400000: train loss: 0.159390
epoch:148, batch7, load frames use: 3.52652716637s
step 6814/400000: train loss: 0.167895
epoch:148, batch8, load frames use: 3.79053807259s
step 6815/400000: train loss: 0.187003
epoch:148, batch9, load frames use: 4.16040897369s
step 6816/400000: train loss: 0.164515
epoch:148, batch10, load frames use: 4.18639588356s
step 6817/400000: train loss: 0.164179
epoch:148, batch11, load frames use: 3.78766489029s
step 6818/400000: train loss: 0.165606
epoch:148, batch12, load frames use: 3.53253006935s
step 6819/400000: train loss: 0.179114
epoch:148, batch13, load frames use: 3.72309398651s
step 6820/400000: train loss: 0.184061
epoch:148, batch14, load frames use: 3.40597701073s
step 6821/400000: train loss: 0.188637
epoch:148, batch15, load frames use: 3.96898794174s
step 6822/400000: train loss: 0.185088
epoch:148, batch16, load frames use: 3.37853002548s
step 6823/400000: train loss: 0.175303
epoch:148, batch17, load frames use: 3.73844099045s
step 6824/400000: train loss: 0.164998
epoch:148, batch18, load frames use: 3.65003991127s
step 6825/400000: train loss: 0.180450
epoch:148, batch19, load frames use: 3.9013478756s
step 6826/400000: train loss: 0.174111
epoch:148, batch20, load frames use: 3.74573802948s
step 6827/400000: train loss: 0.170860
epoch:148, batch21, load frames use: 3.75773596764s
step 6828/400000: train loss: 0.179514
epoch:148, batch22, load frames use: 6.19488501549s
step 6829/400000: train loss: 0.191833
epoch:148, batch23, load frames use: 3.31959700584s
step 6830/400000: train loss: 0.172859
epoch:148, batch24, load frames use: 4.02793598175s
step 6831/400000: train loss: 0.161599
epoch:148, batch25, load frames use: 3.35746192932s
step 6832/400000: train loss: 0.152841
epoch:148, batch26, load frames use: 3.7416779995s
step 6833/400000: train loss: 0.186547
epoch:148, batch27, load frames use: 3.49742007256s
step 6834/400000: train loss: 0.174582
epoch:148, batch28, load frames use: 3.12963294983s
step 6835/400000: train loss: 0.174375
epoch:148, batch29, load frames use: 3.88147711754s
step 6836/400000: train loss: 0.161115
epoch:148, batch30, load frames use: 3.47211289406s
step 6837/400000: train loss: 0.153406
epoch:148, batch31, load frames use: 3.93118906021s
step 6838/400000: train loss: 0.157528
epoch:148, batch32, load frames use: 4.58446002007s
step 6839/400000: train loss: 0.151607
epoch:148, batch33, load frames use: 3.57168197632s
step 6840/400000: train loss: 0.171049
epoch:148, batch34, load frames use: 2.89089608192s
step 6841/400000: train loss: 0.169262
epoch:148, batch35, load frames use: 4.03921794891s
step 6842/400000: train loss: 0.169164
epoch:148, batch36, load frames use: 3.43289494514s
step 6843/400000: train loss: 0.161206
epoch:148, batch37, load frames use: 3.78413510323s
step 6844/400000: train loss: 0.163359
epoch:148, batch38, load frames use: 5.11723899841s
step 6845/400000: train loss: 0.185756
epoch:148, batch39, load frames use: 4.20753002167s
step 6846/400000: train loss: 0.167110
epoch:148, batch40, load frames use: 4.43113303185s
step 6847/400000: train loss: 0.182900
epoch:148, batch41, load frames use: 3.68639397621s
step 6848/400000: train loss: 0.169866
epoch:148, batch42, load frames use: 3.42979383469s
step 6849/400000: train loss: 0.163007
epoch:148, batch43, load frames use: 4.03459119797s
step 6850/400000: train loss: 0.175279
epoch:148, batch44, load frames use: 3.52404713631s
step 6851/400000: train loss: 0.173449
epoch:148, batch45, load frames use: 4.15640497208s
step 6852/400000: train loss: 0.170327
epoch:148, batch46, load frames use: 4.13234090805s
step 6853/400000: train loss: 0.190211
epoch:149, batch1, load frames use: 4.58182907104s
step 6854/400000: train loss: 0.161259
epoch:149, batch2, load frames use: 4.29740977287s
step 6855/400000: train loss: 0.180325
epoch:149, batch3, load frames use: 4.45378398895s
step 6856/400000: train loss: 0.180910
epoch:149, batch4, load frames use: 3.35399699211s
step 6857/400000: train loss: 0.172870
epoch:149, batch5, load frames use: 4.05864214897s
step 6858/400000: train loss: 0.168415
epoch:149, batch6, load frames use: 4.70954322815s
step 6859/400000: train loss: 0.167963
epoch:149, batch7, load frames use: 4.14290595055s
step 6860/400000: train loss: 0.171540
epoch:149, batch8, load frames use: 3.53723192215s
step 6861/400000: train loss: 0.175310
epoch:149, batch9, load frames use: 4.22486710548s
step 6862/400000: train loss: 0.171958
epoch:149, batch10, load frames use: 4.2575469017s
step 6863/400000: train loss: 0.211268
epoch:149, batch11, load frames use: 3.40115189552s
step 6864/400000: train loss: 0.190152
epoch:149, batch12, load frames use: 3.34764909744s
step 6865/400000: train loss: 0.161851
epoch:149, batch13, load frames use: 3.74826097488s
step 6866/400000: train loss: 0.203649
epoch:149, batch14, load frames use: 3.68251299858s
step 6867/400000: train loss: 0.158076
epoch:149, batch15, load frames use: 4.26572704315s
step 6868/400000: train loss: 0.189628
epoch:149, batch16, load frames use: 3.57792019844s
step 6869/400000: train loss: 0.184799
epoch:149, batch17, load frames use: 3.37913990021s
step 6870/400000: train loss: 0.162779
epoch:149, batch18, load frames use: 3.49983406067s
step 6871/400000: train loss: 0.160687
epoch:149, batch19, load frames use: 4.87419009209s
step 6872/400000: train loss: 0.173604
epoch:149, batch20, load frames use: 4.1428129673s
step 6873/400000: train loss: 0.189004
epoch:149, batch21, load frames use: 3.11795401573s
step 6874/400000: train loss: 0.150557
epoch:149, batch22, load frames use: 3.90551590919s
step 6875/400000: train loss: 0.189671
epoch:149, batch23, load frames use: 3.58399701118s
step 6876/400000: train loss: 0.161079
epoch:149, batch24, load frames use: 4.57376003265s
step 6877/400000: train loss: 0.173455
epoch:149, batch25, load frames use: 3.26489686966s
step 6878/400000: train loss: 0.173385
epoch:149, batch26, load frames use: 4.19395518303s
step 6879/400000: train loss: 0.163113
epoch:149, batch27, load frames use: 3.4570851326s
step 6880/400000: train loss: 0.161053
epoch:149, batch28, load frames use: 3.66020989418s
step 6881/400000: train loss: 0.170336
epoch:149, batch29, load frames use: 3.45285391808s
step 6882/400000: train loss: 0.177717
epoch:149, batch30, load frames use: 4.8768029213s
step 6883/400000: train loss: 0.192454
epoch:149, batch31, load frames use: 3.62009596825s
step 6884/400000: train loss: 0.176190
epoch:149, batch32, load frames use: 4.4861869812s
step 6885/400000: train loss: 0.168684
epoch:149, batch33, load frames use: 3.46400284767s
step 6886/400000: train loss: 0.173875
epoch:149, batch34, load frames use: 3.91946196556s
step 6887/400000: train loss: 0.151256
epoch:149, batch35, load frames use: 5.15872597694s
step 6888/400000: train loss: 0.164944
epoch:149, batch36, load frames use: 3.42181611061s
step 6889/400000: train loss: 0.173691
epoch:149, batch37, load frames use: 3.3898768425s
step 6890/400000: train loss: 0.154231
epoch:149, batch38, load frames use: 3.85462594032s
step 6891/400000: train loss: 0.165528
epoch:149, batch39, load frames use: 4.39707803726s
step 6892/400000: train loss: 0.169820
epoch:149, batch40, load frames use: 3.9968559742s
step 6893/400000: train loss: 0.175157
epoch:149, batch41, load frames use: 3.41768312454s
step 6894/400000: train loss: 0.176769
epoch:149, batch42, load frames use: 5.29275202751s
step 6895/400000: train loss: 0.158080
epoch:149, batch43, load frames use: 5.38251495361s
step 6896/400000: train loss: 0.169024
epoch:149, batch44, load frames use: 4.21860408783s
step 6897/400000: train loss: 0.154887
epoch:149, batch45, load frames use: 3.37628388405s
step 6898/400000: train loss: 0.187909
epoch:149, batch46, load frames use: 3.67833399773s
step 6899/400000: train loss: 0.149825
epoch:150, batch1, load frames use: 5.80606198311s
step 6900/400000: train loss: 0.169328
step 6900/400000: validation loss: 0.163022
epoch:150, batch2, load frames use: 3.80965805054s
step 6901/400000: train loss: 0.154862
epoch:150, batch3, load frames use: 4.00019192696s
step 6902/400000: train loss: 0.164925
epoch:150, batch4, load frames use: 3.28934812546s
step 6903/400000: train loss: 0.175410
epoch:150, batch5, load frames use: 3.28323912621s
step 6904/400000: train loss: 0.168579
epoch:150, batch6, load frames use: 3.34356498718s
step 6905/400000: train loss: 0.169881
epoch:150, batch7, load frames use: 3.26571917534s
step 6906/400000: train loss: 0.162246
epoch:150, batch8, load frames use: 3.61215519905s
step 6907/400000: train loss: 0.185071
epoch:150, batch9, load frames use: 3.53314781189s
step 6908/400000: train loss: 0.165872
epoch:150, batch10, load frames use: 3.44601392746s
step 6909/400000: train loss: 0.149497
epoch:150, batch11, load frames use: 3.4309668541s
step 6910/400000: train loss: 0.164989
epoch:150, batch12, load frames use: 3.18800592422s
step 6911/400000: train loss: 0.184750
epoch:150, batch13, load frames use: 3.74968910217s
step 6912/400000: train loss: 0.161801
epoch:150, batch14, load frames use: 3.40359091759s
step 6913/400000: train loss: 0.160646
epoch:150, batch15, load frames use: 3.74735593796s
step 6914/400000: train loss: 0.165147
epoch:150, batch16, load frames use: 6.12782597542s
step 6915/400000: train loss: 0.186587
epoch:150, batch17, load frames use: 3.35328197479s
step 6916/400000: train loss: 0.161844
epoch:150, batch18, load frames use: 3.70328497887s
step 6917/400000: train loss: 0.152044
epoch:150, batch19, load frames use: 3.30834698677s
step 6918/400000: train loss: 0.162481
epoch:150, batch20, load frames use: 3.2852768898s
step 6919/400000: train loss: 0.166754
epoch:150, batch21, load frames use: 4.19379401207s
step 6920/400000: train loss: 0.154624
epoch:150, batch22, load frames use: 4.13965010643s
step 6921/400000: train loss: 0.183373
epoch:150, batch23, load frames use: 3.57981991768s
step 6922/400000: train loss: 0.159570
epoch:150, batch24, load frames use: 3.6087949276s
step 6923/400000: train loss: 0.161857
epoch:150, batch25, load frames use: 3.30104112625s
step 6924/400000: train loss: 0.160178
epoch:150, batch26, load frames use: 3.90146303177s
step 6925/400000: train loss: 0.180393
epoch:150, batch27, load frames use: 4.56145906448s
step 6926/400000: train loss: 0.166437
epoch:150, batch28, load frames use: 5.5506310463s
step 6927/400000: train loss: 0.192751
epoch:150, batch29, load frames use: 3.36213707924s
step 6928/400000: train loss: 0.170232
epoch:150, batch30, load frames use: 3.64032196999s
step 6929/400000: train loss: 0.167127
epoch:150, batch31, load frames use: 3.72090888023s
step 6930/400000: train loss: 0.147288
epoch:150, batch32, load frames use: 3.30308413506s
step 6931/400000: train loss: 0.184270
epoch:150, batch33, load frames use: 3.74523186684s
step 6932/400000: train loss: 0.179794
epoch:150, batch34, load frames use: 3.48054289818s
step 6933/400000: train loss: 0.170935
epoch:150, batch35, load frames use: 3.26156306267s
step 6934/400000: train loss: 0.169929
epoch:150, batch36, load frames use: 4.02909207344s
step 6935/400000: train loss: 0.164074
epoch:150, batch37, load frames use: 3.24344396591s
step 6936/400000: train loss: 0.173279
epoch:150, batch38, load frames use: 5.95440888405s
step 6937/400000: train loss: 0.170838
epoch:150, batch39, load frames use: 3.32330489159s
step 6938/400000: train loss: 0.182788
epoch:150, batch40, load frames use: 4.03874897957s
step 6939/400000: train loss: 0.155502
epoch:150, batch41, load frames use: 4.02836298943s
step 6940/400000: train loss: 0.170705
epoch:150, batch42, load frames use: 3.30955600739s
step 6941/400000: train loss: 0.160522
epoch:150, batch43, load frames use: 3.62379312515s
step 6942/400000: train loss: 0.164900
epoch:150, batch44, load frames use: 3.53860807419s
step 6943/400000: train loss: 0.181831
epoch:150, batch45, load frames use: 3.41626095772s
step 6944/400000: train loss: 0.181299
epoch:150, batch46, load frames use: 3.64891600609s
step 6945/400000: train loss: 0.170313
epoch:151, batch1, load frames use: 3.46497702599s
step 6946/400000: train loss: 0.185831
epoch:151, batch2, load frames use: 3.59965395927s
step 6947/400000: train loss: 0.171051
epoch:151, batch3, load frames use: 4.42158198357s
step 6948/400000: train loss: 0.165325
epoch:151, batch4, load frames use: 4.39948010445s
step 6949/400000: train loss: 0.169934
epoch:151, batch5, load frames use: 3.88751792908s
step 6950/400000: train loss: 0.156648
epoch:151, batch6, load frames use: 4.25611281395s
step 6951/400000: train loss: 0.182758
epoch:151, batch7, load frames use: 3.58594107628s
step 6952/400000: train loss: 0.157793
epoch:151, batch8, load frames use: 4.00379800797s
step 6953/400000: train loss: 0.176487
epoch:151, batch9, load frames use: 3.49412989616s
step 6954/400000: train loss: 0.181095
epoch:151, batch10, load frames use: 3.44304895401s
step 6955/400000: train loss: 0.170894
epoch:151, batch11, load frames use: 3.70419096947s
step 6956/400000: train loss: 0.190500
epoch:151, batch12, load frames use: 4.60723805428s
step 6957/400000: train loss: 0.169879
epoch:151, batch13, load frames use: 3.44834804535s
step 6958/400000: train loss: 0.148421
epoch:151, batch14, load frames use: 3.55465102196s
step 6959/400000: train loss: 0.159880
epoch:151, batch15, load frames use: 3.28704595566s
step 6960/400000: train loss: 0.148907
epoch:151, batch16, load frames use: 3.33063387871s
step 6961/400000: train loss: 0.140673
epoch:151, batch17, load frames use: 3.7740008831s
step 6962/400000: train loss: 0.184291
epoch:151, batch18, load frames use: 3.83100509644s
step 6963/400000: train loss: 0.157237
epoch:151, batch19, load frames use: 3.40873217583s
step 6964/400000: train loss: 0.196271
epoch:151, batch20, load frames use: 3.58649802208s
step 6965/400000: train loss: 0.154706
epoch:151, batch21, load frames use: 3.75843405724s
step 6966/400000: train loss: 0.185538
epoch:151, batch22, load frames use: 3.75091600418s
step 6967/400000: train loss: 0.169406
epoch:151, batch23, load frames use: 3.26110577583s
step 6968/400000: train loss: 0.167862
epoch:151, batch24, load frames use: 3.52507996559s
step 6969/400000: train loss: 0.162348
epoch:151, batch25, load frames use: 4.91965103149s
step 6970/400000: train loss: 0.168965
epoch:151, batch26, load frames use: 3.40415596962s
step 6971/400000: train loss: 0.184668
epoch:151, batch27, load frames use: 3.83732700348s
step 6972/400000: train loss: 0.168024
epoch:151, batch28, load frames use: 3.59111189842s
step 6973/400000: train loss: 0.154956
epoch:151, batch29, load frames use: 6.17291402817s
step 6974/400000: train loss: 0.184379
epoch:151, batch30, load frames use: 4.55238604546s
step 6975/400000: train loss: 0.167172
epoch:151, batch31, load frames use: 3.87189602852s
step 6976/400000: train loss: 0.165253
epoch:151, batch32, load frames use: 3.35938096046s
step 6977/400000: train loss: 0.183447
epoch:151, batch33, load frames use: 3.75055193901s
step 6978/400000: train loss: 0.176428
epoch:151, batch34, load frames use: 3.70728707314s
step 6979/400000: train loss: 0.180707
epoch:151, batch35, load frames use: 3.49226999283s
step 6980/400000: train loss: 0.174003
epoch:151, batch36, load frames use: 4.2848880291s
step 6981/400000: train loss: 0.164772
epoch:151, batch37, load frames use: 3.47815704346s
step 6982/400000: train loss: 0.170498
epoch:151, batch38, load frames use: 3.63669800758s
step 6983/400000: train loss: 0.192283
epoch:151, batch39, load frames use: 3.75378203392s
step 6984/400000: train loss: 0.166637
epoch:151, batch40, load frames use: 3.51704001427s
step 6985/400000: train loss: 0.169934
epoch:151, batch41, load frames use: 3.31270599365s
step 6986/400000: train loss: 0.183426
epoch:151, batch42, load frames use: 3.69800019264s
step 6987/400000: train loss: 0.186633
epoch:151, batch43, load frames use: 3.93831396103s
step 6988/400000: train loss: 0.170497
epoch:151, batch44, load frames use: 3.5455031395s
step 6989/400000: train loss: 0.168605
epoch:151, batch45, load frames use: 3.60905313492s
step 6990/400000: train loss: 0.166894
epoch:151, batch46, load frames use: 3.32313609123s
step 6991/400000: train loss: 0.165252
epoch:152, batch1, load frames use: 3.36708712578s
step 6992/400000: train loss: 0.152797
epoch:152, batch2, load frames use: 4.99061799049s
step 6993/400000: train loss: 0.160931
epoch:152, batch3, load frames use: 3.93526101112s
step 6994/400000: train loss: 0.152361
epoch:152, batch4, load frames use: 6.28877210617s
step 6995/400000: train loss: 0.166119
epoch:152, batch5, load frames use: 4.14843201637s
step 6996/400000: train loss: 0.191941
epoch:152, batch6, load frames use: 3.51498007774s
step 6997/400000: train loss: 0.164021
epoch:152, batch7, load frames use: 4.20536589622s
step 6998/400000: train loss: 0.151947
epoch:152, batch8, load frames use: 3.50325703621s
step 6999/400000: train loss: 0.184173
epoch:152, batch9, load frames use: 3.33686614037s
step 7000/400000: train loss: 0.170838
step 7000/400000: validation loss: 0.156788
epoch:152, batch10, load frames use: 4.05058002472s
step 7001/400000: train loss: 0.195630
epoch:152, batch11, load frames use: 4.08932590485s
step 7002/400000: train loss: 0.196018
epoch:152, batch12, load frames use: 4.16225481033s
step 7003/400000: train loss: 0.178424
epoch:152, batch13, load frames use: 3.81619501114s
step 7004/400000: train loss: 0.164443
epoch:152, batch14, load frames use: 3.8598549366s
step 7005/400000: train loss: 0.158431
epoch:152, batch15, load frames use: 3.42189192772s
step 7006/400000: train loss: 0.196206
epoch:152, batch16, load frames use: 3.33715105057s
step 7007/400000: train loss: 0.153649
epoch:152, batch17, load frames use: 3.16736602783s
step 7008/400000: train loss: 0.182684
epoch:152, batch18, load frames use: 3.57179498672s
step 7009/400000: train loss: 0.153693
epoch:152, batch19, load frames use: 3.38355994225s
step 7010/400000: train loss: 0.143811
epoch:152, batch20, load frames use: 3.66498613358s
step 7011/400000: train loss: 0.181727
epoch:152, batch21, load frames use: 3.5064561367s
step 7012/400000: train loss: 0.157896
epoch:152, batch22, load frames use: 5.29206895828s
step 7013/400000: train loss: 0.178069
epoch:152, batch23, load frames use: 4.2885260582s
step 7014/400000: train loss: 0.174535
epoch:152, batch24, load frames use: 3.65829110146s
step 7015/400000: train loss: 0.160665
epoch:152, batch25, load frames use: 4.60621809959s
step 7016/400000: train loss: 0.175877
epoch:152, batch26, load frames use: 4.14036798477s
step 7017/400000: train loss: 0.173553
epoch:152, batch27, load frames use: 3.50326299667s
step 7018/400000: train loss: 0.223108
epoch:152, batch28, load frames use: 3.74664902687s
step 7019/400000: train loss: 0.194943
epoch:152, batch29, load frames use: 3.7483921051s
step 7020/400000: train loss: 0.169222
epoch:152, batch30, load frames use: 3.59783387184s
step 7021/400000: train loss: 0.160899
epoch:152, batch31, load frames use: 4.05290389061s
step 7022/400000: train loss: 0.184078
epoch:152, batch32, load frames use: 3.52604198456s
step 7023/400000: train loss: 0.177737
epoch:152, batch33, load frames use: 3.49997615814s
step 7024/400000: train loss: 0.173495
epoch:152, batch34, load frames use: 3.81199789047s
step 7025/400000: train loss: 0.182533
epoch:152, batch35, load frames use: 2.90485692024s
step 7026/400000: train loss: 0.182679
epoch:152, batch36, load frames use: 3.52101206779s
step 7027/400000: train loss: 0.192375
epoch:152, batch37, load frames use: 5.54708409309s
step 7028/400000: train loss: 0.179848
epoch:152, batch38, load frames use: 3.60973310471s
step 7029/400000: train loss: 0.172349
epoch:152, batch39, load frames use: 3.52418994904s
step 7030/400000: train loss: 0.166595
epoch:152, batch40, load frames use: 5.10988306999s
step 7031/400000: train loss: 0.174623
epoch:152, batch41, load frames use: 3.80858707428s
step 7032/400000: train loss: 0.174181
epoch:152, batch42, load frames use: 3.89315390587s
step 7033/400000: train loss: 0.171603
epoch:152, batch43, load frames use: 3.72469210625s
step 7034/400000: train loss: 0.171271
epoch:152, batch44, load frames use: 3.43512821198s
step 7035/400000: train loss: 0.176525
epoch:152, batch45, load frames use: 3.43208789825s
step 7036/400000: train loss: 0.170574
epoch:152, batch46, load frames use: 4.23846721649s
step 7037/400000: train loss: 0.159957
epoch:153, batch1, load frames use: 3.74626898766s
step 7038/400000: train loss: 0.163494
epoch:153, batch2, load frames use: 4.08638620377s
step 7039/400000: train loss: 0.147599
epoch:153, batch3, load frames use: 4.42974901199s
step 7040/400000: train loss: 0.172513
epoch:153, batch4, load frames use: 3.82404303551s
step 7041/400000: train loss: 0.159880
epoch:153, batch5, load frames use: 4.74427890778s
step 7042/400000: train loss: 0.143702
epoch:153, batch6, load frames use: 3.65079593658s
step 7043/400000: train loss: 0.147096
epoch:153, batch7, load frames use: 3.4930870533s
step 7044/400000: train loss: 0.169482
epoch:153, batch8, load frames use: 3.43493795395s
step 7045/400000: train loss: 0.161539
epoch:153, batch9, load frames use: 3.37989592552s
step 7046/400000: train loss: 0.169342
epoch:153, batch10, load frames use: 3.85294103622s
step 7047/400000: train loss: 0.162329
epoch:153, batch11, load frames use: 4.1997821331s
step 7048/400000: train loss: 0.156894
epoch:153, batch12, load frames use: 3.38341093063s
step 7049/400000: train loss: 0.161611
epoch:153, batch13, load frames use: 5.8968129158s
step 7050/400000: train loss: 0.158859
epoch:153, batch14, load frames use: 4.0932559967s
step 7051/400000: train loss: 0.170703
epoch:153, batch15, load frames use: 4.22106480598s
step 7052/400000: train loss: 0.187814
epoch:153, batch16, load frames use: 3.69565486908s
step 7053/400000: train loss: 0.160954
epoch:153, batch17, load frames use: 4.26806402206s
step 7054/400000: train loss: 0.145883
epoch:153, batch18, load frames use: 4.85599303246s
step 7055/400000: train loss: 0.179514
epoch:153, batch19, load frames use: 3.3716340065s
step 7056/400000: train loss: 0.174443
epoch:153, batch20, load frames use: 3.54507112503s
step 7057/400000: train loss: 0.184892
epoch:153, batch21, load frames use: 5.37014102936s
step 7058/400000: train loss: 0.158273
epoch:153, batch22, load frames use: 4.74591207504s
step 7059/400000: train loss: 0.159438
epoch:153, batch23, load frames use: 4.39260792732s
step 7060/400000: train loss: 0.170086
epoch:153, batch24, load frames use: 5.04156112671s
step 7061/400000: train loss: 0.154494
epoch:153, batch25, load frames use: 3.4708070755s
step 7062/400000: train loss: 0.162314
epoch:153, batch26, load frames use: 3.59986495972s
step 7063/400000: train loss: 0.174349
epoch:153, batch27, load frames use: 4.16811108589s
step 7064/400000: train loss: 0.166720
epoch:153, batch28, load frames use: 5.01568698883s
step 7065/400000: train loss: 0.151054
epoch:153, batch29, load frames use: 3.73740792274s
step 7066/400000: train loss: 0.146387
epoch:153, batch30, load frames use: 4.09873199463s
step 7067/400000: train loss: 0.155128
epoch:153, batch31, load frames use: 4.82466483116s
step 7068/400000: train loss: 0.177303
epoch:153, batch32, load frames use: 3.80920815468s
step 7069/400000: train loss: 0.161044
epoch:153, batch33, load frames use: 4.21461796761s
step 7070/400000: train loss: 0.166208
epoch:153, batch34, load frames use: 4.0511598587s
step 7071/400000: train loss: 0.177734
epoch:153, batch35, load frames use: 3.54523992538s
step 7072/400000: train loss: 0.162684
epoch:153, batch36, load frames use: 3.97063493729s
step 7073/400000: train loss: 0.174738
epoch:153, batch37, load frames use: 4.07596182823s
step 7074/400000: train loss: 0.160511
epoch:153, batch38, load frames use: 5.98480510712s
step 7075/400000: train loss: 0.176376
epoch:153, batch39, load frames use: 5.63814806938s
step 7076/400000: train loss: 0.167768
epoch:153, batch40, load frames use: 4.95240402222s
step 7077/400000: train loss: 0.171474
epoch:153, batch41, load frames use: 3.55783891678s
step 7078/400000: train loss: 0.175925
epoch:153, batch42, load frames use: 5.57080793381s
step 7079/400000: train loss: 0.210310
epoch:153, batch43, load frames use: 3.4968919754s
step 7080/400000: train loss: 0.171440
epoch:153, batch44, load frames use: 4.72701191902s
step 7081/400000: train loss: 0.151876
epoch:153, batch45, load frames use: 5.72378206253s
step 7082/400000: train loss: 0.168535
epoch:153, batch46, load frames use: 3.39521193504s
step 7083/400000: train loss: 0.175074
epoch:154, batch1, load frames use: 3.40921092033s
step 7084/400000: train loss: 0.146690
epoch:154, batch2, load frames use: 3.97152709961s
step 7085/400000: train loss: 0.184003
epoch:154, batch3, load frames use: 3.57177710533s
step 7086/400000: train loss: 0.172201
epoch:154, batch4, load frames use: 3.34133005142s
step 7087/400000: train loss: 0.174778
epoch:154, batch5, load frames use: 3.62619590759s
step 7088/400000: train loss: 0.172781
epoch:154, batch6, load frames use: 3.49772787094s
step 7089/400000: train loss: 0.170682
epoch:154, batch7, load frames use: 3.50925707817s
step 7090/400000: train loss: 0.179692
epoch:154, batch8, load frames use: 3.30440306664s
step 7091/400000: train loss: 0.178460
epoch:154, batch9, load frames use: 3.32900714874s
step 7092/400000: train loss: 0.197283
epoch:154, batch10, load frames use: 3.61849594116s
step 7093/400000: train loss: 0.173225
epoch:154, batch11, load frames use: 3.49044799805s
step 7094/400000: train loss: 0.174844
epoch:154, batch12, load frames use: 3.63133811951s
step 7095/400000: train loss: 0.181344
epoch:154, batch13, load frames use: 3.37479901314s
step 7096/400000: train loss: 0.169168
epoch:154, batch14, load frames use: 3.37511587143s
step 7097/400000: train loss: 0.158348
epoch:154, batch15, load frames use: 3.44997787476s
step 7098/400000: train loss: 0.166748
epoch:154, batch16, load frames use: 3.28929901123s
step 7099/400000: train loss: 0.164205
epoch:154, batch17, load frames use: 3.36971592903s
step 7100/400000: train loss: 0.155316
step 7100/400000: validation loss: 0.146078
epoch:154, batch18, load frames use: 3.48821783066s
step 7101/400000: train loss: 0.167352
epoch:154, batch19, load frames use: 2.91051506996s
step 7102/400000: train loss: 0.157599
epoch:154, batch20, load frames use: 3.4945909977s
step 7103/400000: train loss: 0.173865
epoch:154, batch21, load frames use: 3.14393401146s
step 7104/400000: train loss: 0.159942
epoch:154, batch22, load frames use: 3.5142250061s
step 7105/400000: train loss: 0.162850
epoch:154, batch23, load frames use: 3.43730211258s
step 7106/400000: train loss: 0.188434
epoch:154, batch24, load frames use: 3.21676802635s
step 7107/400000: train loss: 0.173483
epoch:154, batch25, load frames use: 3.4569170475s
step 7108/400000: train loss: 0.166641
epoch:154, batch26, load frames use: 3.46573901176s
step 7109/400000: train loss: 0.177586
epoch:154, batch27, load frames use: 3.09466791153s
step 7110/400000: train loss: 0.159626
epoch:154, batch28, load frames use: 3.00041699409s
step 7111/400000: train loss: 0.162371
epoch:154, batch29, load frames use: 3.57985091209s
step 7112/400000: train loss: 0.173869
epoch:154, batch30, load frames use: 3.23378586769s
step 7113/400000: train loss: 0.164628
epoch:154, batch31, load frames use: 3.33374810219s
step 7114/400000: train loss: 0.177424
epoch:154, batch32, load frames use: 3.18041610718s
step 7115/400000: train loss: 0.177395
epoch:154, batch33, load frames use: 3.45802283287s
step 7116/400000: train loss: 0.183147
epoch:154, batch34, load frames use: 3.10426616669s
step 7117/400000: train loss: 0.179035
epoch:154, batch35, load frames use: 3.63436293602s
step 7118/400000: train loss: 0.166539
epoch:154, batch36, load frames use: 3.54484605789s
step 7119/400000: train loss: 0.173761
epoch:154, batch37, load frames use: 3.51635098457s
step 7120/400000: train loss: 0.194279
epoch:154, batch38, load frames use: 3.24235010147s
step 7121/400000: train loss: 0.160128
epoch:154, batch39, load frames use: 3.57252502441s
step 7122/400000: train loss: 0.169133
epoch:154, batch40, load frames use: 3.37847208977s
step 7123/400000: train loss: 0.183731
epoch:154, batch41, load frames use: 3.43160200119s
step 7124/400000: train loss: 0.184858
epoch:154, batch42, load frames use: 3.2235121727s
step 7125/400000: train loss: 0.168377
epoch:154, batch43, load frames use: 3.51790189743s
step 7126/400000: train loss: 0.166573
epoch:154, batch44, load frames use: 3.22768712044s
step 7127/400000: train loss: 0.177927
epoch:154, batch45, load frames use: 3.63929080963s
step 7128/400000: train loss: 0.170451
epoch:154, batch46, load frames use: 3.01310515404s
step 7129/400000: train loss: 0.162849
epoch:155, batch1, load frames use: 3.29754900932s
step 7130/400000: train loss: 0.152417
epoch:155, batch2, load frames use: 3.62230491638s
step 7131/400000: train loss: 0.167884
epoch:155, batch3, load frames use: 3.35176491737s
step 7132/400000: train loss: 0.155207
epoch:155, batch4, load frames use: 3.31941604614s
step 7133/400000: train loss: 0.159771
epoch:155, batch5, load frames use: 3.56218695641s
step 7134/400000: train loss: 0.175431
epoch:155, batch6, load frames use: 3.73137593269s
step 7135/400000: train loss: 0.156518
epoch:155, batch7, load frames use: 3.40515089035s
step 7136/400000: train loss: 0.165236
epoch:155, batch8, load frames use: 3.76005792618s
step 7137/400000: train loss: 0.172070
epoch:155, batch9, load frames use: 3.45734786987s
step 7138/400000: train loss: 0.155055
epoch:155, batch10, load frames use: 3.12252688408s
step 7139/400000: train loss: 0.168080
epoch:155, batch11, load frames use: 3.56704592705s
step 7140/400000: train loss: 0.158029
epoch:155, batch12, load frames use: 3.51297402382s
step 7141/400000: train loss: 0.172210
epoch:155, batch13, load frames use: 3.5500497818s
step 7142/400000: train loss: 0.163008
epoch:155, batch14, load frames use: 3.34526395798s
step 7143/400000: train loss: 0.175100
epoch:155, batch15, load frames use: 3.38747000694s
step 7144/400000: train loss: 0.146143
epoch:155, batch16, load frames use: 3.37348794937s
step 7145/400000: train loss: 0.154617
epoch:155, batch17, load frames use: 5.29086923599s
step 7146/400000: train loss: 0.173754
epoch:155, batch18, load frames use: 3.59972500801s
step 7147/400000: train loss: 0.138956
epoch:155, batch19, load frames use: 3.2914981842s
step 7148/400000: train loss: 0.165732
epoch:155, batch20, load frames use: 3.24732089043s
step 7149/400000: train loss: 0.176961
epoch:155, batch21, load frames use: 3.38530611992s
step 7150/400000: train loss: 0.158803
epoch:155, batch22, load frames use: 3.494576931s
step 7151/400000: train loss: 0.173274
epoch:155, batch23, load frames use: 3.34219098091s
step 7152/400000: train loss: 0.168418
epoch:155, batch24, load frames use: 3.28116798401s
step 7153/400000: train loss: 0.151724
epoch:155, batch25, load frames use: 3.57278513908s
step 7154/400000: train loss: 0.165117
epoch:155, batch26, load frames use: 3.29629707336s
step 7155/400000: train loss: 0.155122
epoch:155, batch27, load frames use: 3.49317121506s
step 7156/400000: train loss: 0.157869
epoch:155, batch28, load frames use: 3.54937505722s
step 7157/400000: train loss: 0.160342
epoch:155, batch29, load frames use: 3.57325696945s
step 7158/400000: train loss: 0.158499
epoch:155, batch30, load frames use: 3.3000190258s
step 7159/400000: train loss: 0.173613
epoch:155, batch31, load frames use: 3.25040888786s
step 7160/400000: train loss: 0.164889
epoch:155, batch32, load frames use: 3.38392496109s
step 7161/400000: train loss: 0.177723
epoch:155, batch33, load frames use: 3.51389908791s
step 7162/400000: train loss: 0.167298
epoch:155, batch34, load frames use: 3.67682695389s
step 7163/400000: train loss: 0.178188
epoch:155, batch35, load frames use: 3.33900308609s
step 7164/400000: train loss: 0.169237
epoch:155, batch36, load frames use: 3.40456604958s
step 7165/400000: train loss: 0.190621
epoch:155, batch37, load frames use: 3.38714003563s
step 7166/400000: train loss: 0.182331
epoch:155, batch38, load frames use: 3.36131906509s
step 7167/400000: train loss: 0.190830
epoch:155, batch39, load frames use: 3.78304100037s
step 7168/400000: train loss: 0.158443
epoch:155, batch40, load frames use: 3.26598000526s
step 7169/400000: train loss: 0.178602
epoch:155, batch41, load frames use: 3.14601612091s
step 7170/400000: train loss: 0.178766
epoch:155, batch42, load frames use: 3.42431807518s
step 7171/400000: train loss: 0.157380
epoch:155, batch43, load frames use: 3.31394600868s
step 7172/400000: train loss: 0.181853
epoch:155, batch44, load frames use: 3.36852407455s
step 7173/400000: train loss: 0.168113
epoch:155, batch45, load frames use: 3.41234207153s
step 7174/400000: train loss: 0.150279
epoch:155, batch46, load frames use: 3.21047210693s
step 7175/400000: train loss: 0.190838
epoch:156, batch1, load frames use: 2.85548305511s
step 7176/400000: train loss: 0.174524
epoch:156, batch2, load frames use: 3.2559261322s
step 7177/400000: train loss: 0.166408
epoch:156, batch3, load frames use: 3.16453790665s
step 7178/400000: train loss: 0.161897
epoch:156, batch4, load frames use: 3.44424700737s
step 7179/400000: train loss: 0.191960
epoch:156, batch5, load frames use: 3.42764496803s
step 7180/400000: train loss: 0.170607
epoch:156, batch6, load frames use: 3.61282491684s
step 7181/400000: train loss: 0.155645
epoch:156, batch7, load frames use: 3.24437189102s
step 7182/400000: train loss: 0.149012
epoch:156, batch8, load frames use: 3.32371091843s
step 7183/400000: train loss: 0.158882
epoch:156, batch9, load frames use: 3.36662101746s
step 7184/400000: train loss: 0.176546
epoch:156, batch10, load frames use: 3.52403712273s
step 7185/400000: train loss: 0.144222
epoch:156, batch11, load frames use: 2.9482281208s
step 7186/400000: train loss: 0.175684
epoch:156, batch12, load frames use: 3.63125705719s
step 7187/400000: train loss: 0.178316
epoch:156, batch13, load frames use: 3.53346705437s
step 7188/400000: train loss: 0.153148
epoch:156, batch14, load frames use: 3.32417917252s
step 7189/400000: train loss: 0.178704
epoch:156, batch15, load frames use: 3.32477498055s
step 7190/400000: train loss: 0.155347
epoch:156, batch16, load frames use: 3.29325795174s
step 7191/400000: train loss: 0.167347
epoch:156, batch17, load frames use: 3.58271098137s
step 7192/400000: train loss: 0.157247
epoch:156, batch18, load frames use: 3.50395989418s
step 7193/400000: train loss: 0.155884
epoch:156, batch19, load frames use: 2.72379803658s
step 7194/400000: train loss: 0.161554
epoch:156, batch20, load frames use: 3.57012677193s
step 7195/400000: train loss: 0.162955
epoch:156, batch21, load frames use: 2.93226218224s
step 7196/400000: train loss: 0.175104
epoch:156, batch22, load frames use: 3.45782995224s
step 7197/400000: train loss: 0.185710
epoch:156, batch23, load frames use: 3.51233887672s
step 7198/400000: train loss: 0.174591
epoch:156, batch24, load frames use: 3.5669400692s
step 7199/400000: train loss: 0.183101
epoch:156, batch25, load frames use: 3.41503000259s
step 7200/400000: train loss: 0.158118
step 7200/400000: validation loss: 0.145203
epoch:156, batch26, load frames use: 3.44726800919s
step 7201/400000: train loss: 0.164830
epoch:156, batch27, load frames use: 3.30449795723s
step 7202/400000: train loss: 0.198823
epoch:156, batch28, load frames use: 3.44307088852s
step 7203/400000: train loss: 0.163845
epoch:156, batch29, load frames use: 3.53444004059s
step 7204/400000: train loss: 0.178092
epoch:156, batch30, load frames use: 3.83835816383s
step 7205/400000: train loss: 0.164113
epoch:156, batch31, load frames use: 3.39919495583s
step 7206/400000: train loss: 0.169349
epoch:156, batch32, load frames use: 3.48012018204s
step 7207/400000: train loss: 0.159270
epoch:156, batch33, load frames use: 3.41840314865s
step 7208/400000: train loss: 0.165463
epoch:156, batch34, load frames use: 3.49175596237s
step 7209/400000: train loss: 0.166829
epoch:156, batch35, load frames use: 3.35832381248s
step 7210/400000: train loss: 0.164037
epoch:156, batch36, load frames use: 3.99052405357s
step 7211/400000: train loss: 0.190215
epoch:156, batch37, load frames use: 3.49415683746s
step 7212/400000: train loss: 0.166948
epoch:156, batch38, load frames use: 3.44598507881s
step 7213/400000: train loss: 0.180208
epoch:156, batch39, load frames use: 3.36654901505s
step 7214/400000: train loss: 0.181351
epoch:156, batch40, load frames use: 3.95670104027s
step 7215/400000: train loss: 0.147595
epoch:156, batch41, load frames use: 3.25629997253s
step 7216/400000: train loss: 0.183856
epoch:156, batch42, load frames use: 3.46997714043s
step 7217/400000: train loss: 0.171356
epoch:156, batch43, load frames use: 3.19109296799s
step 7218/400000: train loss: 0.170507
epoch:156, batch44, load frames use: 3.63623309135s
step 7219/400000: train loss: 0.182496
epoch:156, batch45, load frames use: 3.43573093414s
step 7220/400000: train loss: 0.165137
epoch:156, batch46, load frames use: 3.77657198906s
step 7221/400000: train loss: 0.159405
epoch:157, batch1, load frames use: 3.17909812927s
step 7222/400000: train loss: 0.171717
epoch:157, batch2, load frames use: 3.2012090683s
step 7223/400000: train loss: 0.152913
epoch:157, batch3, load frames use: 3.4654211998s
step 7224/400000: train loss: 0.152135
epoch:157, batch4, load frames use: 3.28564691544s
step 7225/400000: train loss: 0.174772
epoch:157, batch5, load frames use: 3.35972690582s
step 7226/400000: train loss: 0.161203
epoch:157, batch6, load frames use: 3.13896298409s
step 7227/400000: train loss: 0.162835
epoch:157, batch7, load frames use: 3.77386188507s
step 7228/400000: train loss: 0.154014
epoch:157, batch8, load frames use: 3.18991398811s
step 7229/400000: train loss: 0.172069
epoch:157, batch9, load frames use: 3.22397398949s
step 7230/400000: train loss: 0.149081
epoch:157, batch10, load frames use: 3.18660402298s
step 7231/400000: train loss: 0.166709
epoch:157, batch11, load frames use: 3.35149788857s
step 7232/400000: train loss: 0.152988
epoch:157, batch12, load frames use: 3.40425896645s
step 7233/400000: train loss: 0.179528
epoch:157, batch13, load frames use: 3.24386191368s
step 7234/400000: train loss: 0.162141
epoch:157, batch14, load frames use: 3.16243100166s
step 7235/400000: train loss: 0.144409
epoch:157, batch15, load frames use: 3.31159496307s
step 7236/400000: train loss: 0.174860
epoch:157, batch16, load frames use: 3.56431508064s
step 7237/400000: train loss: 0.170991
epoch:157, batch17, load frames use: 3.23859095573s
step 7238/400000: train loss: 0.172086
epoch:157, batch18, load frames use: 3.30486989021s
step 7239/400000: train loss: 0.165816
epoch:157, batch19, load frames use: 3.63154220581s
step 7240/400000: train loss: 0.165837
epoch:157, batch20, load frames use: 3.28826880455s
step 7241/400000: train loss: 0.167718
epoch:157, batch21, load frames use: 3.2894089222s
step 7242/400000: train loss: 0.159653
epoch:157, batch22, load frames use: 3.68879795074s
step 7243/400000: train loss: 0.173482
epoch:157, batch23, load frames use: 3.17901110649s
step 7244/400000: train loss: 0.161990
epoch:157, batch24, load frames use: 3.20346593857s
step 7245/400000: train loss: 0.161645
epoch:157, batch25, load frames use: 3.43390893936s
step 7246/400000: train loss: 0.185290
epoch:157, batch26, load frames use: 3.45501112938s
step 7247/400000: train loss: 0.168159
epoch:157, batch27, load frames use: 3.08900904655s
step 7248/400000: train loss: 0.166943
epoch:157, batch28, load frames use: 3.46371984482s
step 7249/400000: train loss: 0.172268
epoch:157, batch29, load frames use: 3.48548698425s
step 7250/400000: train loss: 0.181396
epoch:157, batch30, load frames use: 3.45434689522s
step 7251/400000: train loss: 0.172929
epoch:157, batch31, load frames use: 3.35119795799s
step 7252/400000: train loss: 0.159076
epoch:157, batch32, load frames use: 3.70875191689s
step 7253/400000: train loss: 0.160587
epoch:157, batch33, load frames use: 3.22663116455s
step 7254/400000: train loss: 0.184914
epoch:157, batch34, load frames use: 3.39341402054s
step 7255/400000: train loss: 0.159409
epoch:157, batch35, load frames use: 3.28011393547s
step 7256/400000: train loss: 0.156393
epoch:157, batch36, load frames use: 3.29682803154s
step 7257/400000: train loss: 0.165709
epoch:157, batch37, load frames use: 3.61896395683s
step 7258/400000: train loss: 0.200855
epoch:157, batch38, load frames use: 3.39357185364s
step 7259/400000: train loss: 0.173601
epoch:157, batch39, load frames use: 3.32505893707s
step 7260/400000: train loss: 0.171418
epoch:157, batch40, load frames use: 3.9627289772s
step 7261/400000: train loss: 0.165720
epoch:157, batch41, load frames use: 3.47936415672s
step 7262/400000: train loss: 0.185768
epoch:157, batch42, load frames use: 3.3805539608s
step 7263/400000: train loss: 0.186547
epoch:157, batch43, load frames use: 3.65611505508s
step 7264/400000: train loss: 0.172998
epoch:157, batch44, load frames use: 3.42483401299s
step 7265/400000: train loss: 0.178778
epoch:157, batch45, load frames use: 3.24032711983s
step 7266/400000: train loss: 0.158968
epoch:157, batch46, load frames use: 3.25556087494s
step 7267/400000: train loss: 0.162080
epoch:158, batch1, load frames use: 3.41820502281s
step 7268/400000: train loss: 0.164030
epoch:158, batch2, load frames use: 3.19888687134s
step 7269/400000: train loss: 0.163003
epoch:158, batch3, load frames use: 3.27564096451s
step 7270/400000: train loss: 0.157295
epoch:158, batch4, load frames use: 3.91111207008s
step 7271/400000: train loss: 0.142341
epoch:158, batch5, load frames use: 3.29921793938s
step 7272/400000: train loss: 0.173858
epoch:158, batch6, load frames use: 3.18754601479s
step 7273/400000: train loss: 0.150428
epoch:158, batch7, load frames use: 3.27565693855s
step 7274/400000: train loss: 0.151924
epoch:158, batch8, load frames use: 3.1532831192s
step 7275/400000: train loss: 0.176678
epoch:158, batch9, load frames use: 3.2455830574s
step 7276/400000: train loss: 0.178220
epoch:158, batch10, load frames use: 3.51116299629s
step 7277/400000: train loss: 0.173181
epoch:158, batch11, load frames use: 3.36638617516s
step 7278/400000: train loss: 0.170113
epoch:158, batch12, load frames use: 3.31394290924s
step 7279/400000: train loss: 0.155073
epoch:158, batch13, load frames use: 3.71384382248s
step 7280/400000: train loss: 0.163384
epoch:158, batch14, load frames use: 3.29743003845s
step 7281/400000: train loss: 0.165714
epoch:158, batch15, load frames use: 3.36123895645s
step 7282/400000: train loss: 0.156887
epoch:158, batch16, load frames use: 3.49965190887s
step 7283/400000: train loss: 0.164166
epoch:158, batch17, load frames use: 3.23053407669s
step 7284/400000: train loss: 0.168743
epoch:158, batch18, load frames use: 3.19990515709s
step 7285/400000: train loss: 0.161243
epoch:158, batch19, load frames use: 3.26512217522s
step 7286/400000: train loss: 0.161992
epoch:158, batch20, load frames use: 3.75933289528s
step 7287/400000: train loss: 0.186649
epoch:158, batch21, load frames use: 3.49460601807s
step 7288/400000: train loss: 0.164479
epoch:158, batch22, load frames use: 3.21730589867s
step 7289/400000: train loss: 0.169756
epoch:158, batch23, load frames use: 3.28844213486s
step 7290/400000: train loss: 0.163500
epoch:158, batch24, load frames use: 3.26107907295s
step 7291/400000: train loss: 0.164310
epoch:158, batch25, load frames use: 3.27671694756s
step 7292/400000: train loss: 0.164421
epoch:158, batch26, load frames use: 3.78582286835s
step 7293/400000: train loss: 0.152125
epoch:158, batch27, load frames use: 3.23081088066s
step 7294/400000: train loss: 0.185505
epoch:158, batch28, load frames use: 3.86768198013s
step 7295/400000: train loss: 0.173622
epoch:158, batch29, load frames use: 3.15956115723s
step 7296/400000: train loss: 0.168903
epoch:158, batch30, load frames use: 3.3616900444s
step 7297/400000: train loss: 0.169766
epoch:158, batch31, load frames use: 3.37367582321s
step 7298/400000: train loss: 0.161567
epoch:158, batch32, load frames use: 3.24953508377s
step 7299/400000: train loss: 0.165288
epoch:158, batch33, load frames use: 3.12937188148s
step 7300/400000: train loss: 0.154987
step 7300/400000: validation loss: 0.146235
epoch:158, batch34, load frames use: 3.38268399239s
step 7301/400000: train loss: 0.161530
epoch:158, batch35, load frames use: 3.30657315254s
step 7302/400000: train loss: 0.163067
epoch:158, batch36, load frames use: 3.28680515289s
step 7303/400000: train loss: 0.152885
epoch:158, batch37, load frames use: 3.16080212593s
step 7304/400000: train loss: 0.191409
epoch:158, batch38, load frames use: 3.72346591949s
step 7305/400000: train loss: 0.177349
epoch:158, batch39, load frames use: 3.58651685715s
step 7306/400000: train loss: 0.152714
epoch:158, batch40, load frames use: 3.32913994789s
step 7307/400000: train loss: 0.157903
epoch:158, batch41, load frames use: 3.29009389877s
step 7308/400000: train loss: 0.180562
epoch:158, batch42, load frames use: 3.66669297218s
step 7309/400000: train loss: 0.175923
epoch:158, batch43, load frames use: 3.21828603745s
step 7310/400000: train loss: 0.162796
epoch:158, batch44, load frames use: 3.71976590157s
step 7311/400000: train loss: 0.169424
epoch:158, batch45, load frames use: 3.09093093872s
step 7312/400000: train loss: 0.181139
epoch:158, batch46, load frames use: 3.34026098251s
step 7313/400000: train loss: 0.173707
epoch:159, batch1, load frames use: 3.65268492699s
step 7314/400000: train loss: 0.173512
epoch:159, batch2, load frames use: 3.18023300171s
step 7315/400000: train loss: 0.177885
epoch:159, batch3, load frames use: 3.62229895592s
step 7316/400000: train loss: 0.170674
epoch:159, batch4, load frames use: 3.06991100311s
step 7317/400000: train loss: 0.150312
epoch:159, batch5, load frames use: 4.04711985588s
step 7318/400000: train loss: 0.160646
epoch:159, batch6, load frames use: 4.85943603516s
step 7319/400000: train loss: 0.166253
epoch:159, batch7, load frames use: 3.49588108063s
step 7320/400000: train loss: 0.158285
epoch:159, batch8, load frames use: 3.64272499084s
step 7321/400000: train loss: 0.169306
epoch:159, batch9, load frames use: 5.23144102097s
step 7322/400000: train loss: 0.156389
epoch:159, batch10, load frames use: 5.12677598s
step 7323/400000: train loss: 0.142714
epoch:159, batch11, load frames use: 3.70522379875s
step 7324/400000: train loss: 0.161563
epoch:159, batch12, load frames use: 3.27805805206s
step 7325/400000: train loss: 0.170666
epoch:159, batch13, load frames use: 5.1739089489s
step 7326/400000: train loss: 0.160965
epoch:159, batch14, load frames use: 3.20313119888s
step 7327/400000: train loss: 0.148767
epoch:159, batch15, load frames use: 6.57612991333s
step 7328/400000: train loss: 0.144534
epoch:159, batch16, load frames use: 3.87495613098s
step 7329/400000: train loss: 0.165626
epoch:159, batch17, load frames use: 4.39624118805s
step 7330/400000: train loss: 0.161526
epoch:159, batch18, load frames use: 5.84408187866s
step 7331/400000: train loss: 0.168951
epoch:159, batch19, load frames use: 3.12253499031s
step 7332/400000: train loss: 0.170673
epoch:159, batch20, load frames use: 3.0626039505s
step 7333/400000: train loss: 0.155832
epoch:159, batch21, load frames use: 3.26012897491s
step 7334/400000: train loss: 0.158772
epoch:159, batch22, load frames use: 3.43639612198s
step 7335/400000: train loss: 0.165575
epoch:159, batch23, load frames use: 3.60631394386s
step 7336/400000: train loss: 0.168618
epoch:159, batch24, load frames use: 3.40787601471s
step 7337/400000: train loss: 0.161441
epoch:159, batch25, load frames use: 3.78980088234s
step 7338/400000: train loss: 0.176422
epoch:159, batch26, load frames use: 3.43995809555s
step 7339/400000: train loss: 0.147070
epoch:159, batch27, load frames use: 3.28556704521s
step 7340/400000: train loss: 0.159286
epoch:159, batch28, load frames use: 3.44035100937s
step 7341/400000: train loss: 0.162617
epoch:159, batch29, load frames use: 3.63108897209s
step 7342/400000: train loss: 0.168014
epoch:159, batch30, load frames use: 3.73506116867s
step 7343/400000: train loss: 0.172082
epoch:159, batch31, load frames use: 3.41703701019s
step 7344/400000: train loss: 0.182582
epoch:159, batch32, load frames use: 4.94198393822s
step 7345/400000: train loss: 0.164906
epoch:159, batch33, load frames use: 3.77175498009s
step 7346/400000: train loss: 0.162658
epoch:159, batch34, load frames use: 3.51944994926s
step 7347/400000: train loss: 0.166068
epoch:159, batch35, load frames use: 3.59752988815s
step 7348/400000: train loss: 0.174463
epoch:159, batch36, load frames use: 3.32822489738s
step 7349/400000: train loss: 0.181959
epoch:159, batch37, load frames use: 3.62543988228s
step 7350/400000: train loss: 0.155782
epoch:159, batch38, load frames use: 3.66724205017s
step 7351/400000: train loss: 0.177274
epoch:159, batch39, load frames use: 3.55228996277s
step 7352/400000: train loss: 0.167563
epoch:159, batch40, load frames use: 3.63612103462s
step 7353/400000: train loss: 0.153549
epoch:159, batch41, load frames use: 4.00806093216s
step 7354/400000: train loss: 0.182523
epoch:159, batch42, load frames use: 3.49930405617s
step 7355/400000: train loss: 0.179606
epoch:159, batch43, load frames use: 3.43537592888s
step 7356/400000: train loss: 0.155228
epoch:159, batch44, load frames use: 4.04008102417s
step 7357/400000: train loss: 0.181862
epoch:159, batch45, load frames use: 3.47837090492s
step 7358/400000: train loss: 0.188172
epoch:159, batch46, load frames use: 3.49539613724s
step 7359/400000: train loss: 0.181865
epoch:160, batch1, load frames use: 3.44438719749s
step 7360/400000: train loss: 0.170376
epoch:160, batch2, load frames use: 4.32666802406s
step 7361/400000: train loss: 0.140243
epoch:160, batch3, load frames use: 3.35491609573s
step 7362/400000: train loss: 0.140964
epoch:160, batch4, load frames use: 3.6128718853s
step 7363/400000: train loss: 0.163232
epoch:160, batch5, load frames use: 3.6471889019s
step 7364/400000: train loss: 0.160696
epoch:160, batch6, load frames use: 3.67487716675s
step 7365/400000: train loss: 0.170285
epoch:160, batch7, load frames use: 3.64930486679s
step 7366/400000: train loss: 0.162157
epoch:160, batch8, load frames use: 4.27346992493s
step 7367/400000: train loss: 0.150398
epoch:160, batch9, load frames use: 3.96262288094s
step 7368/400000: train loss: 0.161462
epoch:160, batch10, load frames use: 3.54183506966s
step 7369/400000: train loss: 0.161890
epoch:160, batch11, load frames use: 3.57787799835s
step 7370/400000: train loss: 0.174582
epoch:160, batch12, load frames use: 4.37458300591s
step 7371/400000: train loss: 0.164118
epoch:160, batch13, load frames use: 3.86625504494s
step 7372/400000: train loss: 0.158236
epoch:160, batch14, load frames use: 3.4873149395s
step 7373/400000: train loss: 0.158506
epoch:160, batch15, load frames use: 4.17286992073s
step 7374/400000: train loss: 0.161708
epoch:160, batch16, load frames use: 3.21574187279s
step 7375/400000: train loss: 0.148265
epoch:160, batch17, load frames use: 3.94624996185s
step 7376/400000: train loss: 0.149674
epoch:160, batch18, load frames use: 3.62898111343s
step 7377/400000: train loss: 0.162057
epoch:160, batch19, load frames use: 3.65596914291s
step 7378/400000: train loss: 0.188696
epoch:160, batch20, load frames use: 3.46629977226s
step 7379/400000: train loss: 0.165749
epoch:160, batch21, load frames use: 3.28532886505s
step 7380/400000: train loss: 0.163229
epoch:160, batch22, load frames use: 3.77294301987s
step 7381/400000: train loss: 0.167360
epoch:160, batch23, load frames use: 3.48505592346s
step 7382/400000: train loss: 0.154622
epoch:160, batch24, load frames use: 3.97803711891s
step 7383/400000: train loss: 0.156298
epoch:160, batch25, load frames use: 3.67845416069s
step 7384/400000: train loss: 0.164447
epoch:160, batch26, load frames use: 3.30905890465s
step 7385/400000: train loss: 0.172973
epoch:160, batch27, load frames use: 3.43070697784s
step 7386/400000: train loss: 0.147505
epoch:160, batch28, load frames use: 3.25321984291s
step 7387/400000: train loss: 0.156679
epoch:160, batch29, load frames use: 4.27185106277s
step 7388/400000: train loss: 0.177295
epoch:160, batch30, load frames use: 3.92747998238s
step 7389/400000: train loss: 0.170837
epoch:160, batch31, load frames use: 3.64095282555s
step 7390/400000: train loss: 0.170389
epoch:160, batch32, load frames use: 3.75343084335s
step 7391/400000: train loss: 0.167505
epoch:160, batch33, load frames use: 3.49819588661s
step 7392/400000: train loss: 0.153945
epoch:160, batch34, load frames use: 3.55838990211s
step 7393/400000: train loss: 0.176142
epoch:160, batch35, load frames use: 3.60174107552s
step 7394/400000: train loss: 0.171184
epoch:160, batch36, load frames use: 3.3546230793s
step 7395/400000: train loss: 0.145497
epoch:160, batch37, load frames use: 3.38563394547s
step 7396/400000: train loss: 0.178688
epoch:160, batch38, load frames use: 3.46352410316s
step 7397/400000: train loss: 0.165911
epoch:160, batch39, load frames use: 3.89870715141s
step 7398/400000: train loss: 0.177658
epoch:160, batch40, load frames use: 3.38378715515s
step 7399/400000: train loss: 0.154831
epoch:160, batch41, load frames use: 3.4054889679s
step 7400/400000: train loss: 0.171575
step 7400/400000: validation loss: 0.160379
epoch:160, batch42, load frames use: 3.54630994797s
step 7401/400000: train loss: 0.178578
epoch:160, batch43, load frames use: 3.4475748539s
step 7402/400000: train loss: 0.174169
epoch:160, batch44, load frames use: 4.39972901344s
step 7403/400000: train loss: 0.143969
epoch:160, batch45, load frames use: 3.45192098618s
step 7404/400000: train loss: 0.154893
epoch:160, batch46, load frames use: 3.43299508095s
step 7405/400000: train loss: 0.171166
epoch:161, batch1, load frames use: 3.58329105377s
step 7406/400000: train loss: 0.167297
epoch:161, batch2, load frames use: 3.40297079086s
step 7407/400000: train loss: 0.171176
epoch:161, batch3, load frames use: 3.48686814308s
step 7408/400000: train loss: 0.160772
epoch:161, batch4, load frames use: 3.97429394722s
step 7409/400000: train loss: 0.192212
epoch:161, batch5, load frames use: 3.13625502586s
step 7410/400000: train loss: 0.167975
epoch:161, batch6, load frames use: 3.43548798561s
step 7411/400000: train loss: 0.150056
epoch:161, batch7, load frames use: 2.97943997383s
step 7412/400000: train loss: 0.149564
epoch:161, batch8, load frames use: 3.70314383507s
step 7413/400000: train loss: 0.142881
epoch:161, batch9, load frames use: 3.20439505577s
step 7414/400000: train loss: 0.160157
epoch:161, batch10, load frames use: 4.06468892097s
step 7415/400000: train loss: 0.156692
epoch:161, batch11, load frames use: 2.94656991959s
step 7416/400000: train loss: 0.157230
epoch:161, batch12, load frames use: 3.73751711845s
step 7417/400000: train loss: 0.153475
epoch:161, batch13, load frames use: 2.93154287338s
step 7418/400000: train loss: 0.154850
epoch:161, batch14, load frames use: 4.53000307083s
step 7419/400000: train loss: 0.163925
epoch:161, batch15, load frames use: 3.36044311523s
step 7420/400000: train loss: 0.153131
epoch:161, batch16, load frames use: 3.41805505753s
step 7421/400000: train loss: 0.173597
epoch:161, batch17, load frames use: 3.51941418648s
step 7422/400000: train loss: 0.158169
epoch:161, batch18, load frames use: 3.42833518982s
step 7423/400000: train loss: 0.164476
epoch:161, batch19, load frames use: 2.6216750145s
step 7424/400000: train loss: 0.150151
epoch:161, batch20, load frames use: 3.56765007973s
step 7425/400000: train loss: 0.171223
epoch:161, batch21, load frames use: 3.39989495277s
step 7426/400000: train loss: 0.164457
epoch:161, batch22, load frames use: 4.12832188606s
step 7427/400000: train loss: 0.185836
epoch:161, batch23, load frames use: 3.40060210228s
step 7428/400000: train loss: 0.194520
epoch:161, batch24, load frames use: 2.977602005s
step 7429/400000: train loss: 0.167146
epoch:161, batch25, load frames use: 3.60858798027s
step 7430/400000: train loss: 0.175362
epoch:161, batch26, load frames use: 3.43359899521s
step 7431/400000: train loss: 0.164502
epoch:161, batch27, load frames use: 3.17599701881s
step 7432/400000: train loss: 0.152498
epoch:161, batch28, load frames use: 3.11049604416s
step 7433/400000: train loss: 0.150732
epoch:161, batch29, load frames use: 3.09838891029s
step 7434/400000: train loss: 0.166463
epoch:161, batch30, load frames use: 3.35808300972s
step 7435/400000: train loss: 0.166487
epoch:161, batch31, load frames use: 3.31199002266s
step 7436/400000: train loss: 0.171701
epoch:161, batch32, load frames use: 3.42019200325s
step 7437/400000: train loss: 0.175822
epoch:161, batch33, load frames use: 3.4037270546s
step 7438/400000: train loss: 0.178971
epoch:161, batch34, load frames use: 3.15711188316s
step 7439/400000: train loss: 0.186295
epoch:161, batch35, load frames use: 3.26799893379s
step 7440/400000: train loss: 0.147978
epoch:161, batch36, load frames use: 3.40570402145s
step 7441/400000: train loss: 0.161141
epoch:161, batch37, load frames use: 3.40067696571s
step 7442/400000: train loss: 0.177463
epoch:161, batch38, load frames use: 3.46008419991s
step 7443/400000: train loss: 0.168077
epoch:161, batch39, load frames use: 3.15171194077s
step 7444/400000: train loss: 0.160602
epoch:161, batch40, load frames use: 3.33871889114s
step 7445/400000: train loss: 0.163987
epoch:161, batch41, load frames use: 3.69993901253s
step 7446/400000: train loss: 0.160571
epoch:161, batch42, load frames use: 3.89107489586s
step 7447/400000: train loss: 0.165598
epoch:161, batch43, load frames use: 4.10110783577s
step 7448/400000: train loss: 0.158414
epoch:161, batch44, load frames use: 3.3831410408s
step 7449/400000: train loss: 0.169342
epoch:161, batch45, load frames use: 3.88912582397s
step 7450/400000: train loss: 0.162868
epoch:161, batch46, load frames use: 3.48769402504s
step 7451/400000: train loss: 0.169790
epoch:162, batch1, load frames use: 3.26415801048s
step 7452/400000: train loss: 0.150259
epoch:162, batch2, load frames use: 3.44731807709s
step 7453/400000: train loss: 0.159773
epoch:162, batch3, load frames use: 3.75182795525s
step 7454/400000: train loss: 0.170929
epoch:162, batch4, load frames use: 4.48815393448s
step 7455/400000: train loss: 0.164357
epoch:162, batch5, load frames use: 4.19641709328s
step 7456/400000: train loss: 0.163082
epoch:162, batch6, load frames use: 3.41759681702s
step 7457/400000: train loss: 0.159222
epoch:162, batch7, load frames use: 3.40817689896s
step 7458/400000: train loss: 0.153347
epoch:162, batch8, load frames use: 3.60643005371s
step 7459/400000: train loss: 0.152779
epoch:162, batch9, load frames use: 3.56114602089s
step 7460/400000: train loss: 0.185124
epoch:162, batch10, load frames use: 3.70749902725s
step 7461/400000: train loss: 0.148363
epoch:162, batch11, load frames use: 3.73759293556s
step 7462/400000: train loss: 0.184568
epoch:162, batch12, load frames use: 3.76868700981s
step 7463/400000: train loss: 0.144294
epoch:162, batch13, load frames use: 3.76545405388s
step 7464/400000: train loss: 0.171170
epoch:162, batch14, load frames use: 3.80622410774s
step 7465/400000: train loss: 0.149631
epoch:162, batch15, load frames use: 3.93703889847s
step 7466/400000: train loss: 0.146385
epoch:162, batch16, load frames use: 3.69297194481s
step 7467/400000: train loss: 0.175455
epoch:162, batch17, load frames use: 3.48779296875s
step 7468/400000: train loss: 0.169842
epoch:162, batch18, load frames use: 3.46250700951s
step 7469/400000: train loss: 0.166407
epoch:162, batch19, load frames use: 3.90201997757s
step 7470/400000: train loss: 0.158607
epoch:162, batch20, load frames use: 3.51772999763s
step 7471/400000: train loss: 0.170109
epoch:162, batch21, load frames use: 3.80515909195s
step 7472/400000: train loss: 0.155836
epoch:162, batch22, load frames use: 4.03418803215s
step 7473/400000: train loss: 0.154922
epoch:162, batch23, load frames use: 3.68940401077s
step 7474/400000: train loss: 0.162602
epoch:162, batch24, load frames use: 3.83607006073s
step 7475/400000: train loss: 0.162233
epoch:162, batch25, load frames use: 3.74556493759s
step 7476/400000: train loss: 0.178927
epoch:162, batch26, load frames use: 3.55880188942s
step 7477/400000: train loss: 0.155651
epoch:162, batch27, load frames use: 3.93340301514s
step 7478/400000: train loss: 0.159774
epoch:162, batch28, load frames use: 4.13465094566s
step 7479/400000: train loss: 0.165360
epoch:162, batch29, load frames use: 3.94919109344s
step 7480/400000: train loss: 0.167179
epoch:162, batch30, load frames use: 3.65312790871s
step 7481/400000: train loss: 0.164821
epoch:162, batch31, load frames use: 4.05721187592s
step 7482/400000: train loss: 0.177768
epoch:162, batch32, load frames use: 3.64213299751s
step 7483/400000: train loss: 0.163293
epoch:162, batch33, load frames use: 3.62343406677s
step 7484/400000: train loss: 0.162703
epoch:162, batch34, load frames use: 3.81624603271s
step 7485/400000: train loss: 0.164448
epoch:162, batch35, load frames use: 3.41686081886s
step 7486/400000: train loss: 0.172380
epoch:162, batch36, load frames use: 3.54481196404s
step 7487/400000: train loss: 0.148067
epoch:162, batch37, load frames use: 3.09847784042s
step 7488/400000: train loss: 0.176675
epoch:162, batch38, load frames use: 10.2383549213s
step 7489/400000: train loss: 0.144310
epoch:162, batch39, load frames use: 6.50479507446s
step 7490/400000: train loss: 0.188523
epoch:162, batch40, load frames use: 7.65371203423s
step 7491/400000: train loss: 0.168576
epoch:162, batch41, load frames use: 9.00991296768s
step 7492/400000: train loss: 0.174065
epoch:162, batch42, load frames use: 6.36535310745s
step 7493/400000: train loss: 0.158784
epoch:162, batch43, load frames use: 3.90480494499s
step 7494/400000: train loss: 0.181627
epoch:162, batch44, load frames use: 11.4869539738s
step 7495/400000: train loss: 0.154159
epoch:162, batch45, load frames use: 7.57150292397s
step 7496/400000: train loss: 0.163873
epoch:162, batch46, load frames use: 6.3037109375s
step 7497/400000: train loss: 0.165584
epoch:163, batch1, load frames use: 4.65469002724s
step 7498/400000: train loss: 0.149812
epoch:163, batch2, load frames use: 8.31990599632s
step 7499/400000: train loss: 0.182836
epoch:163, batch3, load frames use: 4.91021609306s
step 7500/400000: train loss: 0.167577
step 7500/400000: validation loss: 0.158329
epoch:163, batch4, load frames use: 6.74628591537s
step 7501/400000: train loss: 0.152909
epoch:163, batch5, load frames use: 8.45530104637s
step 7502/400000: train loss: 0.160384
epoch:163, batch6, load frames use: 11.1837611198s
step 7503/400000: train loss: 0.174832
epoch:163, batch7, load frames use: 7.30471086502s
step 7504/400000: train loss: 0.156206
epoch:163, batch8, load frames use: 11.6574800014s
step 7505/400000: train loss: 0.153800
epoch:163, batch9, load frames use: 6.34665513039s
step 7506/400000: train loss: 0.171037
epoch:163, batch10, load frames use: 3.32448101044s
step 7507/400000: train loss: 0.169283
epoch:163, batch11, load frames use: 7.27318501472s
step 7508/400000: train loss: 0.150222
epoch:163, batch12, load frames use: 6.47612094879s
step 7509/400000: train loss: 0.153409
epoch:163, batch13, load frames use: 13.3610579967s
step 7510/400000: train loss: 0.165216
epoch:163, batch14, load frames use: 8.06184196472s
step 7511/400000: train loss: 0.139341
epoch:163, batch15, load frames use: 8.78575396538s
step 7512/400000: train loss: 0.160839
epoch:163, batch16, load frames use: 12.5468239784s
step 7513/400000: train loss: 0.165616
epoch:163, batch17, load frames use: 6.36766290665s
step 7514/400000: train loss: 0.154185
epoch:163, batch18, load frames use: 6.84355807304s
step 7515/400000: train loss: 0.171621
epoch:163, batch19, load frames use: 7.65797805786s
step 7516/400000: train loss: 0.151372
epoch:163, batch20, load frames use: 6.28875494003s
step 7517/400000: train loss: 0.157097
epoch:163, batch21, load frames use: 6.28897094727s
step 7518/400000: train loss: 0.150469
epoch:163, batch22, load frames use: 8.62222695351s
step 7519/400000: train loss: 0.176428
epoch:163, batch23, load frames use: 6.84316396713s
step 7520/400000: train loss: 0.170738
epoch:163, batch24, load frames use: 7.05270886421s
step 7521/400000: train loss: 0.145878
epoch:163, batch25, load frames use: 3.75652503967s
step 7522/400000: train loss: 0.152323
epoch:163, batch26, load frames use: 3.60090208054s
step 7523/400000: train loss: 0.164092
epoch:163, batch27, load frames use: 3.58235287666s
step 7524/400000: train loss: 0.145451
epoch:163, batch28, load frames use: 3.40886378288s
step 7525/400000: train loss: 0.185213
epoch:163, batch29, load frames use: 4.75284099579s
step 7526/400000: train loss: 0.177556
epoch:163, batch30, load frames use: 2.93726110458s
step 7527/400000: train loss: 0.162772
epoch:163, batch31, load frames use: 5.06284594536s
step 7528/400000: train loss: 0.177494
epoch:163, batch32, load frames use: 5.40551805496s
step 7529/400000: train loss: 0.175485
epoch:163, batch33, load frames use: 4.91819906235s
step 7530/400000: train loss: 0.167787
epoch:163, batch34, load frames use: 5.86245894432s
step 7531/400000: train loss: 0.149710
epoch:163, batch35, load frames use: 5.61325192451s
step 7532/400000: train loss: 0.158012
epoch:163, batch36, load frames use: 5.63032507896s
step 7533/400000: train loss: 0.146120
epoch:163, batch37, load frames use: 5.21998596191s
step 7534/400000: train loss: 0.179770
epoch:163, batch38, load frames use: 5.47831606865s
step 7535/400000: train loss: 0.150438
epoch:163, batch39, load frames use: 5.44475412369s
step 7536/400000: train loss: 0.163514
epoch:163, batch40, load frames use: 4.93047595024s
step 7537/400000: train loss: 0.169331
epoch:163, batch41, load frames use: 3.74711108208s
step 7538/400000: train loss: 0.168824
epoch:163, batch42, load frames use: 9.4165148735s
step 7539/400000: train loss: 0.169409
epoch:163, batch43, load frames use: 3.38712906837s
step 7540/400000: train loss: 0.170269
epoch:163, batch44, load frames use: 11.6461040974s
step 7541/400000: train loss: 0.155641
epoch:163, batch45, load frames use: 11.9429130554s
step 7542/400000: train loss: 0.153536
epoch:163, batch46, load frames use: 8.45993804932s
step 7543/400000: train loss: 0.160995
epoch:164, batch1, load frames use: 11.199244976s
step 7544/400000: train loss: 0.145106
epoch:164, batch2, load frames use: 3.62955999374s
step 7545/400000: train loss: 0.155669
epoch:164, batch3, load frames use: 3.35220193863s
step 7546/400000: train loss: 0.158779
epoch:164, batch4, load frames use: 3.50738501549s
step 7547/400000: train loss: 0.174188
epoch:164, batch5, load frames use: 4.31660604477s
step 7548/400000: train loss: 0.155457
epoch:164, batch6, load frames use: 4.90814185143s
step 7549/400000: train loss: 0.173222
epoch:164, batch7, load frames use: 3.74517297745s
step 7550/400000: train loss: 0.151060
epoch:164, batch8, load frames use: 3.36961102486s
step 7551/400000: train loss: 0.157531
epoch:164, batch9, load frames use: 4.28412389755s
step 7552/400000: train loss: 0.180701
epoch:164, batch10, load frames use: 3.54687404633s
step 7553/400000: train loss: 0.155986
epoch:164, batch11, load frames use: 6.10834288597s
step 7554/400000: train loss: 0.149526
epoch:164, batch12, load frames use: 3.98430895805s
step 7555/400000: train loss: 0.165619
epoch:164, batch13, load frames use: 3.38725495338s
step 7556/400000: train loss: 0.145379
epoch:164, batch14, load frames use: 5.95267796516s
step 7557/400000: train loss: 0.162177
epoch:164, batch15, load frames use: 3.45488595963s
step 7558/400000: train loss: 0.158320
epoch:164, batch16, load frames use: 4.06823396683s
step 7559/400000: train loss: 0.154405
epoch:164, batch17, load frames use: 4.45377993584s
step 7560/400000: train loss: 0.171483
epoch:164, batch18, load frames use: 3.41970896721s
step 7561/400000: train loss: 0.162059
epoch:164, batch19, load frames use: 4.83231687546s
step 7562/400000: train loss: 0.174712
epoch:164, batch20, load frames use: 3.32960391045s
step 7563/400000: train loss: 0.148508
epoch:164, batch21, load frames use: 3.13236093521s
step 7564/400000: train loss: 0.144598
epoch:164, batch22, load frames use: 3.23224616051s
step 7565/400000: train loss: 0.169151
epoch:164, batch23, load frames use: 3.22291398048s
step 7566/400000: train loss: 0.154549
epoch:164, batch24, load frames use: 3.53231406212s
step 7567/400000: train loss: 0.150039
epoch:164, batch25, load frames use: 3.43922901154s
step 7568/400000: train loss: 0.147621
epoch:164, batch26, load frames use: 3.83785796165s
step 7569/400000: train loss: 0.159784
epoch:164, batch27, load frames use: 5.16062402725s
step 7570/400000: train loss: 0.161856
epoch:164, batch28, load frames use: 3.38231492043s
step 7571/400000: train loss: 0.132882
epoch:164, batch29, load frames use: 5.69492721558s
step 7572/400000: train loss: 0.172311
epoch:164, batch30, load frames use: 5.66487407684s
step 7573/400000: train loss: 0.164411
epoch:164, batch31, load frames use: 4.10090994835s
step 7574/400000: train loss: 0.199814
epoch:164, batch32, load frames use: 6.18577098846s
step 7575/400000: train loss: 0.178254
epoch:164, batch33, load frames use: 3.67638897896s
step 7576/400000: train loss: 0.162628
epoch:164, batch34, load frames use: 4.49468016624s
step 7577/400000: train loss: 0.164020
epoch:164, batch35, load frames use: 5.62296891212s
step 7578/400000: train loss: 0.161851
epoch:164, batch36, load frames use: 3.3751039505s
step 7579/400000: train loss: 0.179758
epoch:164, batch37, load frames use: 3.35979008675s
step 7580/400000: train loss: 0.162201
epoch:164, batch38, load frames use: 3.13840985298s
step 7581/400000: train loss: 0.158778
epoch:164, batch39, load frames use: 3.28255391121s
step 7582/400000: train loss: 0.185814
epoch:164, batch40, load frames use: 3.36261487007s
step 7583/400000: train loss: 0.197571
epoch:164, batch41, load frames use: 3.82329416275s
step 7584/400000: train loss: 0.181837
epoch:164, batch42, load frames use: 4.62971901894s
step 7585/400000: train loss: 0.158556
epoch:164, batch43, load frames use: 3.44088888168s
step 7586/400000: train loss: 0.173076
epoch:164, batch44, load frames use: 4.63391709328s
step 7587/400000: train loss: 0.154150
epoch:164, batch45, load frames use: 3.6909840107s
step 7588/400000: train loss: 0.147965
epoch:164, batch46, load frames use: 4.19616580009s
step 7589/400000: train loss: 0.169218
epoch:165, batch1, load frames use: 6.06442284584s
step 7590/400000: train loss: 0.149033
epoch:165, batch2, load frames use: 6.30231809616s
step 7591/400000: train loss: 0.159479
epoch:165, batch3, load frames use: 5.03591108322s
step 7592/400000: train loss: 0.171482
epoch:165, batch4, load frames use: 5.83650302887s
step 7593/400000: train loss: 0.176281
epoch:165, batch5, load frames use: 6.48270821571s
step 7594/400000: train loss: 0.160461
epoch:165, batch6, load frames use: 5.92374610901s
step 7595/400000: train loss: 0.155415
epoch:165, batch7, load frames use: 6.14463400841s
step 7596/400000: train loss: 0.160455
epoch:165, batch8, load frames use: 8.37168884277s
step 7597/400000: train loss: 0.168395
epoch:165, batch9, load frames use: 7.21852993965s
step 7598/400000: train loss: 0.150427
epoch:165, batch10, load frames use: 3.73840713501s
step 7599/400000: train loss: 0.168127
epoch:165, batch11, load frames use: 7.80748295784s
step 7600/400000: train loss: 0.156419
step 7600/400000: validation loss: 0.160720
epoch:165, batch12, load frames use: 8.36165904999s
step 7601/400000: train loss: 0.182392
epoch:165, batch13, load frames use: 3.87714886665s
step 7602/400000: train loss: 0.156369
epoch:165, batch14, load frames use: 4.21623110771s
step 7603/400000: train loss: 0.152040
epoch:165, batch15, load frames use: 6.48276495934s
step 7604/400000: train loss: 0.161673
epoch:165, batch16, load frames use: 3.72125792503s
step 7605/400000: train loss: 0.170805
epoch:165, batch17, load frames use: 5.07754993439s
step 7606/400000: train loss: 0.174617
epoch:165, batch18, load frames use: 3.48018097878s
step 7607/400000: train loss: 0.166177
epoch:165, batch19, load frames use: 3.52125811577s
step 7608/400000: train loss: 0.170587
epoch:165, batch20, load frames use: 4.40833187103s
step 7609/400000: train loss: 0.181429
epoch:165, batch21, load frames use: 3.11861491203s
step 7610/400000: train loss: 0.167170
epoch:165, batch22, load frames use: 4.53778910637s
step 7611/400000: train loss: 0.168930
epoch:165, batch23, load frames use: 3.78266382217s
step 7612/400000: train loss: 0.155147
epoch:165, batch24, load frames use: 3.53740000725s
step 7613/400000: train loss: 0.154212
epoch:165, batch25, load frames use: 7.08940982819s
step 7614/400000: train loss: 0.153875
epoch:165, batch26, load frames use: 6.04854202271s
step 7615/400000: train loss: 0.171263
epoch:165, batch27, load frames use: 6.27056384087s
step 7616/400000: train loss: 0.168856
epoch:165, batch28, load frames use: 6.67123603821s
step 7617/400000: train loss: 0.177389
epoch:165, batch29, load frames use: 6.55117702484s
step 7618/400000: train loss: 0.153710
epoch:165, batch30, load frames use: 6.95421886444s
step 7619/400000: train loss: 0.177247
epoch:165, batch31, load frames use: 6.04179406166s
step 7620/400000: train loss: 0.160358
epoch:165, batch32, load frames use: 4.0943300724s
step 7621/400000: train loss: 0.147943
epoch:165, batch33, load frames use: 4.79367995262s
step 7622/400000: train loss: 0.173193
epoch:165, batch34, load frames use: 3.86928796768s
step 7623/400000: train loss: 0.163338
epoch:165, batch35, load frames use: 4.92297291756s
step 7624/400000: train loss: 0.157332
epoch:165, batch36, load frames use: 4.57590794563s
step 7625/400000: train loss: 0.140079
epoch:165, batch37, load frames use: 7.13650584221s
step 7626/400000: train loss: 0.184102
epoch:165, batch38, load frames use: 5.6299021244s
step 7627/400000: train loss: 0.160746
epoch:165, batch39, load frames use: 4.34793400764s
step 7628/400000: train loss: 0.194062
epoch:165, batch40, load frames use: 6.24154806137s
step 7629/400000: train loss: 0.167526
epoch:165, batch41, load frames use: 5.44888615608s
step 7630/400000: train loss: 0.151616
epoch:165, batch42, load frames use: 9.0703599453s
step 7631/400000: train loss: 0.161116
epoch:165, batch43, load frames use: 19.0004889965s
step 7632/400000: train loss: 0.157794
epoch:165, batch44, load frames use: 7.64922308922s
step 7633/400000: train loss: 0.172207
epoch:165, batch45, load frames use: 9.54498291016s
step 7634/400000: train loss: 0.163812
epoch:165, batch46, load frames use: 6.97214889526s
step 7635/400000: train loss: 0.155684
epoch:166, batch1, load frames use: 8.66821312904s
step 7636/400000: train loss: 0.171880
epoch:166, batch2, load frames use: 11.4221289158s
step 7637/400000: train loss: 0.158890
epoch:166, batch3, load frames use: 9.25170302391s
step 7638/400000: train loss: 0.163705
epoch:166, batch4, load frames use: 8.71209001541s
step 7639/400000: train loss: 0.152671
epoch:166, batch5, load frames use: 4.0478348732s
step 7640/400000: train loss: 0.150604
epoch:166, batch6, load frames use: 7.7626209259s
step 7641/400000: train loss: 0.145738
epoch:166, batch7, load frames use: 3.38259601593s
step 7642/400000: train loss: 0.154801
epoch:166, batch8, load frames use: 3.3575720787s
step 7643/400000: train loss: 0.162738
epoch:166, batch9, load frames use: 3.46017217636s
step 7644/400000: train loss: 0.152850
epoch:166, batch10, load frames use: 3.82387208939s
step 7645/400000: train loss: 0.134366
epoch:166, batch11, load frames use: 5.31967401505s
step 7646/400000: train loss: 0.152972
epoch:166, batch12, load frames use: 9.10879206657s
step 7647/400000: train loss: 0.162712
epoch:166, batch13, load frames use: 9.41445207596s
step 7648/400000: train loss: 0.152385
epoch:166, batch14, load frames use: 3.29234004021s
step 7649/400000: train loss: 0.146240
epoch:166, batch15, load frames use: 13.0998520851s
step 7650/400000: train loss: 0.160807
epoch:166, batch16, load frames use: 8.35636210442s
step 7651/400000: train loss: 0.160586
epoch:166, batch17, load frames use: 2.96571493149s
step 7652/400000: train loss: 0.156492
epoch:166, batch18, load frames use: 3.89159989357s
step 7653/400000: train loss: 0.150252
epoch:166, batch19, load frames use: 4.06084704399s
step 7654/400000: train loss: 0.168111
epoch:166, batch20, load frames use: 3.07560586929s
step 7655/400000: train loss: 0.161450
epoch:166, batch21, load frames use: 3.6045601368s
step 7656/400000: train loss: 0.176521
epoch:166, batch22, load frames use: 4.75316286087s
step 7657/400000: train loss: 0.166451
epoch:166, batch23, load frames use: 2.8428299427s
step 7658/400000: train loss: 0.158866
epoch:166, batch24, load frames use: 4.17769908905s
step 7659/400000: train loss: 0.170658
epoch:166, batch25, load frames use: 6.31534695625s
step 7660/400000: train loss: 0.157885
epoch:166, batch26, load frames use: 3.41114878654s
step 7661/400000: train loss: 0.147756
epoch:166, batch27, load frames use: 3.66685891151s
step 7662/400000: train loss: 0.168795
epoch:166, batch28, load frames use: 3.22135686874s
step 7663/400000: train loss: 0.155102
epoch:166, batch29, load frames use: 2.93241500854s
step 7664/400000: train loss: 0.152604
epoch:166, batch30, load frames use: 3.57699084282s
step 7665/400000: train loss: 0.156571
epoch:166, batch31, load frames use: 3.26463484764s
step 7666/400000: train loss: 0.163937
epoch:166, batch32, load frames use: 5.87886095047s
step 7667/400000: train loss: 0.161694
epoch:166, batch33, load frames use: 4.2565639019s
step 7668/400000: train loss: 0.161448
epoch:166, batch34, load frames use: 3.38307285309s
step 7669/400000: train loss: 0.144306
epoch:166, batch35, load frames use: 7.67402005196s
step 7670/400000: train loss: 0.185837
epoch:166, batch36, load frames use: 3.20846796036s
step 7671/400000: train loss: 0.167437
epoch:166, batch37, load frames use: 6.48783612251s
step 7672/400000: train loss: 0.154527
epoch:166, batch38, load frames use: 5.09584593773s
step 7673/400000: train loss: 0.153626
epoch:166, batch39, load frames use: 3.65658998489s
step 7674/400000: train loss: 0.143983
epoch:166, batch40, load frames use: 5.66555905342s
step 7675/400000: train loss: 0.158789
epoch:166, batch41, load frames use: 3.15510296822s
step 7676/400000: train loss: 0.152933
epoch:166, batch42, load frames use: 3.27628803253s
step 7677/400000: train loss: 0.170156
epoch:166, batch43, load frames use: 5.62595891953s
step 7678/400000: train loss: 0.152935
epoch:166, batch44, load frames use: 6.22984790802s
step 7679/400000: train loss: 0.164550
epoch:166, batch45, load frames use: 4.54074287415s
step 7680/400000: train loss: 0.146369
epoch:166, batch46, load frames use: 3.21263098717s
step 7681/400000: train loss: 0.166517
epoch:167, batch1, load frames use: 4.78757691383s
step 7682/400000: train loss: 0.150336
epoch:167, batch2, load frames use: 3.73717498779s
step 7683/400000: train loss: 0.180873
epoch:167, batch3, load frames use: 4.75109791756s
step 7684/400000: train loss: 0.163182
epoch:167, batch4, load frames use: 3.60885882378s
step 7685/400000: train loss: 0.146220
epoch:167, batch5, load frames use: 3.01751303673s
step 7686/400000: train loss: 0.183467
epoch:167, batch6, load frames use: 5.28254199028s
step 7687/400000: train loss: 0.161248
epoch:167, batch7, load frames use: 2.91180801392s
step 7688/400000: train loss: 0.164965
epoch:167, batch8, load frames use: 3.35407996178s
step 7689/400000: train loss: 0.148470
epoch:167, batch9, load frames use: 3.03599691391s
step 7690/400000: train loss: 0.164084
epoch:167, batch10, load frames use: 4.31224107742s
step 7691/400000: train loss: 0.143515
epoch:167, batch11, load frames use: 4.52796387672s
step 7692/400000: train loss: 0.151157
epoch:167, batch12, load frames use: 5.51951503754s
step 7693/400000: train loss: 0.149977
epoch:167, batch13, load frames use: 3.08466315269s
step 7694/400000: train loss: 0.161666
epoch:167, batch14, load frames use: 4.74017691612s
step 7695/400000: train loss: 0.161095
epoch:167, batch15, load frames use: 7.18808102608s
step 7696/400000: train loss: 0.148037
epoch:167, batch16, load frames use: 3.84214711189s
step 7697/400000: train loss: 0.157982
epoch:167, batch17, load frames use: 2.97807192802s
step 7698/400000: train loss: 0.167276
epoch:167, batch18, load frames use: 2.85045409203s
step 7699/400000: train loss: 0.146768
epoch:167, batch19, load frames use: 3.2994248867s
step 7700/400000: train loss: 0.155559
step 7700/400000: validation loss: 0.142013
epoch:167, batch20, load frames use: 2.98389291763s
step 7701/400000: train loss: 0.160544
epoch:167, batch21, load frames use: 3.60838007927s
step 7702/400000: train loss: 0.180643
epoch:167, batch22, load frames use: 5.15868401527s
step 7703/400000: train loss: 0.181930
epoch:167, batch23, load frames use: 5.83695793152s
step 7704/400000: train loss: 0.143534
epoch:167, batch24, load frames use: 6.48372888565s
step 7705/400000: train loss: 0.171739
epoch:167, batch25, load frames use: 6.2374830246s
step 7706/400000: train loss: 0.148133
epoch:167, batch26, load frames use: 5.63386487961s
step 7707/400000: train loss: 0.143984
epoch:167, batch27, load frames use: 6.036247015s
step 7708/400000: train loss: 0.164353
epoch:167, batch28, load frames use: 6.41447091103s
step 7709/400000: train loss: 0.176744
epoch:167, batch29, load frames use: 3.72511291504s
step 7710/400000: train loss: 0.169541
epoch:167, batch30, load frames use: 3.40410399437s
step 7711/400000: train loss: 0.147739
epoch:167, batch31, load frames use: 3.23232293129s
step 7712/400000: train loss: 0.154607
epoch:167, batch32, load frames use: 2.96088695526s
step 7713/400000: train loss: 0.146076
epoch:167, batch33, load frames use: 3.06546592712s
step 7714/400000: train loss: 0.167630
epoch:167, batch34, load frames use: 3.1529841423s
step 7715/400000: train loss: 0.189731
epoch:167, batch35, load frames use: 3.46493291855s
step 7716/400000: train loss: 0.144113
epoch:167, batch36, load frames use: 3.33254885674s
step 7717/400000: train loss: 0.157419
epoch:167, batch37, load frames use: 3.06964707375s
step 7718/400000: train loss: 0.151637
epoch:167, batch38, load frames use: 3.75266098976s
step 7719/400000: train loss: 0.155924
epoch:167, batch39, load frames use: 3.42639088631s
step 7720/400000: train loss: 0.148877
epoch:167, batch40, load frames use: 3.27260088921s
step 7721/400000: train loss: 0.169036
epoch:167, batch41, load frames use: 3.29006004333s
step 7722/400000: train loss: 0.161027
epoch:167, batch42, load frames use: 3.23638081551s
step 7723/400000: train loss: 0.185919
epoch:167, batch43, load frames use: 3.09222602844s
step 7724/400000: train loss: 0.146968
epoch:167, batch44, load frames use: 3.68248295784s
step 7725/400000: train loss: 0.165367
epoch:167, batch45, load frames use: 3.71830010414s
step 7726/400000: train loss: 0.156025
epoch:167, batch46, load frames use: 3.21201896667s
step 7727/400000: train loss: 0.160406
epoch:168, batch1, load frames use: 3.63041901588s
step 7728/400000: train loss: 0.141519
epoch:168, batch2, load frames use: 3.56635689735s
step 7729/400000: train loss: 0.149943
epoch:168, batch3, load frames use: 3.33794403076s
step 7730/400000: train loss: 0.147863
epoch:168, batch4, load frames use: 3.56662416458s
step 7731/400000: train loss: 0.154458
epoch:168, batch5, load frames use: 3.1548178196s
step 7732/400000: train loss: 0.159402
epoch:168, batch6, load frames use: 3.13047003746s
step 7733/400000: train loss: 0.142880
epoch:168, batch7, load frames use: 3.22345900536s
step 7734/400000: train loss: 0.154158
epoch:168, batch8, load frames use: 3.20525598526s
step 7735/400000: train loss: 0.154535
epoch:168, batch9, load frames use: 4.14824390411s
step 7736/400000: train loss: 0.172684
epoch:168, batch10, load frames use: 3.20532894135s
step 7737/400000: train loss: 0.179364
epoch:168, batch11, load frames use: 3.4372549057s
step 7738/400000: train loss: 0.155529
epoch:168, batch12, load frames use: 3.47606515884s
step 7739/400000: train loss: 0.165359
epoch:168, batch13, load frames use: 3.13643693924s
step 7740/400000: train loss: 0.153234
epoch:168, batch14, load frames use: 3.10458612442s
step 7741/400000: train loss: 0.148973
epoch:168, batch15, load frames use: 3.34664702415s
step 7742/400000: train loss: 0.167057
epoch:168, batch16, load frames use: 3.61921906471s
step 7743/400000: train loss: 0.168637
epoch:168, batch17, load frames use: 3.30978918076s
step 7744/400000: train loss: 0.167550
epoch:168, batch18, load frames use: 4.010242939s
step 7745/400000: train loss: 0.155672
epoch:168, batch19, load frames use: 3.29494905472s
step 7746/400000: train loss: 0.152055
epoch:168, batch20, load frames use: 3.17129087448s
step 7747/400000: train loss: 0.165414
epoch:168, batch21, load frames use: 3.30190300941s
step 7748/400000: train loss: 0.164946
epoch:168, batch22, load frames use: 3.16056203842s
step 7749/400000: train loss: 0.149004
epoch:168, batch23, load frames use: 4.10558700562s
step 7750/400000: train loss: 0.157929
epoch:168, batch24, load frames use: 3.15513491631s
step 7751/400000: train loss: 0.162457
epoch:168, batch25, load frames use: 3.32797312737s
step 7752/400000: train loss: 0.153494
epoch:168, batch26, load frames use: 3.17879700661s
step 7753/400000: train loss: 0.159286
epoch:168, batch27, load frames use: 3.4651260376s
step 7754/400000: train loss: 0.159888
epoch:168, batch28, load frames use: 3.12299895287s
step 7755/400000: train loss: 0.171789
epoch:168, batch29, load frames use: 3.61467313766s
step 7756/400000: train loss: 0.149162
epoch:168, batch30, load frames use: 3.17467904091s
step 7757/400000: train loss: 0.167860
epoch:168, batch31, load frames use: 3.38052606583s
step 7758/400000: train loss: 0.149232
epoch:168, batch32, load frames use: 4.13534212112s
step 7759/400000: train loss: 0.174031
epoch:168, batch33, load frames use: 3.17700481415s
step 7760/400000: train loss: 0.171059
epoch:168, batch34, load frames use: 4.14751696587s
step 7761/400000: train loss: 0.158611
epoch:168, batch35, load frames use: 3.09883999825s
step 7762/400000: train loss: 0.155294
epoch:168, batch36, load frames use: 3.65433001518s
step 7763/400000: train loss: 0.162684
epoch:168, batch37, load frames use: 3.04015398026s
step 7764/400000: train loss: 0.166737
epoch:168, batch38, load frames use: 3.67259788513s
step 7765/400000: train loss: 0.165230
epoch:168, batch39, load frames use: 3.08976197243s
step 7766/400000: train loss: 0.155767
epoch:168, batch40, load frames use: 3.09369587898s
step 7767/400000: train loss: 0.171857
epoch:168, batch41, load frames use: 2.99708199501s
step 7768/400000: train loss: 0.168793
epoch:168, batch42, load frames use: 3.21804690361s
step 7769/400000: train loss: 0.148508
epoch:168, batch43, load frames use: 3.02640891075s
step 7770/400000: train loss: 0.163237
epoch:168, batch44, load frames use: 2.89967083931s
step 7771/400000: train loss: 0.178488
epoch:168, batch45, load frames use: 3.22230601311s
step 7772/400000: train loss: 0.178007
epoch:168, batch46, load frames use: 2.63800406456s
step 7773/400000: train loss: 0.157816
epoch:169, batch1, load frames use: 2.55661392212s
step 7774/400000: train loss: 0.147955
epoch:169, batch2, load frames use: 3.30647301674s
step 7775/400000: train loss: 0.145759
epoch:169, batch3, load frames use: 2.94251608849s
step 7776/400000: train loss: 0.162298
epoch:169, batch4, load frames use: 3.49721097946s
step 7777/400000: train loss: 0.160140
epoch:169, batch5, load frames use: 3.09633803368s
step 7778/400000: train loss: 0.151667
epoch:169, batch6, load frames use: 2.69365596771s
step 7779/400000: train loss: 0.151052
epoch:169, batch7, load frames use: 2.57826304436s
step 7780/400000: train loss: 0.150749
epoch:169, batch8, load frames use: 2.96246910095s
step 7781/400000: train loss: 0.145911
epoch:169, batch9, load frames use: 3.08971500397s
step 7782/400000: train loss: 0.157961
epoch:169, batch10, load frames use: 2.58038902283s
step 7783/400000: train loss: 0.158901
epoch:169, batch11, load frames use: 2.98165488243s
step 7784/400000: train loss: 0.153643
epoch:169, batch12, load frames use: 3.35421490669s
step 7785/400000: train loss: 0.147371
epoch:169, batch13, load frames use: 2.65385198593s
step 7786/400000: train loss: 0.161604
epoch:169, batch14, load frames use: 2.81807494164s
step 7787/400000: train loss: 0.165219
epoch:169, batch15, load frames use: 2.84726023674s
step 7788/400000: train loss: 0.145398
epoch:169, batch16, load frames use: 3.19298791885s
step 7789/400000: train loss: 0.151110
epoch:169, batch17, load frames use: 2.67524290085s
step 7790/400000: train loss: 0.146202
epoch:169, batch18, load frames use: 2.70476603508s
step 7791/400000: train loss: 0.141544
epoch:169, batch19, load frames use: 3.02120399475s
step 7792/400000: train loss: 0.143875
epoch:169, batch20, load frames use: 2.90930080414s
step 7793/400000: train loss: 0.151478
epoch:169, batch21, load frames use: 3.08382797241s
step 7794/400000: train loss: 0.152849
epoch:169, batch22, load frames use: 3.41374397278s
step 7795/400000: train loss: 0.147451
epoch:169, batch23, load frames use: 2.77443408966s
step 7796/400000: train loss: 0.158361
epoch:169, batch24, load frames use: 3.11231708527s
step 7797/400000: train loss: 0.160176
epoch:169, batch25, load frames use: 2.92913508415s
step 7798/400000: train loss: 0.159988
epoch:169, batch26, load frames use: 2.6792371273s
step 7799/400000: train loss: 0.155716
epoch:169, batch27, load frames use: 3.18747997284s
step 7800/400000: train loss: 0.161820
step 7800/400000: validation loss: 0.156394
epoch:169, batch28, load frames use: 2.56901192665s
step 7801/400000: train loss: 0.157444
epoch:169, batch29, load frames use: 3.22989916801s
step 7802/400000: train loss: 0.169830
epoch:169, batch30, load frames use: 3.01098799706s
step 7803/400000: train loss: 0.161268
epoch:169, batch31, load frames use: 3.15995907784s
step 7804/400000: train loss: 0.161558
epoch:169, batch32, load frames use: 2.65040183067s
step 7805/400000: train loss: 0.168298
epoch:169, batch33, load frames use: 2.93223810196s
step 7806/400000: train loss: 0.157765
epoch:169, batch34, load frames use: 2.88923120499s
step 7807/400000: train loss: 0.167881
epoch:169, batch35, load frames use: 3.63728904724s
step 7808/400000: train loss: 0.170842
epoch:169, batch36, load frames use: 3.1489648819s
step 7809/400000: train loss: 0.145631
epoch:169, batch37, load frames use: 2.62827396393s
step 7810/400000: train loss: 0.157151
epoch:169, batch38, load frames use: 3.61008691788s
step 7811/400000: train loss: 0.182980
epoch:169, batch39, load frames use: 2.98404407501s
step 7812/400000: train loss: 0.157301
epoch:169, batch40, load frames use: 3.04988098145s
step 7813/400000: train loss: 0.151784
epoch:169, batch41, load frames use: 3.0278570652s
step 7814/400000: train loss: 0.152786
epoch:169, batch42, load frames use: 2.72433209419s
step 7815/400000: train loss: 0.182619
epoch:169, batch43, load frames use: 2.63610005379s
step 7816/400000: train loss: 0.157159
epoch:169, batch44, load frames use: 2.6777510643s
step 7817/400000: train loss: 0.167661
epoch:169, batch45, load frames use: 2.97840809822s
step 7818/400000: train loss: 0.153839
epoch:169, batch46, load frames use: 3.25930404663s
step 7819/400000: train loss: 0.141031
epoch:170, batch1, load frames use: 3.12725996971s
step 7820/400000: train loss: 0.148463
epoch:170, batch2, load frames use: 3.24812197685s
step 7821/400000: train loss: 0.170286
epoch:170, batch3, load frames use: 2.70867395401s
step 7822/400000: train loss: 0.147376
epoch:170, batch4, load frames use: 2.75822997093s
step 7823/400000: train loss: 0.154074
epoch:170, batch5, load frames use: 2.62871193886s
step 7824/400000: train loss: 0.154809
epoch:170, batch6, load frames use: 2.9710547924s
step 7825/400000: train loss: 0.147487
epoch:170, batch7, load frames use: 3.32546591759s
step 7826/400000: train loss: 0.136964
epoch:170, batch8, load frames use: 3.10735797882s
step 7827/400000: train loss: 0.161360
epoch:170, batch9, load frames use: 3.30012798309s
step 7828/400000: train loss: 0.158723
epoch:170, batch10, load frames use: 3.16291499138s
step 7829/400000: train loss: 0.158622
epoch:170, batch11, load frames use: 4.07674503326s
step 7830/400000: train loss: 0.152875
epoch:170, batch12, load frames use: 4.7962770462s
step 7831/400000: train loss: 0.141089
epoch:170, batch13, load frames use: 4.55673718452s
step 7832/400000: train loss: 0.141137
epoch:170, batch14, load frames use: 4.8286781311s
step 7833/400000: train loss: 0.147320
epoch:170, batch15, load frames use: 3.90911412239s
step 7834/400000: train loss: 0.148868
epoch:170, batch16, load frames use: 3.41600608826s
step 7835/400000: train loss: 0.163872
epoch:170, batch17, load frames use: 4.13271403313s
step 7836/400000: train loss: 0.152969
epoch:170, batch18, load frames use: 4.07722020149s
step 7837/400000: train loss: 0.152352
epoch:170, batch19, load frames use: 4.14693403244s
step 7838/400000: train loss: 0.150798
epoch:170, batch20, load frames use: 4.05015921593s
step 7839/400000: train loss: 0.155587
epoch:170, batch21, load frames use: 4.40582299232s
step 7840/400000: train loss: 0.145945
epoch:170, batch22, load frames use: 5.11004590988s
step 7841/400000: train loss: 0.154053
epoch:170, batch23, load frames use: 3.44308209419s
step 7842/400000: train loss: 0.158256
epoch:170, batch24, load frames use: 4.79977989197s
step 7843/400000: train loss: 0.141166
epoch:170, batch25, load frames use: 5.55397987366s
step 7844/400000: train loss: 0.186607
epoch:170, batch26, load frames use: 3.74200797081s
step 7845/400000: train loss: 0.160328
epoch:170, batch27, load frames use: 3.17426395416s
step 7846/400000: train loss: 0.160126
epoch:170, batch28, load frames use: 2.88850498199s
step 7847/400000: train loss: 0.155242
epoch:170, batch29, load frames use: 3.94155693054s
step 7848/400000: train loss: 0.164715
epoch:170, batch30, load frames use: 4.15092396736s
step 7849/400000: train loss: 0.149851
epoch:170, batch31, load frames use: 5.13295197487s
step 7850/400000: train loss: 0.147789
epoch:170, batch32, load frames use: 3.14722704887s
step 7851/400000: train loss: 0.169562
epoch:170, batch33, load frames use: 2.76003503799s
step 7852/400000: train loss: 0.151327
epoch:170, batch34, load frames use: 2.82605695724s
step 7853/400000: train loss: 0.164077
epoch:170, batch35, load frames use: 2.69130086899s
step 7854/400000: train loss: 0.172914
epoch:170, batch36, load frames use: 3.15536904335s
step 7855/400000: train loss: 0.162720
epoch:170, batch37, load frames use: 3.15464901924s
step 7856/400000: train loss: 0.163693
epoch:170, batch38, load frames use: 2.56885099411s
step 7857/400000: train loss: 0.148528
epoch:170, batch39, load frames use: 3.39961600304s
step 7858/400000: train loss: 0.168043
epoch:170, batch40, load frames use: 2.91191601753s
step 7859/400000: train loss: 0.164930
epoch:170, batch41, load frames use: 2.5235118866s
step 7860/400000: train loss: 0.144756
epoch:170, batch42, load frames use: 3.08938193321s
step 7861/400000: train loss: 0.184234
epoch:170, batch43, load frames use: 3.05484390259s
step 7862/400000: train loss: 0.176444
epoch:170, batch44, load frames use: 3.36614513397s
step 7863/400000: train loss: 0.151749
epoch:170, batch45, load frames use: 4.10841298103s
step 7864/400000: train loss: 0.153900
epoch:170, batch46, load frames use: 5.47740101814s
step 7865/400000: train loss: 0.157779
epoch:171, batch1, load frames use: 5.65924501419s
step 7866/400000: train loss: 0.146896
epoch:171, batch2, load frames use: 5.92089509964s
step 7867/400000: train loss: 0.119454
epoch:171, batch3, load frames use: 5.38322496414s
step 7868/400000: train loss: 0.159993
epoch:171, batch4, load frames use: 6.21429300308s
step 7869/400000: train loss: 0.173759
epoch:171, batch5, load frames use: 5.88189387321s
step 7870/400000: train loss: 0.141064
epoch:171, batch6, load frames use: 4.92094397545s
step 7871/400000: train loss: 0.169554
epoch:171, batch7, load frames use: 3.02689909935s
step 7872/400000: train loss: 0.141217
epoch:171, batch8, load frames use: 3.09629797935s
step 7873/400000: train loss: 0.161414
epoch:171, batch9, load frames use: 3.39029192924s
step 7874/400000: train loss: 0.157236
epoch:171, batch10, load frames use: 3.18315410614s
step 7875/400000: train loss: 0.154884
epoch:171, batch11, load frames use: 3.14900803566s
step 7876/400000: train loss: 0.149458
epoch:171, batch12, load frames use: 3.37609410286s
step 7877/400000: train loss: 0.152651
epoch:171, batch13, load frames use: 3.07874798775s
step 7878/400000: train loss: 0.146099
epoch:171, batch14, load frames use: 3.22120904922s
step 7879/400000: train loss: 0.144977
epoch:171, batch15, load frames use: 2.82640814781s
step 7880/400000: train loss: 0.151127
epoch:171, batch16, load frames use: 2.88957500458s
step 7881/400000: train loss: 0.166625
epoch:171, batch17, load frames use: 3.50111103058s
step 7882/400000: train loss: 0.154273
epoch:171, batch18, load frames use: 2.92017412186s
step 7883/400000: train loss: 0.147774
epoch:171, batch19, load frames use: 2.51771903038s
step 7884/400000: train loss: 0.161190
epoch:171, batch20, load frames use: 3.34554195404s
step 7885/400000: train loss: 0.169882
epoch:171, batch21, load frames use: 3.20436906815s
step 7886/400000: train loss: 0.165225
epoch:171, batch22, load frames use: 3.22118902206s
step 7887/400000: train loss: 0.140423
epoch:171, batch23, load frames use: 2.98466515541s
step 7888/400000: train loss: 0.177788
epoch:171, batch24, load frames use: 3.24041104317s
step 7889/400000: train loss: 0.144582
epoch:171, batch25, load frames use: 2.60743689537s
step 7890/400000: train loss: 0.170066
epoch:171, batch26, load frames use: 3.82602787018s
step 7891/400000: train loss: 0.156617
epoch:171, batch27, load frames use: 3.34482312202s
step 7892/400000: train loss: 0.171404
epoch:171, batch28, load frames use: 2.70510792732s
step 7893/400000: train loss: 0.156221
epoch:171, batch29, load frames use: 3.40615320206s
step 7894/400000: train loss: 0.172498
epoch:171, batch30, load frames use: 3.11689996719s
step 7895/400000: train loss: 0.134558
epoch:171, batch31, load frames use: 2.50812697411s
step 7896/400000: train loss: 0.190333
epoch:171, batch32, load frames use: 3.00583481789s
step 7897/400000: train loss: 0.171702
epoch:171, batch33, load frames use: 2.50073981285s
step 7898/400000: train loss: 0.147159
epoch:171, batch34, load frames use: 2.98135900497s
step 7899/400000: train loss: 0.172701
epoch:171, batch35, load frames use: 3.27478981018s
step 7900/400000: train loss: 0.152748
step 7900/400000: validation loss: 0.143666
epoch:171, batch36, load frames use: 3.35375809669s
step 7901/400000: train loss: 0.158568
epoch:171, batch37, load frames use: 3.31901693344s
step 7902/400000: train loss: 0.150880
epoch:171, batch38, load frames use: 2.69879293442s
step 7903/400000: train loss: 0.149764
epoch:171, batch39, load frames use: 2.99151110649s
step 7904/400000: train loss: 0.170922
epoch:171, batch40, load frames use: 3.51041603088s
step 7905/400000: train loss: 0.153075
epoch:171, batch41, load frames use: 2.70812082291s
step 7906/400000: train loss: 0.154885
epoch:171, batch42, load frames use: 2.90740394592s
step 7907/400000: train loss: 0.154448
epoch:171, batch43, load frames use: 3.89989495277s
step 7908/400000: train loss: 0.183703
epoch:171, batch44, load frames use: 3.23939394951s
step 7909/400000: train loss: 0.174352
epoch:171, batch45, load frames use: 3.30665683746s
step 7910/400000: train loss: 0.162604
epoch:171, batch46, load frames use: 2.8430659771s
step 7911/400000: train loss: 0.172371
epoch:172, batch1, load frames use: 3.06023406982s
step 7912/400000: train loss: 0.139787
epoch:172, batch2, load frames use: 3.31765794754s
step 7913/400000: train loss: 0.143952
epoch:172, batch3, load frames use: 3.42131400108s
step 7914/400000: train loss: 0.141915
epoch:172, batch4, load frames use: 3.16061091423s
step 7915/400000: train loss: 0.168062
epoch:172, batch5, load frames use: 3.06266093254s
step 7916/400000: train loss: 0.153246
epoch:172, batch6, load frames use: 3.52852010727s
step 7917/400000: train loss: 0.161955
epoch:172, batch7, load frames use: 2.90817809105s
step 7918/400000: train loss: 0.164872
epoch:172, batch8, load frames use: 2.98858904839s
step 7919/400000: train loss: 0.147782
epoch:172, batch9, load frames use: 2.56948494911s
step 7920/400000: train loss: 0.157259
epoch:172, batch10, load frames use: 3.10342407227s
step 7921/400000: train loss: 0.153318
epoch:172, batch11, load frames use: 2.99514603615s
step 7922/400000: train loss: 0.161528
epoch:172, batch12, load frames use: 3.170399189s
step 7923/400000: train loss: 0.162502
epoch:172, batch13, load frames use: 3.19003486633s
step 7924/400000: train loss: 0.168115
epoch:172, batch14, load frames use: 2.73743605614s
step 7925/400000: train loss: 0.159651
epoch:172, batch15, load frames use: 3.0945649147s
step 7926/400000: train loss: 0.171190
epoch:172, batch16, load frames use: 3.0547978878s
step 7927/400000: train loss: 0.159182
epoch:172, batch17, load frames use: 2.74151301384s
step 7928/400000: train loss: 0.159506
epoch:172, batch18, load frames use: 2.50167894363s
step 7929/400000: train loss: 0.145595
epoch:172, batch19, load frames use: 3.44610190392s
step 7930/400000: train loss: 0.155122
epoch:172, batch20, load frames use: 3.04964709282s
step 7931/400000: train loss: 0.164172
epoch:172, batch21, load frames use: 2.57033205032s
step 7932/400000: train loss: 0.152345
epoch:172, batch22, load frames use: 2.85472893715s
step 7933/400000: train loss: 0.141166
epoch:172, batch23, load frames use: 3.2491300106s
step 7934/400000: train loss: 0.176224
epoch:172, batch24, load frames use: 2.78208518028s
step 7935/400000: train loss: 0.160285
epoch:172, batch25, load frames use: 2.59522104263s
step 7936/400000: train loss: 0.151411
epoch:172, batch26, load frames use: 2.96730399132s
step 7937/400000: train loss: 0.145679
epoch:172, batch27, load frames use: 3.22153496742s
step 7938/400000: train loss: 0.177153
epoch:172, batch28, load frames use: 3.01217389107s
step 7939/400000: train loss: 0.154105
epoch:172, batch29, load frames use: 3.35265016556s
step 7940/400000: train loss: 0.164146
epoch:172, batch30, load frames use: 3.18912100792s
step 7941/400000: train loss: 0.158128
epoch:172, batch31, load frames use: 3.38028097153s
step 7942/400000: train loss: 0.147478
epoch:172, batch32, load frames use: 2.59082102776s
step 7943/400000: train loss: 0.162059
epoch:172, batch33, load frames use: 3.68506193161s
step 7944/400000: train loss: 0.159966
epoch:172, batch34, load frames use: 2.55122208595s
step 7945/400000: train loss: 0.158494
epoch:172, batch35, load frames use: 3.23282599449s
step 7946/400000: train loss: 0.164041
epoch:172, batch36, load frames use: 3.43746685982s
step 7947/400000: train loss: 0.156844
epoch:172, batch37, load frames use: 3.12532401085s
step 7948/400000: train loss: 0.169392
epoch:172, batch38, load frames use: 3.26649689674s
step 7949/400000: train loss: 0.169913
epoch:172, batch39, load frames use: 3.21149802208s
step 7950/400000: train loss: 0.162750
epoch:172, batch40, load frames use: 3.12052702904s
step 7951/400000: train loss: 0.151866
epoch:172, batch41, load frames use: 3.01404094696s
step 7952/400000: train loss: 0.162272
epoch:172, batch42, load frames use: 3.14222693443s
step 7953/400000: train loss: 0.168986
epoch:172, batch43, load frames use: 3.07625293732s
step 7954/400000: train loss: 0.147573
epoch:172, batch44, load frames use: 2.82902097702s
step 7955/400000: train loss: 0.157746
epoch:172, batch45, load frames use: 3.42296886444s
step 7956/400000: train loss: 0.181688
epoch:172, batch46, load frames use: 2.95314598083s
step 7957/400000: train loss: 0.152574
epoch:173, batch1, load frames use: 3.35322904587s
step 7958/400000: train loss: 0.146775
epoch:173, batch2, load frames use: 2.7421901226s
step 7959/400000: train loss: 0.160051
epoch:173, batch3, load frames use: 2.98564195633s
step 7960/400000: train loss: 0.153826
epoch:173, batch4, load frames use: 3.26051306725s
step 7961/400000: train loss: 0.151322
epoch:173, batch5, load frames use: 2.71808886528s
step 7962/400000: train loss: 0.176545
epoch:173, batch6, load frames use: 3.11748409271s
step 7963/400000: train loss: 0.143743
epoch:173, batch7, load frames use: 3.10597014427s
step 7964/400000: train loss: 0.148730
epoch:173, batch8, load frames use: 2.85010313988s
step 7965/400000: train loss: 0.146070
epoch:173, batch9, load frames use: 2.96059012413s
step 7966/400000: train loss: 0.165410
epoch:173, batch10, load frames use: 3.76768898964s
step 7967/400000: train loss: 0.153536
epoch:173, batch11, load frames use: 3.21115589142s
step 7968/400000: train loss: 0.159769
epoch:173, batch12, load frames use: 2.99138879776s
step 7969/400000: train loss: 0.156037
epoch:173, batch13, load frames use: 2.79277801514s
step 7970/400000: train loss: 0.170768
epoch:173, batch14, load frames use: 3.04449009895s
step 7971/400000: train loss: 0.152435
epoch:173, batch15, load frames use: 2.57713794708s
step 7972/400000: train loss: 0.152255
epoch:173, batch16, load frames use: 3.10103106499s
step 7973/400000: train loss: 0.160468
epoch:173, batch17, load frames use: 2.89872002602s
step 7974/400000: train loss: 0.156818
epoch:173, batch18, load frames use: 3.54343700409s
step 7975/400000: train loss: 0.172562
epoch:173, batch19, load frames use: 2.81638002396s
step 7976/400000: train loss: 0.167037
epoch:173, batch20, load frames use: 2.53419303894s
step 7977/400000: train loss: 0.151565
epoch:173, batch21, load frames use: 3.827283144s
step 7978/400000: train loss: 0.177242
epoch:173, batch22, load frames use: 3.41558814049s
step 7979/400000: train loss: 0.152974
epoch:173, batch23, load frames use: 3.00527191162s
step 7980/400000: train loss: 0.153300
epoch:173, batch24, load frames use: 3.13985490799s
step 7981/400000: train loss: 0.152044
epoch:173, batch25, load frames use: 3.3174829483s
step 7982/400000: train loss: 0.171668
epoch:173, batch26, load frames use: 2.99881601334s
step 7983/400000: train loss: 0.162884
epoch:173, batch27, load frames use: 3.51567101479s
step 7984/400000: train loss: 0.162651
epoch:173, batch28, load frames use: 3.09460496902s
step 7985/400000: train loss: 0.166549
epoch:173, batch29, load frames use: 3.15470004082s
step 7986/400000: train loss: 0.161452
epoch:173, batch30, load frames use: 2.92262411118s
step 7987/400000: train loss: 0.157540
epoch:173, batch31, load frames use: 3.72073698044s
step 7988/400000: train loss: 0.171122
epoch:173, batch32, load frames use: 3.08597588539s
step 7989/400000: train loss: 0.148395
epoch:173, batch33, load frames use: 3.18000388145s
step 7990/400000: train loss: 0.163123
epoch:173, batch34, load frames use: 3.07707309723s
step 7991/400000: train loss: 0.160126
epoch:173, batch35, load frames use: 3.02333498001s
step 7992/400000: train loss: 0.150818
epoch:173, batch36, load frames use: 3.50964093208s
step 7993/400000: train loss: 0.157725
epoch:173, batch37, load frames use: 2.97579097748s
step 7994/400000: train loss: 0.161607
epoch:173, batch38, load frames use: 3.19150996208s
step 7995/400000: train loss: 0.175836
epoch:173, batch39, load frames use: 3.20397591591s
step 7996/400000: train loss: 0.164597
epoch:173, batch40, load frames use: 2.85077381134s
step 7997/400000: train loss: 0.157348
epoch:173, batch41, load frames use: 2.57758998871s
step 7998/400000: train loss: 0.151287
epoch:173, batch42, load frames use: 2.87598180771s
step 7999/400000: train loss: 0.140389
epoch:173, batch43, load frames use: 2.638463974s
step 8000/400000: train loss: 0.181853
step 8000/400000: validation loss: 0.181421
epoch:173, batch44, load frames use: 3.23673701286s
step 8001/400000: train loss: 0.171362
epoch:173, batch45, load frames use: 4.18655800819s
step 8002/400000: train loss: 0.172304
epoch:173, batch46, load frames use: 3.42977309227s
step 8003/400000: train loss: 0.163620
epoch:174, batch1, load frames use: 2.92375397682s
step 8004/400000: train loss: 0.161221
epoch:174, batch2, load frames use: 3.11090087891s
step 8005/400000: train loss: 0.135372
epoch:174, batch3, load frames use: 3.63229608536s
step 8006/400000: train loss: 0.165011
epoch:174, batch4, load frames use: 3.27612900734s
step 8007/400000: train loss: 0.146600
epoch:174, batch5, load frames use: 2.67630600929s
step 8008/400000: train loss: 0.157028
epoch:174, batch6, load frames use: 2.61705613136s
step 8009/400000: train loss: 0.159831
epoch:174, batch7, load frames use: 3.0911128521s
step 8010/400000: train loss: 0.163770
epoch:174, batch8, load frames use: 3.05123090744s
step 8011/400000: train loss: 0.155606
epoch:174, batch9, load frames use: 3.35618305206s
step 8012/400000: train loss: 0.153646
epoch:174, batch10, load frames use: 3.41451692581s
step 8013/400000: train loss: 0.153227
epoch:174, batch11, load frames use: 3.20118284225s
step 8014/400000: train loss: 0.153097
epoch:174, batch12, load frames use: 3.10883784294s
step 8015/400000: train loss: 0.153604
epoch:174, batch13, load frames use: 3.16363215446s
step 8016/400000: train loss: 0.146475
epoch:174, batch14, load frames use: 2.96062397957s
step 8017/400000: train loss: 0.140366
epoch:174, batch15, load frames use: 3.63584303856s
step 8018/400000: train loss: 0.151473
epoch:174, batch16, load frames use: 3.02290201187s
step 8019/400000: train loss: 0.142659
epoch:174, batch17, load frames use: 3.39612603188s
step 8020/400000: train loss: 0.160417
epoch:174, batch18, load frames use: 3.4458129406s
step 8021/400000: train loss: 0.161799
epoch:174, batch19, load frames use: 2.77232789993s
step 8022/400000: train loss: 0.167620
epoch:174, batch20, load frames use: 2.94871497154s
step 8023/400000: train loss: 0.162686
epoch:174, batch21, load frames use: 2.65853095055s
step 8024/400000: train loss: 0.159552
epoch:174, batch22, load frames use: 3.48074293137s
step 8025/400000: train loss: 0.156254
epoch:174, batch23, load frames use: 3.22727608681s
step 8026/400000: train loss: 0.147457
epoch:174, batch24, load frames use: 3.27694988251s
step 8027/400000: train loss: 0.169142
epoch:174, batch25, load frames use: 2.77880620956s
step 8028/400000: train loss: 0.157215
epoch:174, batch26, load frames use: 3.25037002563s
step 8029/400000: train loss: 0.150662
epoch:174, batch27, load frames use: 3.04902505875s
step 8030/400000: train loss: 0.161722
epoch:174, batch28, load frames use: 3.37602210045s
step 8031/400000: train loss: 0.153049
epoch:174, batch29, load frames use: 2.82633495331s
step 8032/400000: train loss: 0.155510
epoch:174, batch30, load frames use: 2.72355484962s
step 8033/400000: train loss: 0.159444
epoch:174, batch31, load frames use: 2.8169939518s
step 8034/400000: train loss: 0.153196
epoch:174, batch32, load frames use: 3.12489199638s
step 8035/400000: train loss: 0.147651
epoch:174, batch33, load frames use: 3.53534913063s
step 8036/400000: train loss: 0.158736
epoch:174, batch34, load frames use: 2.92030191422s
step 8037/400000: train loss: 0.165889
epoch:174, batch35, load frames use: 3.0811009407s
step 8038/400000: train loss: 0.165164
epoch:174, batch36, load frames use: 3.46058011055s
step 8039/400000: train loss: 0.151371
epoch:174, batch37, load frames use: 3.06416511536s
step 8040/400000: train loss: 0.156508
epoch:174, batch38, load frames use: 3.304915905s
step 8041/400000: train loss: 0.167628
epoch:174, batch39, load frames use: 3.60866093636s
step 8042/400000: train loss: 0.157642
epoch:174, batch40, load frames use: 2.92780399323s
step 8043/400000: train loss: 0.155066
epoch:174, batch41, load frames use: 2.77209091187s
step 8044/400000: train loss: 0.151412
epoch:174, batch42, load frames use: 3.4303548336s
step 8045/400000: train loss: 0.168343
epoch:174, batch43, load frames use: 2.84570384026s
step 8046/400000: train loss: 0.176604
epoch:174, batch44, load frames use: 2.48012113571s
step 8047/400000: train loss: 0.174903
epoch:174, batch45, load frames use: 2.29926395416s
step 8048/400000: train loss: 0.152815
epoch:174, batch46, load frames use: 2.96417498589s
step 8049/400000: train loss: 0.167391
epoch:175, batch1, load frames use: 3.29505395889s
step 8050/400000: train loss: 0.137409
epoch:175, batch2, load frames use: 3.07495379448s
step 8051/400000: train loss: 0.135761
epoch:175, batch3, load frames use: 3.46020507812s
step 8052/400000: train loss: 0.150144
epoch:175, batch4, load frames use: 3.20078396797s
step 8053/400000: train loss: 0.153167
epoch:175, batch5, load frames use: 2.61665201187s
step 8054/400000: train loss: 0.165791
epoch:175, batch6, load frames use: 3.34462404251s
step 8055/400000: train loss: 0.141245
epoch:175, batch7, load frames use: 3.50611400604s
step 8056/400000: train loss: 0.142253
epoch:175, batch8, load frames use: 3.18772697449s
step 8057/400000: train loss: 0.154545
epoch:175, batch9, load frames use: 3.16911888123s
step 8058/400000: train loss: 0.153664
epoch:175, batch10, load frames use: 2.9989490509s
step 8059/400000: train loss: 0.158484
epoch:175, batch11, load frames use: 3.23400592804s
step 8060/400000: train loss: 0.152509
epoch:175, batch12, load frames use: 3.08765411377s
step 8061/400000: train loss: 0.182071
epoch:175, batch13, load frames use: 3.65149903297s
step 8062/400000: train loss: 0.156468
epoch:175, batch14, load frames use: 2.71853017807s
step 8063/400000: train loss: 0.150207
epoch:175, batch15, load frames use: 3.06786608696s
step 8064/400000: train loss: 0.146206
epoch:175, batch16, load frames use: 2.80035209656s
step 8065/400000: train loss: 0.161326
epoch:175, batch17, load frames use: 3.13193702698s
step 8066/400000: train loss: 0.160458
epoch:175, batch18, load frames use: 3.18624997139s
step 8067/400000: train loss: 0.166465
epoch:175, batch19, load frames use: 3.21911406517s
step 8068/400000: train loss: 0.168730
epoch:175, batch20, load frames use: 2.80911803246s
step 8069/400000: train loss: 0.148898
epoch:175, batch21, load frames use: 2.96110510826s
step 8070/400000: train loss: 0.180836
epoch:175, batch22, load frames use: 3.33508586884s
step 8071/400000: train loss: 0.170274
epoch:175, batch23, load frames use: 3.37043690681s
step 8072/400000: train loss: 0.159241
epoch:175, batch24, load frames use: 4.5402610302s
step 8073/400000: train loss: 0.163799
epoch:175, batch25, load frames use: 5.88364195824s
step 8074/400000: train loss: 0.171759
epoch:175, batch26, load frames use: 5.75864100456s
step 8075/400000: train loss: 0.160520
epoch:175, batch27, load frames use: 6.16004109383s
step 8076/400000: train loss: 0.149586
epoch:175, batch28, load frames use: 5.63825011253s
step 8077/400000: train loss: 0.153939
epoch:175, batch29, load frames use: 5.22395706177s
step 8078/400000: train loss: 0.164789
epoch:175, batch30, load frames use: 6.34012293816s
step 8079/400000: train loss: 0.157804
epoch:175, batch31, load frames use: 5.35888910294s
step 8080/400000: train loss: 0.142700
epoch:175, batch32, load frames use: 6.10323691368s
step 8081/400000: train loss: 0.155017
epoch:175, batch33, load frames use: 5.89030003548s
step 8082/400000: train loss: 0.172935
epoch:175, batch34, load frames use: 6.11511182785s
step 8083/400000: train loss: 0.158143
epoch:175, batch35, load frames use: 5.93332290649s
step 8084/400000: train loss: 0.164695
epoch:175, batch36, load frames use: 5.46837496758s
step 8085/400000: train loss: 0.168797
epoch:175, batch37, load frames use: 4.21915507317s
step 8086/400000: train loss: 0.176837
epoch:175, batch38, load frames use: 5.00097084045s
step 8087/400000: train loss: 0.151771
epoch:175, batch39, load frames use: 5.46487188339s
step 8088/400000: train loss: 0.172353
epoch:175, batch40, load frames use: 4.67172122002s
step 8089/400000: train loss: 0.180157
epoch:175, batch41, load frames use: 5.46508693695s
step 8090/400000: train loss: 0.158726
epoch:175, batch42, load frames use: 5.26184010506s
step 8091/400000: train loss: 0.164071
epoch:175, batch43, load frames use: 4.76994919777s
step 8092/400000: train loss: 0.163375
epoch:175, batch44, load frames use: 4.99689698219s
step 8093/400000: train loss: 0.150962
epoch:175, batch45, load frames use: 4.99353909492s
step 8094/400000: train loss: 0.157302
epoch:175, batch46, load frames use: 4.89010906219s
step 8095/400000: train loss: 0.149427
epoch:176, batch1, load frames use: 5.5329990387s
step 8096/400000: train loss: 0.152810
epoch:176, batch2, load frames use: 4.31263399124s
step 8097/400000: train loss: 0.153485
epoch:176, batch3, load frames use: 5.2128841877s
step 8098/400000: train loss: 0.140657
epoch:176, batch4, load frames use: 4.3243188858s
step 8099/400000: train loss: 0.163439
epoch:176, batch5, load frames use: 4.69230818748s
step 8100/400000: train loss: 0.157866
step 8100/400000: validation loss: 0.140539
epoch:176, batch6, load frames use: 5.03050613403s
step 8101/400000: train loss: 0.141149
epoch:176, batch7, load frames use: 5.24955296516s
step 8102/400000: train loss: 0.151375
epoch:176, batch8, load frames use: 4.30741214752s
step 8103/400000: train loss: 0.157074
epoch:176, batch9, load frames use: 4.73495697975s
step 8104/400000: train loss: 0.156197
epoch:176, batch10, load frames use: 4.69921302795s
step 8105/400000: train loss: 0.151393
epoch:176, batch11, load frames use: 4.98939394951s
step 8106/400000: train loss: 0.152914
epoch:176, batch12, load frames use: 5.35059094429s
step 8107/400000: train loss: 0.164170
epoch:176, batch13, load frames use: 5.20661401749s
step 8108/400000: train loss: 0.187478
epoch:176, batch14, load frames use: 5.21103096008s
step 8109/400000: train loss: 0.150953
epoch:176, batch15, load frames use: 4.71035504341s
step 8110/400000: train loss: 0.149632
epoch:176, batch16, load frames use: 4.8046169281s
step 8111/400000: train loss: 0.155827
epoch:176, batch17, load frames use: 4.82025885582s
step 8112/400000: train loss: 0.142305
epoch:176, batch18, load frames use: 4.64275288582s
step 8113/400000: train loss: 0.138020
epoch:176, batch19, load frames use: 4.66997909546s
step 8114/400000: train loss: 0.154017
epoch:176, batch20, load frames use: 4.63118100166s
step 8115/400000: train loss: 0.146480
epoch:176, batch21, load frames use: 4.9441511631s
step 8116/400000: train loss: 0.141206
epoch:176, batch22, load frames use: 4.75056314468s
step 8117/400000: train loss: 0.151362
epoch:176, batch23, load frames use: 4.85640311241s
step 8118/400000: train loss: 0.165224
epoch:176, batch24, load frames use: 5.03577017784s
step 8119/400000: train loss: 0.157378
epoch:176, batch25, load frames use: 4.8740708828s
step 8120/400000: train loss: 0.152083
epoch:176, batch26, load frames use: 5.0776450634s
step 8121/400000: train loss: 0.175764
epoch:176, batch27, load frames use: 4.85237193108s
step 8122/400000: train loss: 0.158750
epoch:176, batch28, load frames use: 4.55504608154s
step 8123/400000: train loss: 0.151103
epoch:176, batch29, load frames use: 4.36465287209s
step 8124/400000: train loss: 0.165732
epoch:176, batch30, load frames use: 5.59990501404s
step 8125/400000: train loss: 0.165359
epoch:176, batch31, load frames use: 5.01284599304s
step 8126/400000: train loss: 0.169969
epoch:176, batch32, load frames use: 5.27473688126s
step 8127/400000: train loss: 0.180172
epoch:176, batch33, load frames use: 4.74310302734s
step 8128/400000: train loss: 0.163099
epoch:176, batch34, load frames use: 3.90545201302s
step 8129/400000: train loss: 0.157473
epoch:176, batch35, load frames use: 4.85770201683s
step 8130/400000: train loss: 0.143582
epoch:176, batch36, load frames use: 4.3422999382s
step 8131/400000: train loss: 0.170529
epoch:176, batch37, load frames use: 4.91444587708s
step 8132/400000: train loss: 0.169351
epoch:176, batch38, load frames use: 5.22953414917s
step 8133/400000: train loss: 0.166033
epoch:176, batch39, load frames use: 5.28504395485s
step 8134/400000: train loss: 0.173553
epoch:176, batch40, load frames use: 5.01479101181s
step 8135/400000: train loss: 0.165804
epoch:176, batch41, load frames use: 4.54631900787s
step 8136/400000: train loss: 0.175053
epoch:176, batch42, load frames use: 4.8995950222s
step 8137/400000: train loss: 0.179682
epoch:176, batch43, load frames use: 5.27061700821s
step 8138/400000: train loss: 0.161631
epoch:176, batch44, load frames use: 4.35817694664s
step 8139/400000: train loss: 0.166617
epoch:176, batch45, load frames use: 3.81838393211s
step 8140/400000: train loss: 0.155822
epoch:176, batch46, load frames use: 4.63080501556s
step 8141/400000: train loss: 0.162638
epoch:177, batch1, load frames use: 4.82135796547s
step 8142/400000: train loss: 0.155000
epoch:177, batch2, load frames use: 4.7776658535s
step 8143/400000: train loss: 0.150351
epoch:177, batch3, load frames use: 4.59016704559s
step 8144/400000: train loss: 0.159048
epoch:177, batch4, load frames use: 4.83697295189s
step 8145/400000: train loss: 0.153626
epoch:177, batch5, load frames use: 4.84585404396s
step 8146/400000: train loss: 0.152468
epoch:177, batch6, load frames use: 5.12157988548s
step 8147/400000: train loss: 0.150314
epoch:177, batch7, load frames use: 5.02781105042s
step 8148/400000: train loss: 0.161165
epoch:177, batch8, load frames use: 4.92905402184s
step 8149/400000: train loss: 0.153402
epoch:177, batch9, load frames use: 4.51911497116s
step 8150/400000: train loss: 0.158047
epoch:177, batch10, load frames use: 4.97324180603s
step 8151/400000: train loss: 0.160932
epoch:177, batch11, load frames use: 5.27438402176s
step 8152/400000: train loss: 0.146130
epoch:177, batch12, load frames use: 5.06487703323s
step 8153/400000: train loss: 0.178969
epoch:177, batch13, load frames use: 5.16139602661s
step 8154/400000: train loss: 0.151752
epoch:177, batch14, load frames use: 4.90502810478s
step 8155/400000: train loss: 0.144632
epoch:177, batch15, load frames use: 4.90713810921s
step 8156/400000: train loss: 0.162717
epoch:177, batch16, load frames use: 4.81771492958s
step 8157/400000: train loss: 0.156072
epoch:177, batch17, load frames use: 4.96092200279s
step 8158/400000: train loss: 0.158219
epoch:177, batch18, load frames use: 4.47748208046s
step 8159/400000: train loss: 0.143414
epoch:177, batch19, load frames use: 4.58731102943s
step 8160/400000: train loss: 0.146559
epoch:177, batch20, load frames use: 5.28164696693s
step 8161/400000: train loss: 0.172372
epoch:177, batch21, load frames use: 5.07183098793s
step 8162/400000: train loss: 0.168877
epoch:177, batch22, load frames use: 4.88018488884s
step 8163/400000: train loss: 0.164423
epoch:177, batch23, load frames use: 4.70001077652s
step 8164/400000: train loss: 0.163660
epoch:177, batch24, load frames use: 5.03758382797s
step 8165/400000: train loss: 0.155570
epoch:177, batch25, load frames use: 4.88284397125s
step 8166/400000: train loss: 0.175986
epoch:177, batch26, load frames use: 4.91423201561s
step 8167/400000: train loss: 0.145332
epoch:177, batch27, load frames use: 4.78152799606s
step 8168/400000: train loss: 0.171762
epoch:177, batch28, load frames use: 5.19821596146s
step 8169/400000: train loss: 0.151584
epoch:177, batch29, load frames use: 5.35184884071s
step 8170/400000: train loss: 0.158081
epoch:177, batch30, load frames use: 4.94815683365s
step 8171/400000: train loss: 0.171191
epoch:177, batch31, load frames use: 4.72205209732s
step 8172/400000: train loss: 0.167779
epoch:177, batch32, load frames use: 4.83941888809s
step 8173/400000: train loss: 0.142304
epoch:177, batch33, load frames use: 4.93766689301s
step 8174/400000: train loss: 0.150094
epoch:177, batch34, load frames use: 4.72788500786s
step 8175/400000: train loss: 0.150991
epoch:177, batch35, load frames use: 5.34600520134s
step 8176/400000: train loss: 0.159138
epoch:177, batch36, load frames use: 4.7256629467s
step 8177/400000: train loss: 0.148921
epoch:177, batch37, load frames use: 5.7149040699s
step 8178/400000: train loss: 0.162837
epoch:177, batch38, load frames use: 5.14512181282s
step 8179/400000: train loss: 0.142023
epoch:177, batch39, load frames use: 4.77136802673s
step 8180/400000: train loss: 0.148526
epoch:177, batch40, load frames use: 4.9987051487s
step 8181/400000: train loss: 0.156146
epoch:177, batch41, load frames use: 4.97489380836s
step 8182/400000: train loss: 0.159069
epoch:177, batch42, load frames use: 5.02199912071s
step 8183/400000: train loss: 0.161474
epoch:177, batch43, load frames use: 4.25548815727s
step 8184/400000: train loss: 0.144980
epoch:177, batch44, load frames use: 4.78537106514s
step 8185/400000: train loss: 0.156237
epoch:177, batch45, load frames use: 5.67007184029s
step 8186/400000: train loss: 0.157155
epoch:177, batch46, load frames use: 4.27254199982s
step 8187/400000: train loss: 0.154661
epoch:178, batch1, load frames use: 5.35530591011s
step 8188/400000: train loss: 0.152569
epoch:178, batch2, load frames use: 5.5760910511s
step 8189/400000: train loss: 0.147625
epoch:178, batch3, load frames use: 4.81778812408s
step 8190/400000: train loss: 0.148158
epoch:178, batch4, load frames use: 4.91181206703s
step 8191/400000: train loss: 0.141772
epoch:178, batch5, load frames use: 4.57821989059s
step 8192/400000: train loss: 0.158896
epoch:178, batch6, load frames use: 4.58522510529s
step 8193/400000: train loss: 0.155120
epoch:178, batch7, load frames use: 4.87099504471s
step 8194/400000: train loss: 0.151746
epoch:178, batch8, load frames use: 4.94718909264s
step 8195/400000: train loss: 0.137873
epoch:178, batch9, load frames use: 4.31201601028s
step 8196/400000: train loss: 0.148653
epoch:178, batch10, load frames use: 4.65064501762s
step 8197/400000: train loss: 0.159689
epoch:178, batch11, load frames use: 5.92249107361s
step 8198/400000: train loss: 0.162580
epoch:178, batch12, load frames use: 5.03383898735s
step 8199/400000: train loss: 0.149254
epoch:178, batch13, load frames use: 5.49560713768s
step 8200/400000: train loss: 0.155274
step 8200/400000: validation loss: 0.142824
epoch:178, batch14, load frames use: 5.07750511169s
step 8201/400000: train loss: 0.134093
epoch:178, batch15, load frames use: 4.77640986443s
step 8202/400000: train loss: 0.154827
epoch:178, batch16, load frames use: 5.43272614479s
step 8203/400000: train loss: 0.170837
epoch:178, batch17, load frames use: 5.58246016502s
step 8204/400000: train loss: 0.139074
epoch:178, batch18, load frames use: 4.98945617676s
step 8205/400000: train loss: 0.153608
epoch:178, batch19, load frames use: 5.1033270359s
step 8206/400000: train loss: 0.167473
epoch:178, batch20, load frames use: 5.10059404373s
step 8207/400000: train loss: 0.170998
epoch:178, batch21, load frames use: 4.12159490585s
step 8208/400000: train loss: 0.166066
epoch:178, batch22, load frames use: 5.30208396912s
step 8209/400000: train loss: 0.164549
epoch:178, batch23, load frames use: 5.44860100746s
step 8210/400000: train loss: 0.149054
epoch:178, batch24, load frames use: 4.89736604691s
step 8211/400000: train loss: 0.147538
epoch:178, batch25, load frames use: 4.32922887802s
step 8212/400000: train loss: 0.158747
epoch:178, batch26, load frames use: 4.55707097054s
step 8213/400000: train loss: 0.182736
epoch:178, batch27, load frames use: 5.1122508049s
step 8214/400000: train loss: 0.140527
epoch:178, batch28, load frames use: 4.58173203468s
step 8215/400000: train loss: 0.161945
epoch:178, batch29, load frames use: 5.26843500137s
step 8216/400000: train loss: 0.168142
epoch:178, batch30, load frames use: 4.58125495911s
step 8217/400000: train loss: 0.159116
epoch:178, batch31, load frames use: 4.82510113716s
step 8218/400000: train loss: 0.168552
epoch:178, batch32, load frames use: 5.20351791382s
step 8219/400000: train loss: 0.160092
epoch:178, batch33, load frames use: 4.81428408623s
step 8220/400000: train loss: 0.163168
epoch:178, batch34, load frames use: 4.96049785614s
step 8221/400000: train loss: 0.182994
epoch:178, batch35, load frames use: 4.54096198082s
step 8222/400000: train loss: 0.160836
epoch:178, batch36, load frames use: 5.09887599945s
step 8223/400000: train loss: 0.159773
epoch:178, batch37, load frames use: 5.13500404358s
step 8224/400000: train loss: 0.174053
epoch:178, batch38, load frames use: 5.18058300018s
step 8225/400000: train loss: 0.155346
epoch:178, batch39, load frames use: 4.79988384247s
step 8226/400000: train loss: 0.161190
epoch:178, batch40, load frames use: 4.90786194801s
step 8227/400000: train loss: 0.165343
epoch:178, batch41, load frames use: 4.75533509254s
step 8228/400000: train loss: 0.150633
epoch:178, batch42, load frames use: 4.88021397591s
step 8229/400000: train loss: 0.159403
epoch:178, batch43, load frames use: 4.63450908661s
step 8230/400000: train loss: 0.167205
epoch:178, batch44, load frames use: 5.01129293442s
step 8231/400000: train loss: 0.160171
epoch:178, batch45, load frames use: 5.31470990181s
step 8232/400000: train loss: 0.172704
epoch:178, batch46, load frames use: 5.99932098389s
step 8233/400000: train loss: 0.161008
epoch:179, batch1, load frames use: 4.74770116806s
step 8234/400000: train loss: 0.142976
epoch:179, batch2, load frames use: 4.34005498886s
step 8235/400000: train loss: 0.143617
epoch:179, batch3, load frames use: 5.11067509651s
step 8236/400000: train loss: 0.136494
epoch:179, batch4, load frames use: 4.98792791367s
step 8237/400000: train loss: 0.170383
epoch:179, batch5, load frames use: 5.63441205025s
step 8238/400000: train loss: 0.162137
epoch:179, batch6, load frames use: 5.77979421616s
step 8239/400000: train loss: 0.145409
epoch:179, batch7, load frames use: 5.23602294922s
step 8240/400000: train loss: 0.176201
epoch:179, batch8, load frames use: 5.15691113472s
step 8241/400000: train loss: 0.148932
epoch:179, batch9, load frames use: 4.54181313515s
step 8242/400000: train loss: 0.157393
epoch:179, batch10, load frames use: 4.37919998169s
step 8243/400000: train loss: 0.147940
epoch:179, batch11, load frames use: 5.24690318108s
step 8244/400000: train loss: 0.150576
epoch:179, batch12, load frames use: 4.49697899818s
step 8245/400000: train loss: 0.151345
epoch:179, batch13, load frames use: 5.09149599075s
step 8246/400000: train loss: 0.184292
epoch:179, batch14, load frames use: 5.35800886154s
step 8247/400000: train loss: 0.152263
epoch:179, batch15, load frames use: 4.93717002869s
step 8248/400000: train loss: 0.159685
epoch:179, batch16, load frames use: 4.99014210701s
step 8249/400000: train loss: 0.151148
epoch:179, batch17, load frames use: 4.75205802917s
step 8250/400000: train loss: 0.153352
epoch:179, batch18, load frames use: 5.1305680275s
step 8251/400000: train loss: 0.140810
epoch:179, batch19, load frames use: 4.88330698013s
step 8252/400000: train loss: 0.153788
epoch:179, batch20, load frames use: 5.07719707489s
step 8253/400000: train loss: 0.163049
epoch:179, batch21, load frames use: 4.62537312508s
step 8254/400000: train loss: 0.155994
epoch:179, batch22, load frames use: 5.14397597313s
step 8255/400000: train loss: 0.158372
epoch:179, batch23, load frames use: 5.04007411003s
step 8256/400000: train loss: 0.155570
epoch:179, batch24, load frames use: 4.93655991554s
step 8257/400000: train loss: 0.160449
epoch:179, batch25, load frames use: 4.63853883743s
step 8258/400000: train loss: 0.146214
epoch:179, batch26, load frames use: 5.72535395622s
step 8259/400000: train loss: 0.152993
epoch:179, batch27, load frames use: 5.01752686501s
step 8260/400000: train loss: 0.167442
epoch:179, batch28, load frames use: 5.27725100517s
step 8261/400000: train loss: 0.154670
epoch:179, batch29, load frames use: 4.41814517975s
step 8262/400000: train loss: 0.154521
epoch:179, batch30, load frames use: 5.04202914238s
step 8263/400000: train loss: 0.156545
epoch:179, batch31, load frames use: 4.59406304359s
step 8264/400000: train loss: 0.170932
epoch:179, batch32, load frames use: 4.92052102089s
step 8265/400000: train loss: 0.169001
epoch:179, batch33, load frames use: 4.59925103188s
step 8266/400000: train loss: 0.158492
epoch:179, batch34, load frames use: 4.70000886917s
step 8267/400000: train loss: 0.159279
epoch:179, batch35, load frames use: 4.91144704819s
step 8268/400000: train loss: 0.166182
epoch:179, batch36, load frames use: 5.1496090889s
step 8269/400000: train loss: 0.173619
epoch:179, batch37, load frames use: 5.02700781822s
step 8270/400000: train loss: 0.145644
epoch:179, batch38, load frames use: 4.89342713356s
step 8271/400000: train loss: 0.162577
epoch:179, batch39, load frames use: 5.16105294228s
step 8272/400000: train loss: 0.177862
epoch:179, batch40, load frames use: 5.30880999565s
step 8273/400000: train loss: 0.170109
epoch:179, batch41, load frames use: 4.95739102364s
step 8274/400000: train loss: 0.173644
epoch:179, batch42, load frames use: 4.60735201836s
step 8275/400000: train loss: 0.155533
epoch:179, batch43, load frames use: 4.12881183624s
step 8276/400000: train loss: 0.140856
epoch:179, batch44, load frames use: 4.98413991928s
step 8277/400000: train loss: 0.143239
epoch:179, batch45, load frames use: 4.83986592293s
step 8278/400000: train loss: 0.163341
epoch:179, batch46, load frames use: 5.22934484482s
step 8279/400000: train loss: 0.164792
epoch:180, batch1, load frames use: 4.82303905487s
step 8280/400000: train loss: 0.133461
epoch:180, batch2, load frames use: 4.78450083733s
step 8281/400000: train loss: 0.151646
epoch:180, batch3, load frames use: 5.01901698112s
step 8282/400000: train loss: 0.141494
epoch:180, batch4, load frames use: 4.75334191322s
step 8283/400000: train loss: 0.138619
epoch:180, batch5, load frames use: 4.41261410713s
step 8284/400000: train loss: 0.151479
epoch:180, batch6, load frames use: 4.90742492676s
step 8285/400000: train loss: 0.140598
epoch:180, batch7, load frames use: 4.33709692955s
step 8286/400000: train loss: 0.159141
epoch:180, batch8, load frames use: 5.14929103851s
step 8287/400000: train loss: 0.160797
epoch:180, batch9, load frames use: 5.01302695274s
step 8288/400000: train loss: 0.150714
epoch:180, batch10, load frames use: 5.4051887989s
step 8289/400000: train loss: 0.149306
epoch:180, batch11, load frames use: 4.69633507729s
step 8290/400000: train loss: 0.138022
epoch:180, batch12, load frames use: 4.90249800682s
step 8291/400000: train loss: 0.149793
epoch:180, batch13, load frames use: 4.72880601883s
step 8292/400000: train loss: 0.163959
epoch:180, batch14, load frames use: 4.30969500542s
step 8293/400000: train loss: 0.154471
epoch:180, batch15, load frames use: 4.50172305107s
step 8294/400000: train loss: 0.152958
epoch:180, batch16, load frames use: 4.87178897858s
step 8295/400000: train loss: 0.150147
epoch:180, batch17, load frames use: 5.05211997032s
step 8296/400000: train loss: 0.146394
epoch:180, batch18, load frames use: 4.83585095406s
step 8297/400000: train loss: 0.159295
epoch:180, batch19, load frames use: 4.60215187073s
step 8298/400000: train loss: 0.160265
epoch:180, batch20, load frames use: 4.57077693939s
step 8299/400000: train loss: 0.150091
epoch:180, batch21, load frames use: 4.72629284859s
step 8300/400000: train loss: 0.164484
step 8300/400000: validation loss: 0.151004
epoch:180, batch22, load frames use: 5.24453997612s
step 8301/400000: train loss: 0.148464
epoch:180, batch23, load frames use: 4.74168014526s
step 8302/400000: train loss: 0.170595
epoch:180, batch24, load frames use: 5.25670385361s
step 8303/400000: train loss: 0.142659
epoch:180, batch25, load frames use: 4.85834002495s
step 8304/400000: train loss: 0.143236
epoch:180, batch26, load frames use: 4.6383690834s
step 8305/400000: train loss: 0.157922
epoch:180, batch27, load frames use: 4.00157403946s
step 8306/400000: train loss: 0.158124
epoch:180, batch28, load frames use: 4.96048617363s
step 8307/400000: train loss: 0.156841
epoch:180, batch29, load frames use: 4.50325512886s
step 8308/400000: train loss: 0.167535
epoch:180, batch30, load frames use: 4.88294196129s
step 8309/400000: train loss: 0.164338
epoch:180, batch31, load frames use: 5.18388295174s
step 8310/400000: train loss: 0.155321
epoch:180, batch32, load frames use: 4.03264307976s
step 8311/400000: train loss: 0.150980
epoch:180, batch33, load frames use: 4.74994516373s
step 8312/400000: train loss: 0.162905
epoch:180, batch34, load frames use: 4.74350905418s
step 8313/400000: train loss: 0.164799
epoch:180, batch35, load frames use: 4.47548484802s
step 8314/400000: train loss: 0.196702
epoch:180, batch36, load frames use: 4.80658602715s
step 8315/400000: train loss: 0.141909
epoch:180, batch37, load frames use: 5.00194311142s
step 8316/400000: train loss: 0.157778
epoch:180, batch38, load frames use: 4.42675304413s
step 8317/400000: train loss: 0.156310
epoch:180, batch39, load frames use: 4.72789001465s
step 8318/400000: train loss: 0.151303
epoch:180, batch40, load frames use: 5.02908492088s
step 8319/400000: train loss: 0.144471
epoch:180, batch41, load frames use: 5.07075500488s
step 8320/400000: train loss: 0.161819
epoch:180, batch42, load frames use: 4.85570883751s
step 8321/400000: train loss: 0.156870
epoch:180, batch43, load frames use: 4.99729585648s
step 8322/400000: train loss: 0.155778
epoch:180, batch44, load frames use: 4.59325909615s
step 8323/400000: train loss: 0.144040
epoch:180, batch45, load frames use: 5.05451893806s
step 8324/400000: train loss: 0.165180
epoch:180, batch46, load frames use: 4.61768198013s
step 8325/400000: train loss: 0.155903
epoch:181, batch1, load frames use: 5.04516196251s
step 8326/400000: train loss: 0.168521
epoch:181, batch2, load frames use: 4.77210998535s
step 8327/400000: train loss: 0.158102
epoch:181, batch3, load frames use: 5.40411806107s
step 8328/400000: train loss: 0.158010
epoch:181, batch4, load frames use: 4.50002098083s
step 8329/400000: train loss: 0.150463
epoch:181, batch5, load frames use: 4.66671609879s
step 8330/400000: train loss: 0.159204
epoch:181, batch6, load frames use: 4.980260849s
step 8331/400000: train loss: 0.142367
epoch:181, batch7, load frames use: 5.1063439846s
step 8332/400000: train loss: 0.145067
epoch:181, batch8, load frames use: 4.35561990738s
step 8333/400000: train loss: 0.145381
epoch:181, batch9, load frames use: 4.88758015633s
step 8334/400000: train loss: 0.154094
epoch:181, batch10, load frames use: 4.5724902153s
step 8335/400000: train loss: 0.170839
epoch:181, batch11, load frames use: 4.21647787094s
step 8336/400000: train loss: 0.135878
epoch:181, batch12, load frames use: 5.04328203201s
step 8337/400000: train loss: 0.140359
epoch:181, batch13, load frames use: 5.6798210144s
step 8338/400000: train loss: 0.148790
epoch:181, batch14, load frames use: 5.19442200661s
step 8339/400000: train loss: 0.149146
epoch:181, batch15, load frames use: 4.90891218185s
step 8340/400000: train loss: 0.158188
epoch:181, batch16, load frames use: 5.83578109741s
step 8341/400000: train loss: 0.168138
epoch:181, batch17, load frames use: 4.85535001755s
step 8342/400000: train loss: 0.154988
epoch:181, batch18, load frames use: 5.11982297897s
step 8343/400000: train loss: 0.156960
epoch:181, batch19, load frames use: 5.05026698112s
step 8344/400000: train loss: 0.161195
epoch:181, batch20, load frames use: 4.93583798409s
step 8345/400000: train loss: 0.158716
epoch:181, batch21, load frames use: 4.76022601128s
step 8346/400000: train loss: 0.147968
epoch:181, batch22, load frames use: 4.55133295059s
step 8347/400000: train loss: 0.147613
epoch:181, batch23, load frames use: 4.97815418243s
step 8348/400000: train loss: 0.143107
epoch:181, batch24, load frames use: 4.34088897705s
step 8349/400000: train loss: 0.151575
epoch:181, batch25, load frames use: 4.07600784302s
step 8350/400000: train loss: 0.159630
epoch:181, batch26, load frames use: 3.77958893776s
step 8351/400000: train loss: 0.165518
epoch:181, batch27, load frames use: 5.57660007477s
step 8352/400000: train loss: 0.157292
epoch:181, batch28, load frames use: 4.54271101952s
step 8353/400000: train loss: 0.169310
epoch:181, batch29, load frames use: 5.44821500778s
step 8354/400000: train loss: 0.152651
epoch:181, batch30, load frames use: 4.50071001053s
step 8355/400000: train loss: 0.155881
epoch:181, batch31, load frames use: 5.00071692467s
step 8356/400000: train loss: 0.136882
epoch:181, batch32, load frames use: 4.1755399704s
step 8357/400000: train loss: 0.167544
epoch:181, batch33, load frames use: 4.65373301506s
step 8358/400000: train loss: 0.159496
epoch:181, batch34, load frames use: 4.4915728569s
step 8359/400000: train loss: 0.145166
epoch:181, batch35, load frames use: 4.85132312775s
step 8360/400000: train loss: 0.143878
epoch:181, batch36, load frames use: 4.58293914795s
step 8361/400000: train loss: 0.164870
epoch:181, batch37, load frames use: 4.41181087494s
step 8362/400000: train loss: 0.176144
epoch:181, batch38, load frames use: 4.22349190712s
step 8363/400000: train loss: 0.154048
epoch:181, batch39, load frames use: 4.80711102486s
step 8364/400000: train loss: 0.160050
epoch:181, batch40, load frames use: 4.41886496544s
step 8365/400000: train loss: 0.159320
epoch:181, batch41, load frames use: 4.67456793785s
step 8366/400000: train loss: 0.142726
epoch:181, batch42, load frames use: 5.09301781654s
step 8367/400000: train loss: 0.156440
epoch:181, batch43, load frames use: 5.40054392815s
step 8368/400000: train loss: 0.154534
epoch:181, batch44, load frames use: 4.70604610443s
step 8369/400000: train loss: 0.167962
epoch:181, batch45, load frames use: 4.93177700043s
step 8370/400000: train loss: 0.173422
epoch:181, batch46, load frames use: 4.03748989105s
step 8371/400000: train loss: 0.161271
epoch:182, batch1, load frames use: 4.74308395386s
step 8372/400000: train loss: 0.143551
epoch:182, batch2, load frames use: 4.86713385582s
step 8373/400000: train loss: 0.156541
epoch:182, batch3, load frames use: 5.17607688904s
step 8374/400000: train loss: 0.152952
epoch:182, batch4, load frames use: 4.22443914413s
step 8375/400000: train loss: 0.162375
epoch:182, batch5, load frames use: 4.84991884232s
step 8376/400000: train loss: 0.158066
epoch:182, batch6, load frames use: 4.72536993027s
step 8377/400000: train loss: 0.158594
epoch:182, batch7, load frames use: 5.01647806168s
step 8378/400000: train loss: 0.171201
epoch:182, batch8, load frames use: 4.14199399948s
step 8379/400000: train loss: 0.158591
epoch:182, batch9, load frames use: 4.47114491463s
step 8380/400000: train loss: 0.161741
epoch:182, batch10, load frames use: 4.69817709923s
step 8381/400000: train loss: 0.154388
epoch:182, batch11, load frames use: 4.41187691689s
step 8382/400000: train loss: 0.151709
epoch:182, batch12, load frames use: 4.88543891907s
step 8383/400000: train loss: 0.168052
epoch:182, batch13, load frames use: 5.22624206543s
step 8384/400000: train loss: 0.174299
epoch:182, batch14, load frames use: 4.75103878975s
step 8385/400000: train loss: 0.160524
epoch:182, batch15, load frames use: 4.55383992195s
step 8386/400000: train loss: 0.152841
epoch:182, batch16, load frames use: 4.72014188766s
step 8387/400000: train loss: 0.145253
epoch:182, batch17, load frames use: 4.77954292297s
step 8388/400000: train loss: 0.179382
epoch:182, batch18, load frames use: 5.16423797607s
step 8389/400000: train loss: 0.147368
epoch:182, batch19, load frames use: 4.77721190453s
step 8390/400000: train loss: 0.180154
epoch:182, batch20, load frames use: 5.20729494095s
step 8391/400000: train loss: 0.147114
epoch:182, batch21, load frames use: 5.01085686684s
step 8392/400000: train loss: 0.151942
epoch:182, batch22, load frames use: 4.59869599342s
step 8393/400000: train loss: 0.153513
epoch:182, batch23, load frames use: 5.11330795288s
step 8394/400000: train loss: 0.157105
epoch:182, batch24, load frames use: 4.88730716705s
step 8395/400000: train loss: 0.162902
epoch:182, batch25, load frames use: 5.3270509243s
step 8396/400000: train loss: 0.174766
epoch:182, batch26, load frames use: 5.22751092911s
step 8397/400000: train loss: 0.158447
epoch:182, batch27, load frames use: 5.00452399254s
step 8398/400000: train loss: 0.147446
epoch:182, batch28, load frames use: 4.44279789925s
step 8399/400000: train loss: 0.174402
epoch:182, batch29, load frames use: 4.69268107414s
step 8400/400000: train loss: 0.155799
step 8400/400000: validation loss: 0.148557
epoch:182, batch30, load frames use: 4.78746581078s
step 8401/400000: train loss: 0.148929
epoch:182, batch31, load frames use: 4.10394406319s
step 8402/400000: train loss: 0.135467
epoch:182, batch32, load frames use: 4.30244612694s
step 8403/400000: train loss: 0.138287
epoch:182, batch33, load frames use: 5.27808499336s
step 8404/400000: train loss: 0.189225
epoch:182, batch34, load frames use: 4.58115100861s
step 8405/400000: train loss: 0.168488
epoch:182, batch35, load frames use: 4.88883686066s
step 8406/400000: train loss: 0.136287
epoch:182, batch36, load frames use: 4.05265688896s
step 8407/400000: train loss: 0.175069
epoch:182, batch37, load frames use: 4.98520612717s
step 8408/400000: train loss: 0.147741
epoch:182, batch38, load frames use: 4.55531597137s
step 8409/400000: train loss: 0.166621
epoch:182, batch39, load frames use: 5.31025004387s
step 8410/400000: train loss: 0.144109
epoch:182, batch40, load frames use: 4.72564005852s
step 8411/400000: train loss: 0.154117
epoch:182, batch41, load frames use: 4.49521017075s
step 8412/400000: train loss: 0.140642
epoch:182, batch42, load frames use: 4.92248010635s
step 8413/400000: train loss: 0.140283
epoch:182, batch43, load frames use: 5.17337584496s
step 8414/400000: train loss: 0.157744
epoch:182, batch44, load frames use: 4.76669812202s
step 8415/400000: train loss: 0.171096
epoch:182, batch45, load frames use: 4.2331609726s
step 8416/400000: train loss: 0.138649
epoch:182, batch46, load frames use: 4.78878498077s
step 8417/400000: train loss: 0.166899
epoch:183, batch1, load frames use: 4.99391388893s
step 8418/400000: train loss: 0.131936
epoch:183, batch2, load frames use: 4.223498106s
step 8419/400000: train loss: 0.145349
epoch:183, batch3, load frames use: 4.76418209076s
step 8420/400000: train loss: 0.138450
epoch:183, batch4, load frames use: 5.20008397102s
step 8421/400000: train loss: 0.149374
epoch:183, batch5, load frames use: 4.80841112137s
step 8422/400000: train loss: 0.166723
epoch:183, batch6, load frames use: 5.09576702118s
step 8423/400000: train loss: 0.155259
epoch:183, batch7, load frames use: 4.75250697136s
step 8424/400000: train loss: 0.151301
epoch:183, batch8, load frames use: 4.83172893524s
step 8425/400000: train loss: 0.142825
epoch:183, batch9, load frames use: 5.18717694283s
step 8426/400000: train loss: 0.155655
epoch:183, batch10, load frames use: 4.60252785683s
step 8427/400000: train loss: 0.158820
epoch:183, batch11, load frames use: 4.82253789902s
step 8428/400000: train loss: 0.141972
epoch:183, batch12, load frames use: 4.74586796761s
step 8429/400000: train loss: 0.150391
epoch:183, batch13, load frames use: 4.67665696144s
step 8430/400000: train loss: 0.161190
epoch:183, batch14, load frames use: 4.98201417923s
step 8431/400000: train loss: 0.147049
epoch:183, batch15, load frames use: 4.98620414734s
step 8432/400000: train loss: 0.150183
epoch:183, batch16, load frames use: 4.69748902321s
step 8433/400000: train loss: 0.149963
epoch:183, batch17, load frames use: 4.61850786209s
step 8434/400000: train loss: 0.155756
epoch:183, batch18, load frames use: 4.59052991867s
step 8435/400000: train loss: 0.149054
epoch:183, batch19, load frames use: 4.39549303055s
step 8436/400000: train loss: 0.175241
epoch:183, batch20, load frames use: 4.67979216576s
step 8437/400000: train loss: 0.147861
epoch:183, batch21, load frames use: 4.56948900223s
step 8438/400000: train loss: 0.167045
epoch:183, batch22, load frames use: 4.58503293991s
step 8439/400000: train loss: 0.159297
epoch:183, batch23, load frames use: 3.69983100891s
step 8440/400000: train loss: 0.147403
epoch:183, batch24, load frames use: 4.27343297005s
step 8441/400000: train loss: 0.159767
epoch:183, batch25, load frames use: 5.37951493263s
step 8442/400000: train loss: 0.167146
epoch:183, batch26, load frames use: 4.61511707306s
step 8443/400000: train loss: 0.178084
epoch:183, batch27, load frames use: 3.52110695839s
step 8444/400000: train loss: 0.163767
epoch:183, batch28, load frames use: 3.793405056s
step 8445/400000: train loss: 0.148481
epoch:183, batch29, load frames use: 4.89560294151s
step 8446/400000: train loss: 0.151281
epoch:183, batch30, load frames use: 4.99520492554s
step 8447/400000: train loss: 0.170779
epoch:183, batch31, load frames use: 4.49344396591s
step 8448/400000: train loss: 0.151165
epoch:183, batch32, load frames use: 5.10373497009s
step 8449/400000: train loss: 0.146637
epoch:183, batch33, load frames use: 4.46395492554s
step 8450/400000: train loss: 0.157999
epoch:183, batch34, load frames use: 4.62924289703s
step 8451/400000: train loss: 0.148885
epoch:183, batch35, load frames use: 4.56113100052s
step 8452/400000: train loss: 0.163528
epoch:183, batch36, load frames use: 4.4603471756s
step 8453/400000: train loss: 0.156564
epoch:183, batch37, load frames use: 3.43129014969s
step 8454/400000: train loss: 0.193012
epoch:183, batch38, load frames use: 3.86371803284s
step 8455/400000: train loss: 0.170930
epoch:183, batch39, load frames use: 4.33314085007s
step 8456/400000: train loss: 0.129030
epoch:183, batch40, load frames use: 3.90959882736s
step 8457/400000: train loss: 0.159204
epoch:183, batch41, load frames use: 4.42450809479s
step 8458/400000: train loss: 0.145487
epoch:183, batch42, load frames use: 4.10516095161s
step 8459/400000: train loss: 0.164896
epoch:183, batch43, load frames use: 5.42551803589s
step 8460/400000: train loss: 0.143886
epoch:183, batch44, load frames use: 5.05190205574s
step 8461/400000: train loss: 0.155716
epoch:183, batch45, load frames use: 4.98074388504s
step 8462/400000: train loss: 0.146544
epoch:183, batch46, load frames use: 4.96426510811s
step 8463/400000: train loss: 0.155132
epoch:184, batch1, load frames use: 4.33181810379s
step 8464/400000: train loss: 0.138634
epoch:184, batch2, load frames use: 5.31259512901s
step 8465/400000: train loss: 0.147556
epoch:184, batch3, load frames use: 4.73456096649s
step 8466/400000: train loss: 0.157059
epoch:184, batch4, load frames use: 4.15919709206s
step 8467/400000: train loss: 0.159373
epoch:184, batch5, load frames use: 4.46689486504s
step 8468/400000: train loss: 0.159763
epoch:184, batch6, load frames use: 4.04956197739s
step 8469/400000: train loss: 0.146532
epoch:184, batch7, load frames use: 4.39004802704s
step 8470/400000: train loss: 0.154925
epoch:184, batch8, load frames use: 4.90246200562s
step 8471/400000: train loss: 0.154866
epoch:184, batch9, load frames use: 5.02670907974s
step 8472/400000: train loss: 0.154217
epoch:184, batch10, load frames use: 4.76474499702s
step 8473/400000: train loss: 0.166683
epoch:184, batch11, load frames use: 5.10504198074s
step 8474/400000: train loss: 0.139669
epoch:184, batch12, load frames use: 4.69591116905s
step 8475/400000: train loss: 0.155297
epoch:184, batch13, load frames use: 4.59435796738s
step 8476/400000: train loss: 0.174741
epoch:184, batch14, load frames use: 4.61173796654s
step 8477/400000: train loss: 0.143136
epoch:184, batch15, load frames use: 4.72352409363s
step 8478/400000: train loss: 0.149223
epoch:184, batch16, load frames use: 5.22593402863s
step 8479/400000: train loss: 0.149509
epoch:184, batch17, load frames use: 4.34864211082s
step 8480/400000: train loss: 0.152658
epoch:184, batch18, load frames use: 4.67806506157s
step 8481/400000: train loss: 0.163202
epoch:184, batch19, load frames use: 4.71021580696s
step 8482/400000: train loss: 0.156625
epoch:184, batch20, load frames use: 4.85127782822s
step 8483/400000: train loss: 0.164774
epoch:184, batch21, load frames use: 4.66546392441s
step 8484/400000: train loss: 0.145367
epoch:184, batch22, load frames use: 4.79142999649s
step 8485/400000: train loss: 0.152243
epoch:184, batch23, load frames use: 4.47604393959s
step 8486/400000: train loss: 0.143843
epoch:184, batch24, load frames use: 4.94739294052s
step 8487/400000: train loss: 0.139654
epoch:184, batch25, load frames use: 5.03403615952s
step 8488/400000: train loss: 0.157023
epoch:184, batch26, load frames use: 4.01723504066s
step 8489/400000: train loss: 0.145581
epoch:184, batch27, load frames use: 4.51999902725s
step 8490/400000: train loss: 0.153704
epoch:184, batch28, load frames use: 4.38239693642s
step 8491/400000: train loss: 0.152874
epoch:184, batch29, load frames use: 4.56033396721s
step 8492/400000: train loss: 0.163411
epoch:184, batch30, load frames use: 5.27364397049s
step 8493/400000: train loss: 0.140597
epoch:184, batch31, load frames use: 4.27433514595s
step 8494/400000: train loss: 0.156011
epoch:184, batch32, load frames use: 4.03102898598s
step 8495/400000: train loss: 0.160036
epoch:184, batch33, load frames use: 4.60016918182s
step 8496/400000: train loss: 0.143404
epoch:184, batch34, load frames use: 4.44767093658s
step 8497/400000: train loss: 0.161750
epoch:184, batch35, load frames use: 4.79282402992s
step 8498/400000: train loss: 0.150267
epoch:184, batch36, load frames use: 5.06658005714s
step 8499/400000: train loss: 0.150041
epoch:184, batch37, load frames use: 4.39770197868s
step 8500/400000: train loss: 0.159630
step 8500/400000: validation loss: 0.141424
epoch:184, batch38, load frames use: 4.4125430584s
step 8501/400000: train loss: 0.155314
epoch:184, batch39, load frames use: 4.20753312111s
step 8502/400000: train loss: 0.159204
epoch:184, batch40, load frames use: 4.63568806648s
step 8503/400000: train loss: 0.154896
epoch:184, batch41, load frames use: 4.80000591278s
step 8504/400000: train loss: 0.155789
epoch:184, batch42, load frames use: 4.65727710724s
step 8505/400000: train loss: 0.142531
epoch:184, batch43, load frames use: 4.88914084435s
step 8506/400000: train loss: 0.163618
epoch:184, batch44, load frames use: 4.0190179348s
step 8507/400000: train loss: 0.162407
epoch:184, batch45, load frames use: 4.67319488525s
step 8508/400000: train loss: 0.178202
epoch:184, batch46, load frames use: 4.76609992981s
step 8509/400000: train loss: 0.150677
epoch:185, batch1, load frames use: 4.96024799347s
step 8510/400000: train loss: 0.159582
epoch:185, batch2, load frames use: 4.89211297035s
step 8511/400000: train loss: 0.150975
epoch:185, batch3, load frames use: 4.89311790466s
step 8512/400000: train loss: 0.160227
epoch:185, batch4, load frames use: 5.31441187859s
step 8513/400000: train loss: 0.160266
epoch:185, batch5, load frames use: 4.69941902161s
step 8514/400000: train loss: 0.144496
epoch:185, batch6, load frames use: 4.65344309807s
step 8515/400000: train loss: 0.186305
epoch:185, batch7, load frames use: 4.21093010902s
step 8516/400000: train loss: 0.143693
epoch:185, batch8, load frames use: 4.29623007774s
step 8517/400000: train loss: 0.157346
epoch:185, batch9, load frames use: 5.33045697212s
step 8518/400000: train loss: 0.151105
epoch:185, batch10, load frames use: 4.74173307419s
step 8519/400000: train loss: 0.159761
epoch:185, batch11, load frames use: 4.75760197639s
step 8520/400000: train loss: 0.160398
epoch:185, batch12, load frames use: 4.93610310555s
step 8521/400000: train loss: 0.137497
epoch:185, batch13, load frames use: 4.41481184959s
step 8522/400000: train loss: 0.160277
epoch:185, batch14, load frames use: 4.55276203156s
step 8523/400000: train loss: 0.133616
epoch:185, batch15, load frames use: 4.29539394379s
step 8524/400000: train loss: 0.153522
epoch:185, batch16, load frames use: 5.4889330864s
step 8525/400000: train loss: 0.154141
epoch:185, batch17, load frames use: 4.5969350338s
step 8526/400000: train loss: 0.149678
epoch:185, batch18, load frames use: 4.91266012192s
step 8527/400000: train loss: 0.162091
epoch:185, batch19, load frames use: 4.55082011223s
step 8528/400000: train loss: 0.164716
epoch:185, batch20, load frames use: 4.51955604553s
step 8529/400000: train loss: 0.164474
epoch:185, batch21, load frames use: 4.66143894196s
step 8530/400000: train loss: 0.150867
epoch:185, batch22, load frames use: 4.55928397179s
step 8531/400000: train loss: 0.132509
epoch:185, batch23, load frames use: 4.82282710075s
step 8532/400000: train loss: 0.156344
epoch:185, batch24, load frames use: 4.17426204681s
step 8533/400000: train loss: 0.153791
epoch:185, batch25, load frames use: 4.63106799126s
step 8534/400000: train loss: 0.145451
epoch:185, batch26, load frames use: 4.34235191345s
step 8535/400000: train loss: 0.158772
epoch:185, batch27, load frames use: 4.99819993973s
step 8536/400000: train loss: 0.153484
epoch:185, batch28, load frames use: 4.52523303032s
step 8537/400000: train loss: 0.155219
epoch:185, batch29, load frames use: 4.89472103119s
step 8538/400000: train loss: 0.148460
epoch:185, batch30, load frames use: 4.73656988144s
step 8539/400000: train loss: 0.174424
epoch:185, batch31, load frames use: 3.87147402763s
step 8540/400000: train loss: 0.174405
epoch:185, batch32, load frames use: 4.71514797211s
step 8541/400000: train loss: 0.154158
epoch:185, batch33, load frames use: 3.93995499611s
step 8542/400000: train loss: 0.161452
epoch:185, batch34, load frames use: 4.41905617714s
step 8543/400000: train loss: 0.160612
epoch:185, batch35, load frames use: 4.12375998497s
step 8544/400000: train loss: 0.145960
epoch:185, batch36, load frames use: 4.81278991699s
step 8545/400000: train loss: 0.141034
epoch:185, batch37, load frames use: 4.59203195572s
step 8546/400000: train loss: 0.135286
epoch:185, batch38, load frames use: 4.07862496376s
step 8547/400000: train loss: 0.143423
epoch:185, batch39, load frames use: 4.62818408012s
step 8548/400000: train loss: 0.161703
epoch:185, batch40, load frames use: 4.40408992767s
step 8549/400000: train loss: 0.155642
epoch:185, batch41, load frames use: 4.68368506432s
step 8550/400000: train loss: 0.157793
epoch:185, batch42, load frames use: 4.65643596649s
step 8551/400000: train loss: 0.154852
epoch:185, batch43, load frames use: 4.8553211689s
step 8552/400000: train loss: 0.147860
epoch:185, batch44, load frames use: 4.02652001381s
step 8553/400000: train loss: 0.171028
epoch:185, batch45, load frames use: 4.19730997086s
step 8554/400000: train loss: 0.158990
epoch:185, batch46, load frames use: 3.62979102135s
step 8555/400000: train loss: 0.132842
epoch:186, batch1, load frames use: 3.64902305603s
step 8556/400000: train loss: 0.142784
epoch:186, batch2, load frames use: 4.3301589489s
step 8557/400000: train loss: 0.155425
epoch:186, batch3, load frames use: 4.61554908752s
step 8558/400000: train loss: 0.131785
epoch:186, batch4, load frames use: 4.60708498955s
step 8559/400000: train loss: 0.137552
epoch:186, batch5, load frames use: 4.57565116882s
step 8560/400000: train loss: 0.158384
epoch:186, batch6, load frames use: 4.55138301849s
step 8561/400000: train loss: 0.156159
epoch:186, batch7, load frames use: 4.39465308189s
step 8562/400000: train loss: 0.147967
epoch:186, batch8, load frames use: 5.49516415596s
step 8563/400000: train loss: 0.140188
epoch:186, batch9, load frames use: 4.76616501808s
step 8564/400000: train loss: 0.139917
epoch:186, batch10, load frames use: 4.5107319355s
step 8565/400000: train loss: 0.142520
epoch:186, batch11, load frames use: 4.72839808464s
step 8566/400000: train loss: 0.135086
epoch:186, batch12, load frames use: 4.66006088257s
step 8567/400000: train loss: 0.144041
epoch:186, batch13, load frames use: 5.15119600296s
step 8568/400000: train loss: 0.148710
epoch:186, batch14, load frames use: 4.51392889023s
step 8569/400000: train loss: 0.149826
epoch:186, batch15, load frames use: 5.0330851078s
step 8570/400000: train loss: 0.145378
epoch:186, batch16, load frames use: 4.43148207664s
step 8571/400000: train loss: 0.144462
epoch:186, batch17, load frames use: 4.71532583237s
step 8572/400000: train loss: 0.169155
epoch:186, batch18, load frames use: 4.13503789902s
step 8573/400000: train loss: 0.138265
epoch:186, batch19, load frames use: 5.26081585884s
step 8574/400000: train loss: 0.165284
epoch:186, batch20, load frames use: 4.66131401062s
step 8575/400000: train loss: 0.146530
epoch:186, batch21, load frames use: 4.49340701103s
step 8576/400000: train loss: 0.175642
epoch:186, batch22, load frames use: 4.51861190796s
step 8577/400000: train loss: 0.173376
epoch:186, batch23, load frames use: 4.37571406364s
step 8578/400000: train loss: 0.145840
epoch:186, batch24, load frames use: 5.07675600052s
step 8579/400000: train loss: 0.150101
epoch:186, batch25, load frames use: 4.86433315277s
step 8580/400000: train loss: 0.153446
epoch:186, batch26, load frames use: 4.67885899544s
step 8581/400000: train loss: 0.164189
epoch:186, batch27, load frames use: 4.47468686104s
step 8582/400000: train loss: 0.179349
epoch:186, batch28, load frames use: 4.44836091995s
step 8583/400000: train loss: 0.164771
epoch:186, batch29, load frames use: 4.57103919983s
step 8584/400000: train loss: 0.149704
epoch:186, batch30, load frames use: 4.597219944s
step 8585/400000: train loss: 0.161689
epoch:186, batch31, load frames use: 5.20866799355s
step 8586/400000: train loss: 0.164006
epoch:186, batch32, load frames use: 5.22304081917s
step 8587/400000: train loss: 0.160280
epoch:186, batch33, load frames use: 4.22439312935s
step 8588/400000: train loss: 0.135800
epoch:186, batch34, load frames use: 4.76761198044s
step 8589/400000: train loss: 0.147501
epoch:186, batch35, load frames use: 3.63305091858s
step 8590/400000: train loss: 0.144928
epoch:186, batch36, load frames use: 4.39788007736s
step 8591/400000: train loss: 0.143807
epoch:186, batch37, load frames use: 4.42902612686s
step 8592/400000: train loss: 0.159987
epoch:186, batch38, load frames use: 4.73719096184s
step 8593/400000: train loss: 0.173925
epoch:186, batch39, load frames use: 3.97709107399s
step 8594/400000: train loss: 0.145324
epoch:186, batch40, load frames use: 4.2423081398s
step 8595/400000: train loss: 0.166063
epoch:186, batch41, load frames use: 4.24063682556s
step 8596/400000: train loss: 0.168930
epoch:186, batch42, load frames use: 4.21965909004s
step 8597/400000: train loss: 0.146702
epoch:186, batch43, load frames use: 5.03325295448s
step 8598/400000: train loss: 0.166131
epoch:186, batch44, load frames use: 5.12563896179s
step 8599/400000: train loss: 0.149918
epoch:186, batch45, load frames use: 4.74556684494s
step 8600/400000: train loss: 0.149209
step 8600/400000: validation loss: 0.140832
epoch:186, batch46, load frames use: 4.7353990078s
step 8601/400000: train loss: 0.141066
epoch:187, batch1, load frames use: 5.17634701729s
step 8602/400000: train loss: 0.150674
epoch:187, batch2, load frames use: 4.72082185745s
step 8603/400000: train loss: 0.143355
epoch:187, batch3, load frames use: 4.22322010994s
step 8604/400000: train loss: 0.151531
epoch:187, batch4, load frames use: 4.868019104s
step 8605/400000: train loss: 0.151255
epoch:187, batch5, load frames use: 4.77838301659s
step 8606/400000: train loss: 0.160708
epoch:187, batch6, load frames use: 3.93499016762s
step 8607/400000: train loss: 0.145705
epoch:187, batch7, load frames use: 4.41454005241s
step 8608/400000: train loss: 0.167002
epoch:187, batch8, load frames use: 4.14656209946s
step 8609/400000: train loss: 0.155870
epoch:187, batch9, load frames use: 4.57455706596s
step 8610/400000: train loss: 0.167028
epoch:187, batch10, load frames use: 5.34597110748s
step 8611/400000: train loss: 0.170308
epoch:187, batch11, load frames use: 4.22493100166s
step 8612/400000: train loss: 0.148215
epoch:187, batch12, load frames use: 4.21426987648s
step 8613/400000: train loss: 0.136936
epoch:187, batch13, load frames use: 4.05696201324s
step 8614/400000: train loss: 0.133414
epoch:187, batch14, load frames use: 5.03584003448s
step 8615/400000: train loss: 0.148257
epoch:187, batch15, load frames use: 4.71627283096s
step 8616/400000: train loss: 0.140603
epoch:187, batch16, load frames use: 5.06312203407s
step 8617/400000: train loss: 0.151274
epoch:187, batch17, load frames use: 4.58258891106s
step 8618/400000: train loss: 0.161594
epoch:187, batch18, load frames use: 4.49714303017s
step 8619/400000: train loss: 0.155304
epoch:187, batch19, load frames use: 4.54977893829s
step 8620/400000: train loss: 0.155073
epoch:187, batch20, load frames use: 4.86739778519s
step 8621/400000: train loss: 0.144704
epoch:187, batch21, load frames use: 4.65831899643s
step 8622/400000: train loss: 0.141626
epoch:187, batch22, load frames use: 4.76404905319s
step 8623/400000: train loss: 0.151937
epoch:187, batch23, load frames use: 5.00421500206s
step 8624/400000: train loss: 0.163522
epoch:187, batch24, load frames use: 5.10716295242s
step 8625/400000: train loss: 0.155841
epoch:187, batch25, load frames use: 4.41085505486s
step 8626/400000: train loss: 0.134470
epoch:187, batch26, load frames use: 4.6908249855s
step 8627/400000: train loss: 0.149392
epoch:187, batch27, load frames use: 5.22267818451s
step 8628/400000: train loss: 0.169881
epoch:187, batch28, load frames use: 5.1777780056s
step 8629/400000: train loss: 0.137121
epoch:187, batch29, load frames use: 4.83120989799s
step 8630/400000: train loss: 0.154193
epoch:187, batch30, load frames use: 4.2790749073s
step 8631/400000: train loss: 0.149053
epoch:187, batch31, load frames use: 4.76455616951s
step 8632/400000: train loss: 0.161271
epoch:187, batch32, load frames use: 4.4147040844s
step 8633/400000: train loss: 0.151144
epoch:187, batch33, load frames use: 4.29876613617s
step 8634/400000: train loss: 0.152141
epoch:187, batch34, load frames use: 5.02675390244s
step 8635/400000: train loss: 0.165175
epoch:187, batch35, load frames use: 4.87450098991s
step 8636/400000: train loss: 0.144388
epoch:187, batch36, load frames use: 4.42858290672s
step 8637/400000: train loss: 0.164796
epoch:187, batch37, load frames use: 4.57022619247s
step 8638/400000: train loss: 0.171945
epoch:187, batch38, load frames use: 5.1388399601s
step 8639/400000: train loss: 0.159889
epoch:187, batch39, load frames use: 4.47473502159s
step 8640/400000: train loss: 0.160802
epoch:187, batch40, load frames use: 4.92550587654s
step 8641/400000: train loss: 0.160287
epoch:187, batch41, load frames use: 5.44788002968s
step 8642/400000: train loss: 0.185358
epoch:187, batch42, load frames use: 5.0020339489s
step 8643/400000: train loss: 0.174664
epoch:187, batch43, load frames use: 4.24957704544s
step 8644/400000: train loss: 0.145167
epoch:187, batch44, load frames use: 4.16102194786s
step 8645/400000: train loss: 0.159432
epoch:187, batch45, load frames use: 4.46147894859s
step 8646/400000: train loss: 0.153280
epoch:187, batch46, load frames use: 4.70658516884s
step 8647/400000: train loss: 0.150056
epoch:188, batch1, load frames use: 4.63923311234s
step 8648/400000: train loss: 0.157576
epoch:188, batch2, load frames use: 5.09441208839s
step 8649/400000: train loss: 0.158268
epoch:188, batch3, load frames use: 4.86819195747s
step 8650/400000: train loss: 0.155782
epoch:188, batch4, load frames use: 4.28025913239s
step 8651/400000: train loss: 0.139817
epoch:188, batch5, load frames use: 4.36267781258s
step 8652/400000: train loss: 0.159871
epoch:188, batch6, load frames use: 4.85924601555s
step 8653/400000: train loss: 0.164357
epoch:188, batch7, load frames use: 4.42132401466s
step 8654/400000: train loss: 0.143941
epoch:188, batch8, load frames use: 4.2399289608s
step 8655/400000: train loss: 0.140062
epoch:188, batch9, load frames use: 4.34596586227s
step 8656/400000: train loss: 0.148140
epoch:188, batch10, load frames use: 4.98648309708s
step 8657/400000: train loss: 0.151723
epoch:188, batch11, load frames use: 4.79217600822s
step 8658/400000: train loss: 0.166814
epoch:188, batch12, load frames use: 4.85538482666s
step 8659/400000: train loss: 0.147281
epoch:188, batch13, load frames use: 4.49127984047s
step 8660/400000: train loss: 0.159245
epoch:188, batch14, load frames use: 4.79080986977s
step 8661/400000: train loss: 0.149643
epoch:188, batch15, load frames use: 4.42467689514s
step 8662/400000: train loss: 0.175456
epoch:188, batch16, load frames use: 4.78689289093s
step 8663/400000: train loss: 0.153840
epoch:188, batch17, load frames use: 4.53903889656s
step 8664/400000: train loss: 0.157217
epoch:188, batch18, load frames use: 4.9468319416s
step 8665/400000: train loss: 0.147180
epoch:188, batch19, load frames use: 5.15632414818s
step 8666/400000: train loss: 0.165062
epoch:188, batch20, load frames use: 4.76736903191s
step 8667/400000: train loss: 0.175417
epoch:188, batch21, load frames use: 4.19674897194s
step 8668/400000: train loss: 0.153027
epoch:188, batch22, load frames use: 5.2126121521s
step 8669/400000: train loss: 0.165132
epoch:188, batch23, load frames use: 4.75262284279s
step 8670/400000: train loss: 0.185818
epoch:188, batch24, load frames use: 5.53449296951s
step 8671/400000: train loss: 0.148070
epoch:188, batch25, load frames use: 4.51037406921s
step 8672/400000: train loss: 0.157942
epoch:188, batch26, load frames use: 5.04617786407s
step 8673/400000: train loss: 0.156729
epoch:188, batch27, load frames use: 4.69738698006s
step 8674/400000: train loss: 0.149907
epoch:188, batch28, load frames use: 4.45624685287s
step 8675/400000: train loss: 0.148769
epoch:188, batch29, load frames use: 4.63636302948s
step 8676/400000: train loss: 0.153270
epoch:188, batch30, load frames use: 4.29187679291s
step 8677/400000: train loss: 0.163251
epoch:188, batch31, load frames use: 3.80232381821s
step 8678/400000: train loss: 0.148410
epoch:188, batch32, load frames use: 4.14108490944s
step 8679/400000: train loss: 0.136835
epoch:188, batch33, load frames use: 3.64640712738s
step 8680/400000: train loss: 0.155964
epoch:188, batch34, load frames use: 4.74542188644s
step 8681/400000: train loss: 0.177551
epoch:188, batch35, load frames use: 4.99684691429s
step 8682/400000: train loss: 0.162541
epoch:188, batch36, load frames use: 4.35480880737s
step 8683/400000: train loss: 0.153467
epoch:188, batch37, load frames use: 4.69809818268s
step 8684/400000: train loss: 0.150395
epoch:188, batch38, load frames use: 4.49743008614s
step 8685/400000: train loss: 0.166592
epoch:188, batch39, load frames use: 4.28457999229s
step 8686/400000: train loss: 0.160383
epoch:188, batch40, load frames use: 4.14790892601s
step 8687/400000: train loss: 0.149414
epoch:188, batch41, load frames use: 5.04111409187s
step 8688/400000: train loss: 0.163037
epoch:188, batch42, load frames use: 4.40522885323s
step 8689/400000: train loss: 0.143871
epoch:188, batch43, load frames use: 4.2314350605s
step 8690/400000: train loss: 0.157231
epoch:188, batch44, load frames use: 4.53205800056s
step 8691/400000: train loss: 0.149997
epoch:188, batch45, load frames use: 4.72462296486s
step 8692/400000: train loss: 0.161036
epoch:188, batch46, load frames use: 4.45208215714s
step 8693/400000: train loss: 0.161129
epoch:189, batch1, load frames use: 4.99502301216s
step 8694/400000: train loss: 0.142436
epoch:189, batch2, load frames use: 4.69248700142s
step 8695/400000: train loss: 0.153616
epoch:189, batch3, load frames use: 4.6327188015s
step 8696/400000: train loss: 0.156141
epoch:189, batch4, load frames use: 5.06149601936s
step 8697/400000: train loss: 0.139462
epoch:189, batch5, load frames use: 4.90941810608s
step 8698/400000: train loss: 0.165040
epoch:189, batch6, load frames use: 4.34580206871s
step 8699/400000: train loss: 0.135869
epoch:189, batch7, load frames use: 4.79299402237s
step 8700/400000: train loss: 0.125970
step 8700/400000: validation loss: 0.130384
epoch:189, batch8, load frames use: 4.97410082817s
step 8701/400000: train loss: 0.146123
epoch:189, batch9, load frames use: 4.88272213936s
step 8702/400000: train loss: 0.159555
epoch:189, batch10, load frames use: 4.95079994202s
step 8703/400000: train loss: 0.134184
epoch:189, batch11, load frames use: 4.74678015709s
step 8704/400000: train loss: 0.153524
epoch:189, batch12, load frames use: 4.53647899628s
step 8705/400000: train loss: 0.144214
epoch:189, batch13, load frames use: 5.32918691635s
step 8706/400000: train loss: 0.146309
epoch:189, batch14, load frames use: 4.85749578476s
step 8707/400000: train loss: 0.160361
epoch:189, batch15, load frames use: 5.01227617264s
step 8708/400000: train loss: 0.153767
epoch:189, batch16, load frames use: 5.10558199883s
step 8709/400000: train loss: 0.151633
epoch:189, batch17, load frames use: 5.86882305145s
step 8710/400000: train loss: 0.138054
epoch:189, batch18, load frames use: 3.97286510468s
step 8711/400000: train loss: 0.170347
epoch:189, batch19, load frames use: 4.21041989326s
step 8712/400000: train loss: 0.145371
epoch:189, batch20, load frames use: 4.72196698189s
step 8713/400000: train loss: 0.158142
epoch:189, batch21, load frames use: 4.87698221207s
step 8714/400000: train loss: 0.130251
epoch:189, batch22, load frames use: 4.50489902496s
step 8715/400000: train loss: 0.143843
epoch:189, batch23, load frames use: 4.62317609787s
step 8716/400000: train loss: 0.157245
epoch:189, batch24, load frames use: 4.24274110794s
step 8717/400000: train loss: 0.145343
epoch:189, batch25, load frames use: 5.01973414421s
step 8718/400000: train loss: 0.161109
epoch:189, batch26, load frames use: 4.5873298645s
step 8719/400000: train loss: 0.145051
epoch:189, batch27, load frames use: 4.63928508759s
step 8720/400000: train loss: 0.173812
epoch:189, batch28, load frames use: 4.47252297401s
step 8721/400000: train loss: 0.170287
epoch:189, batch29, load frames use: 4.97126603127s
step 8722/400000: train loss: 0.170544
epoch:189, batch30, load frames use: 4.78979110718s
step 8723/400000: train loss: 0.143499
epoch:189, batch31, load frames use: 4.74187207222s
step 8724/400000: train loss: 0.150508
epoch:189, batch32, load frames use: 4.14663100243s
step 8725/400000: train loss: 0.144960
epoch:189, batch33, load frames use: 4.08000421524s
step 8726/400000: train loss: 0.160704
epoch:189, batch34, load frames use: 5.28302884102s
step 8727/400000: train loss: 0.171842
epoch:189, batch35, load frames use: 4.16825199127s
step 8728/400000: train loss: 0.162436
epoch:189, batch36, load frames use: 4.96329379082s
step 8729/400000: train loss: 0.159335
epoch:189, batch37, load frames use: 4.39256000519s
step 8730/400000: train loss: 0.164178
epoch:189, batch38, load frames use: 5.12442183495s
step 8731/400000: train loss: 0.171298
epoch:189, batch39, load frames use: 4.37154603004s
step 8732/400000: train loss: 0.157589
epoch:189, batch40, load frames use: 4.156717062s
step 8733/400000: train loss: 0.168754
epoch:189, batch41, load frames use: 5.05249500275s
step 8734/400000: train loss: 0.139999
epoch:189, batch42, load frames use: 4.9095749855s
step 8735/400000: train loss: 0.159640
epoch:189, batch43, load frames use: 3.77384805679s
step 8736/400000: train loss: 0.161025
epoch:189, batch44, load frames use: 5.48047804832s
step 8737/400000: train loss: 0.153546
epoch:189, batch45, load frames use: 5.13760900497s
step 8738/400000: train loss: 0.155576
epoch:189, batch46, load frames use: 4.59720492363s
step 8739/400000: train loss: 0.144455
epoch:190, batch1, load frames use: 4.70320796967s
step 8740/400000: train loss: 0.155760
epoch:190, batch2, load frames use: 4.66917085648s
step 8741/400000: train loss: 0.159212
epoch:190, batch3, load frames use: 4.76268601418s
step 8742/400000: train loss: 0.154074
epoch:190, batch4, load frames use: 4.91877198219s
step 8743/400000: train loss: 0.158820
epoch:190, batch5, load frames use: 4.99367308617s
step 8744/400000: train loss: 0.162762
epoch:190, batch6, load frames use: 4.49534893036s
step 8745/400000: train loss: 0.164744
epoch:190, batch7, load frames use: 5.22760891914s
step 8746/400000: train loss: 0.166511
epoch:190, batch8, load frames use: 4.77067899704s
step 8747/400000: train loss: 0.141552
epoch:190, batch9, load frames use: 4.3784930706s
step 8748/400000: train loss: 0.153149
epoch:190, batch10, load frames use: 4.76192498207s
step 8749/400000: train loss: 0.140038
epoch:190, batch11, load frames use: 4.23575091362s
step 8750/400000: train loss: 0.161051
epoch:190, batch12, load frames use: 5.24908804893s
step 8751/400000: train loss: 0.164900
epoch:190, batch13, load frames use: 4.01576685905s
step 8752/400000: train loss: 0.147128
epoch:190, batch14, load frames use: 4.67540407181s
step 8753/400000: train loss: 0.149037
epoch:190, batch15, load frames use: 3.95151996613s
step 8754/400000: train loss: 0.155751
epoch:190, batch16, load frames use: 4.01104211807s
step 8755/400000: train loss: 0.169508
epoch:190, batch17, load frames use: 4.16652083397s
step 8756/400000: train loss: 0.164205
epoch:190, batch18, load frames use: 4.31408596039s
step 8757/400000: train loss: 0.144345
epoch:190, batch19, load frames use: 4.34812808037s
step 8758/400000: train loss: 0.149104
epoch:190, batch20, load frames use: 4.73852109909s
step 8759/400000: train loss: 0.181959
epoch:190, batch21, load frames use: 4.52015280724s
step 8760/400000: train loss: 0.139929
epoch:190, batch22, load frames use: 4.09701085091s
step 8761/400000: train loss: 0.149195
epoch:190, batch23, load frames use: 3.91338801384s
step 8762/400000: train loss: 0.157763
epoch:190, batch24, load frames use: 4.3601231575s
step 8763/400000: train loss: 0.138495
epoch:190, batch25, load frames use: 4.96569299698s
step 8764/400000: train loss: 0.141536
epoch:190, batch26, load frames use: 4.95523500443s
step 8765/400000: train loss: 0.158889
epoch:190, batch27, load frames use: 4.57508015633s
step 8766/400000: train loss: 0.176822
epoch:190, batch28, load frames use: 4.87507390976s
step 8767/400000: train loss: 0.162941
epoch:190, batch29, load frames use: 4.47088503838s
step 8768/400000: train loss: 0.157936
epoch:190, batch30, load frames use: 4.94273304939s
step 8769/400000: train loss: 0.153913
epoch:190, batch31, load frames use: 4.63531589508s
step 8770/400000: train loss: 0.145440
epoch:190, batch32, load frames use: 4.83675312996s
step 8771/400000: train loss: 0.154790
epoch:190, batch33, load frames use: 4.96415877342s
step 8772/400000: train loss: 0.150266
epoch:190, batch34, load frames use: 4.34100484848s
step 8773/400000: train loss: 0.158017
epoch:190, batch35, load frames use: 4.95726513863s
step 8774/400000: train loss: 0.159908
epoch:190, batch36, load frames use: 4.81947612762s
step 8775/400000: train loss: 0.163705
epoch:190, batch37, load frames use: 4.34162211418s
step 8776/400000: train loss: 0.136653
epoch:190, batch38, load frames use: 4.99609899521s
step 8777/400000: train loss: 0.129661
epoch:190, batch39, load frames use: 4.37104916573s
step 8778/400000: train loss: 0.146759
epoch:190, batch40, load frames use: 4.72268986702s
step 8779/400000: train loss: 0.175040
epoch:190, batch41, load frames use: 4.98061490059s
step 8780/400000: train loss: 0.148648
epoch:190, batch42, load frames use: 5.0557551384s
step 8781/400000: train loss: 0.155914
epoch:190, batch43, load frames use: 4.71136307716s
step 8782/400000: train loss: 0.153383
epoch:190, batch44, load frames use: 4.35310482979s
step 8783/400000: train loss: 0.167683
epoch:190, batch45, load frames use: 4.69096899033s
step 8784/400000: train loss: 0.149886
epoch:190, batch46, load frames use: 4.9820959568s
step 8785/400000: train loss: 0.146954
epoch:191, batch1, load frames use: 4.65764403343s
step 8786/400000: train loss: 0.148497
epoch:191, batch2, load frames use: 4.64856004715s
step 8787/400000: train loss: 0.139378
epoch:191, batch3, load frames use: 4.4404540062s
step 8788/400000: train loss: 0.133436
epoch:191, batch4, load frames use: 4.06723308563s
step 8789/400000: train loss: 0.132925
epoch:191, batch5, load frames use: 4.37748885155s
step 8790/400000: train loss: 0.137437
epoch:191, batch6, load frames use: 4.90996098518s
step 8791/400000: train loss: 0.144499
epoch:191, batch7, load frames use: 4.83706998825s
step 8792/400000: train loss: 0.144322
epoch:191, batch8, load frames use: 5.04139399529s
step 8793/400000: train loss: 0.155990
epoch:191, batch9, load frames use: 4.00057697296s
step 8794/400000: train loss: 0.147087
epoch:191, batch10, load frames use: 4.55589318275s
step 8795/400000: train loss: 0.148138
epoch:191, batch11, load frames use: 4.78512215614s
step 8796/400000: train loss: 0.146338
epoch:191, batch12, load frames use: 4.28894591331s
step 8797/400000: train loss: 0.144224
epoch:191, batch13, load frames use: 4.90968108177s
step 8798/400000: train loss: 0.152464
epoch:191, batch14, load frames use: 4.9554438591s
step 8799/400000: train loss: 0.159114
epoch:191, batch15, load frames use: 5.15982198715s
step 8800/400000: train loss: 0.161125
step 8800/400000: validation loss: 0.146336
epoch:191, batch16, load frames use: 4.51565098763s
step 8801/400000: train loss: 0.154174
epoch:191, batch17, load frames use: 4.55940699577s
step 8802/400000: train loss: 0.155793
epoch:191, batch18, load frames use: 4.16170096397s
step 8803/400000: train loss: 0.144884
epoch:191, batch19, load frames use: 4.43194103241s
step 8804/400000: train loss: 0.153880
epoch:191, batch20, load frames use: 4.20898389816s
step 8805/400000: train loss: 0.157251
epoch:191, batch21, load frames use: 4.2778570652s
step 8806/400000: train loss: 0.159819
epoch:191, batch22, load frames use: 5.11090183258s
step 8807/400000: train loss: 0.166257
epoch:191, batch23, load frames use: 5.13841414452s
step 8808/400000: train loss: 0.160517
epoch:191, batch24, load frames use: 5.02454710007s
step 8809/400000: train loss: 0.149774
epoch:191, batch25, load frames use: 5.01452898979s
step 8810/400000: train loss: 0.150920
epoch:191, batch26, load frames use: 4.14611196518s
step 8811/400000: train loss: 0.163056
epoch:191, batch27, load frames use: 4.0843398571s
step 8812/400000: train loss: 0.170865
epoch:191, batch28, load frames use: 4.25383806229s
step 8813/400000: train loss: 0.156488
epoch:191, batch29, load frames use: 4.55840086937s
step 8814/400000: train loss: 0.150691
epoch:191, batch30, load frames use: 4.68415999413s
step 8815/400000: train loss: 0.143389
epoch:191, batch31, load frames use: 4.67752218246s
step 8816/400000: train loss: 0.144680
epoch:191, batch32, load frames use: 4.71652412415s
step 8817/400000: train loss: 0.137913
epoch:191, batch33, load frames use: 4.53492999077s
step 8818/400000: train loss: 0.154750
epoch:191, batch34, load frames use: 4.88907122612s
step 8819/400000: train loss: 0.161410
epoch:191, batch35, load frames use: 4.77650380135s
step 8820/400000: train loss: 0.158432
epoch:191, batch36, load frames use: 5.11457490921s
step 8821/400000: train loss: 0.155399
epoch:191, batch37, load frames use: 4.59606194496s
step 8822/400000: train loss: 0.144170
epoch:191, batch38, load frames use: 4.98899912834s
step 8823/400000: train loss: 0.170908
epoch:191, batch39, load frames use: 4.80222201347s
step 8824/400000: train loss: 0.151305
epoch:191, batch40, load frames use: 4.50498104095s
step 8825/400000: train loss: 0.134705
epoch:191, batch41, load frames use: 4.97597885132s
step 8826/400000: train loss: 0.134886
epoch:191, batch42, load frames use: 5.23733186722s
step 8827/400000: train loss: 0.137341
epoch:191, batch43, load frames use: 4.41041707993s
step 8828/400000: train loss: 0.161393
epoch:191, batch44, load frames use: 4.75279998779s
step 8829/400000: train loss: 0.151711
epoch:191, batch45, load frames use: 5.11815214157s
step 8830/400000: train loss: 0.162229
epoch:191, batch46, load frames use: 4.72653412819s
step 8831/400000: train loss: 0.143400
epoch:192, batch1, load frames use: 4.28379893303s
step 8832/400000: train loss: 0.138070
epoch:192, batch2, load frames use: 4.80913209915s
step 8833/400000: train loss: 0.148561
epoch:192, batch3, load frames use: 4.63822793961s
step 8834/400000: train loss: 0.145018
epoch:192, batch4, load frames use: 4.6436560154s
step 8835/400000: train loss: 0.176753
epoch:192, batch5, load frames use: 4.37054991722s
step 8836/400000: train loss: 0.143678
epoch:192, batch6, load frames use: 4.56992888451s
step 8837/400000: train loss: 0.138727
epoch:192, batch7, load frames use: 4.51468205452s
step 8838/400000: train loss: 0.158117
epoch:192, batch8, load frames use: 4.70539212227s
step 8839/400000: train loss: 0.171022
epoch:192, batch9, load frames use: 4.52074599266s
step 8840/400000: train loss: 0.167946
epoch:192, batch10, load frames use: 5.54094791412s
step 8841/400000: train loss: 0.136620
epoch:192, batch11, load frames use: 4.90973305702s
step 8842/400000: train loss: 0.127447
epoch:192, batch12, load frames use: 4.52886795998s
step 8843/400000: train loss: 0.162787
epoch:192, batch13, load frames use: 4.58957505226s
step 8844/400000: train loss: 0.150390
epoch:192, batch14, load frames use: 5.12305188179s
step 8845/400000: train loss: 0.161638
epoch:192, batch15, load frames use: 4.92142796516s
step 8846/400000: train loss: 0.150264
epoch:192, batch16, load frames use: 4.77597212791s
step 8847/400000: train loss: 0.148963
epoch:192, batch17, load frames use: 4.95328497887s
step 8848/400000: train loss: 0.152002
epoch:192, batch18, load frames use: 4.75623393059s
step 8849/400000: train loss: 0.143039
epoch:192, batch19, load frames use: 5.04064893723s
step 8850/400000: train loss: 0.165739
epoch:192, batch20, load frames use: 4.74240493774s
step 8851/400000: train loss: 0.141248
epoch:192, batch21, load frames use: 4.47142314911s
step 8852/400000: train loss: 0.159191
epoch:192, batch22, load frames use: 5.27163290977s
step 8853/400000: train loss: 0.158882
epoch:192, batch23, load frames use: 4.70182085037s
step 8854/400000: train loss: 0.164255
epoch:192, batch24, load frames use: 4.53422999382s
step 8855/400000: train loss: 0.147084
epoch:192, batch25, load frames use: 4.24605178833s
step 8856/400000: train loss: 0.143564
epoch:192, batch26, load frames use: 4.91307902336s
step 8857/400000: train loss: 0.140847
epoch:192, batch27, load frames use: 4.39318704605s
step 8858/400000: train loss: 0.162836
epoch:192, batch28, load frames use: 4.42462491989s
step 8859/400000: train loss: 0.169181
epoch:192, batch29, load frames use: 4.33868908882s
step 8860/400000: train loss: 0.146605
epoch:192, batch30, load frames use: 4.70113801956s
step 8861/400000: train loss: 0.136593
epoch:192, batch31, load frames use: 5.08518505096s
step 8862/400000: train loss: 0.154729
epoch:192, batch32, load frames use: 4.70168614388s
step 8863/400000: train loss: 0.149225
epoch:192, batch33, load frames use: 3.68329501152s
step 8864/400000: train loss: 0.149487
epoch:192, batch34, load frames use: 4.71039295197s
step 8865/400000: train loss: 0.155474
epoch:192, batch35, load frames use: 4.94992208481s
step 8866/400000: train loss: 0.136330
epoch:192, batch36, load frames use: 5.16378498077s
step 8867/400000: train loss: 0.163152
epoch:192, batch37, load frames use: 5.62088012695s
step 8868/400000: train loss: 0.161247
epoch:192, batch38, load frames use: 5.02812004089s
step 8869/400000: train loss: 0.142042
epoch:192, batch39, load frames use: 4.68892502785s
step 8870/400000: train loss: 0.159191
epoch:192, batch40, load frames use: 4.66594910622s
step 8871/400000: train loss: 0.154367
epoch:192, batch41, load frames use: 4.87822508812s
step 8872/400000: train loss: 0.145175
epoch:192, batch42, load frames use: 4.86678099632s
step 8873/400000: train loss: 0.161943
epoch:192, batch43, load frames use: 4.67831087112s
step 8874/400000: train loss: 0.163653
epoch:192, batch44, load frames use: 4.34277105331s
step 8875/400000: train loss: 0.154968
epoch:192, batch45, load frames use: 5.31247282028s
step 8876/400000: train loss: 0.142216
epoch:192, batch46, load frames use: 4.28029489517s
step 8877/400000: train loss: 0.138936
epoch:193, batch1, load frames use: 4.85206389427s
step 8878/400000: train loss: 0.162060
epoch:193, batch2, load frames use: 4.90971207619s
step 8879/400000: train loss: 0.146184
epoch:193, batch3, load frames use: 4.60664916039s
step 8880/400000: train loss: 0.140580
epoch:193, batch4, load frames use: 4.32263708115s
step 8881/400000: train loss: 0.145261
epoch:193, batch5, load frames use: 5.1456758976s
step 8882/400000: train loss: 0.149524
epoch:193, batch6, load frames use: 4.4110159874s
step 8883/400000: train loss: 0.145262
epoch:193, batch7, load frames use: 5.07608914375s
step 8884/400000: train loss: 0.138125
epoch:193, batch8, load frames use: 4.47823691368s
step 8885/400000: train loss: 0.147807
epoch:193, batch9, load frames use: 4.10998487473s
step 8886/400000: train loss: 0.152392
epoch:193, batch10, load frames use: 4.96618390083s
step 8887/400000: train loss: 0.123159
epoch:193, batch11, load frames use: 4.26890707016s
step 8888/400000: train loss: 0.154644
epoch:193, batch12, load frames use: 4.54717493057s
step 8889/400000: train loss: 0.139075
epoch:193, batch13, load frames use: 5.14045500755s
step 8890/400000: train loss: 0.138973
epoch:193, batch14, load frames use: 4.86479592323s
step 8891/400000: train loss: 0.142695
epoch:193, batch15, load frames use: 4.81736803055s
step 8892/400000: train loss: 0.141399
epoch:193, batch16, load frames use: 4.05276083946s
step 8893/400000: train loss: 0.143023
epoch:193, batch17, load frames use: 5.12440896034s
step 8894/400000: train loss: 0.141151
epoch:193, batch18, load frames use: 4.43762898445s
step 8895/400000: train loss: 0.135354
epoch:193, batch19, load frames use: 4.50090312958s
step 8896/400000: train loss: 0.152354
epoch:193, batch20, load frames use: 4.5991871357s
step 8897/400000: train loss: 0.149644
epoch:193, batch21, load frames use: 4.89438986778s
step 8898/400000: train loss: 0.159713
epoch:193, batch22, load frames use: 5.04268503189s
step 8899/400000: train loss: 0.152913
epoch:193, batch23, load frames use: 4.76657509804s
step 8900/400000: train loss: 0.156053
step 8900/400000: validation loss: 0.149723
epoch:193, batch24, load frames use: 5.01961708069s
step 8901/400000: train loss: 0.149688
epoch:193, batch25, load frames use: 4.38784503937s
step 8902/400000: train loss: 0.162055
epoch:193, batch26, load frames use: 5.02451205254s
step 8903/400000: train loss: 0.159706
epoch:193, batch27, load frames use: 4.96462678909s
step 8904/400000: train loss: 0.147109
epoch:193, batch28, load frames use: 4.84180903435s
step 8905/400000: train loss: 0.144039
epoch:193, batch29, load frames use: 4.66403198242s
step 8906/400000: train loss: 0.152251
epoch:193, batch30, load frames use: 4.3677649498s
step 8907/400000: train loss: 0.161960
epoch:193, batch31, load frames use: 5.00095891953s
step 8908/400000: train loss: 0.150419
epoch:193, batch32, load frames use: 4.69538402557s
step 8909/400000: train loss: 0.128006
epoch:193, batch33, load frames use: 4.66575813293s
step 8910/400000: train loss: 0.171590
epoch:193, batch34, load frames use: 4.28986692429s
step 8911/400000: train loss: 0.156114
epoch:193, batch35, load frames use: 4.53990387917s
step 8912/400000: train loss: 0.146641
epoch:193, batch36, load frames use: 4.36816287041s
step 8913/400000: train loss: 0.152664
epoch:193, batch37, load frames use: 5.0241189003s
step 8914/400000: train loss: 0.153179
epoch:193, batch38, load frames use: 4.82930588722s
step 8915/400000: train loss: 0.146961
epoch:193, batch39, load frames use: 4.65660905838s
step 8916/400000: train loss: 0.161137
epoch:193, batch40, load frames use: 4.43910598755s
step 8917/400000: train loss: 0.152033
epoch:193, batch41, load frames use: 4.51998114586s
step 8918/400000: train loss: 0.142579
epoch:193, batch42, load frames use: 4.97385716438s
step 8919/400000: train loss: 0.155598
epoch:193, batch43, load frames use: 4.91127610207s
step 8920/400000: train loss: 0.157933
epoch:193, batch44, load frames use: 4.50152897835s
step 8921/400000: train loss: 0.148916
epoch:193, batch45, load frames use: 5.08749604225s
step 8922/400000: train loss: 0.162201
epoch:193, batch46, load frames use: 4.32795596123s
step 8923/400000: train loss: 0.135656
epoch:194, batch1, load frames use: 4.85535597801s
step 8924/400000: train loss: 0.150604
epoch:194, batch2, load frames use: 4.45564293861s
step 8925/400000: train loss: 0.148451
epoch:194, batch3, load frames use: 4.61802697182s
step 8926/400000: train loss: 0.159732
epoch:194, batch4, load frames use: 4.92694687843s
step 8927/400000: train loss: 0.147764
epoch:194, batch5, load frames use: 4.6827249527s
step 8928/400000: train loss: 0.142801
epoch:194, batch6, load frames use: 4.83627414703s
step 8929/400000: train loss: 0.169336
epoch:194, batch7, load frames use: 3.98048186302s
step 8930/400000: train loss: 0.155734
epoch:194, batch8, load frames use: 4.57137703896s
step 8931/400000: train loss: 0.147730
epoch:194, batch9, load frames use: 4.45619893074s
step 8932/400000: train loss: 0.182421
epoch:194, batch10, load frames use: 4.69499588013s
step 8933/400000: train loss: 0.142177
epoch:194, batch11, load frames use: 4.87206506729s
step 8934/400000: train loss: 0.153442
epoch:194, batch12, load frames use: 4.36053800583s
step 8935/400000: train loss: 0.137168
epoch:194, batch13, load frames use: 4.760212183s
step 8936/400000: train loss: 0.137975
epoch:194, batch14, load frames use: 3.99379205704s
step 8937/400000: train loss: 0.138962
epoch:194, batch15, load frames use: 4.66270399094s
step 8938/400000: train loss: 0.159722
epoch:194, batch16, load frames use: 4.40799593925s
step 8939/400000: train loss: 0.151843
epoch:194, batch17, load frames use: 4.64541602135s
step 8940/400000: train loss: 0.155671
epoch:194, batch18, load frames use: 5.25672793388s
step 8941/400000: train loss: 0.165560
epoch:194, batch19, load frames use: 4.90353107452s
step 8942/400000: train loss: 0.159084
epoch:194, batch20, load frames use: 5.77115607262s
step 8943/400000: train loss: 0.146509
epoch:194, batch21, load frames use: 4.14439797401s
step 8944/400000: train loss: 0.149949
epoch:194, batch22, load frames use: 4.84483194351s
step 8945/400000: train loss: 0.140928
epoch:194, batch23, load frames use: 4.72513723373s
step 8946/400000: train loss: 0.178645
epoch:194, batch24, load frames use: 4.61737895012s
step 8947/400000: train loss: 0.138439
epoch:194, batch25, load frames use: 4.4935259819s
step 8948/400000: train loss: 0.142996
epoch:194, batch26, load frames use: 4.93645906448s
step 8949/400000: train loss: 0.160364
epoch:194, batch27, load frames use: 4.48041605949s
step 8950/400000: train loss: 0.142392
epoch:194, batch28, load frames use: 3.75467300415s
step 8951/400000: train loss: 0.157457
epoch:194, batch29, load frames use: 4.11238908768s
step 8952/400000: train loss: 0.140738
epoch:194, batch30, load frames use: 4.09303998947s
step 8953/400000: train loss: 0.145991
epoch:194, batch31, load frames use: 4.11436390877s
step 8954/400000: train loss: 0.149532
epoch:194, batch32, load frames use: 4.38295602798s
step 8955/400000: train loss: 0.155417
epoch:194, batch33, load frames use: 4.07925605774s
step 8956/400000: train loss: 0.147122
epoch:194, batch34, load frames use: 3.7607960701s
step 8957/400000: train loss: 0.167062
epoch:194, batch35, load frames use: 4.60699105263s
step 8958/400000: train loss: 0.163052
epoch:194, batch36, load frames use: 4.69370007515s
step 8959/400000: train loss: 0.148229
epoch:194, batch37, load frames use: 4.40728902817s
step 8960/400000: train loss: 0.166544
epoch:194, batch38, load frames use: 4.71676588058s
step 8961/400000: train loss: 0.143494
epoch:194, batch39, load frames use: 4.32602787018s
step 8962/400000: train loss: 0.147938
epoch:194, batch40, load frames use: 4.52229094505s
step 8963/400000: train loss: 0.146653
epoch:194, batch41, load frames use: 4.68973684311s
step 8964/400000: train loss: 0.161444
epoch:194, batch42, load frames use: 4.04899907112s
step 8965/400000: train loss: 0.157883
epoch:194, batch43, load frames use: 5.07243013382s
step 8966/400000: train loss: 0.150690
epoch:194, batch44, load frames use: 4.1587240696s
step 8967/400000: train loss: 0.163890
epoch:194, batch45, load frames use: 4.19871592522s
step 8968/400000: train loss: 0.156193
epoch:194, batch46, load frames use: 3.79048395157s
step 8969/400000: train loss: 0.176361
epoch:195, batch1, load frames use: 4.66560196877s
step 8970/400000: train loss: 0.134614
epoch:195, batch2, load frames use: 4.25663590431s
step 8971/400000: train loss: 0.137669
epoch:195, batch3, load frames use: 4.38008594513s
step 8972/400000: train loss: 0.141122
epoch:195, batch4, load frames use: 4.77565693855s
step 8973/400000: train loss: 0.155823
epoch:195, batch5, load frames use: 5.18754291534s
step 8974/400000: train loss: 0.162829
epoch:195, batch6, load frames use: 4.5792350769s
step 8975/400000: train loss: 0.145865
epoch:195, batch7, load frames use: 4.80510997772s
step 8976/400000: train loss: 0.146864
epoch:195, batch8, load frames use: 4.57363510132s
step 8977/400000: train loss: 0.152049
epoch:195, batch9, load frames use: 5.12537693977s
step 8978/400000: train loss: 0.165456
epoch:195, batch10, load frames use: 4.19255900383s
step 8979/400000: train loss: 0.137501
epoch:195, batch11, load frames use: 4.22896313667s
step 8980/400000: train loss: 0.147388
epoch:195, batch12, load frames use: 5.12018990517s
step 8981/400000: train loss: 0.139588
epoch:195, batch13, load frames use: 4.46338200569s
step 8982/400000: train loss: 0.150541
epoch:195, batch14, load frames use: 4.69822502136s
step 8983/400000: train loss: 0.148308
epoch:195, batch15, load frames use: 3.81987595558s
step 8984/400000: train loss: 0.147901
epoch:195, batch16, load frames use: 4.82306909561s
step 8985/400000: train loss: 0.145785
epoch:195, batch17, load frames use: 3.91184186935s
step 8986/400000: train loss: 0.149710
epoch:195, batch18, load frames use: 4.59480690956s
step 8987/400000: train loss: 0.157763
epoch:195, batch19, load frames use: 4.71134591103s
step 8988/400000: train loss: 0.148942
epoch:195, batch20, load frames use: 4.80631709099s
step 8989/400000: train loss: 0.161222
epoch:195, batch21, load frames use: 4.43100500107s
step 8990/400000: train loss: 0.144068
epoch:195, batch22, load frames use: 4.2210290432s
step 8991/400000: train loss: 0.141354
epoch:195, batch23, load frames use: 4.76570820808s
step 8992/400000: train loss: 0.142767
epoch:195, batch24, load frames use: 4.4881029129s
step 8993/400000: train loss: 0.136355
epoch:195, batch25, load frames use: 4.59119105339s
step 8994/400000: train loss: 0.151116
epoch:195, batch26, load frames use: 4.96211218834s
step 8995/400000: train loss: 0.142545
epoch:195, batch27, load frames use: 4.69721508026s
step 8996/400000: train loss: 0.139129
epoch:195, batch28, load frames use: 4.37813091278s
step 8997/400000: train loss: 0.134439
epoch:195, batch29, load frames use: 4.53044509888s
step 8998/400000: train loss: 0.149856
epoch:195, batch30, load frames use: 4.29978680611s
step 8999/400000: train loss: 0.156058
epoch:195, batch31, load frames use: 4.83060693741s
step 9000/400000: train loss: 0.151799
step 9000/400000: validation loss: 0.147465
epoch:195, batch32, load frames use: 4.67571687698s
step 9001/400000: train loss: 0.150745
epoch:195, batch33, load frames use: 4.5658261776s
step 9002/400000: train loss: 0.135072
epoch:195, batch34, load frames use: 4.52166485786s
step 9003/400000: train loss: 0.150764
epoch:195, batch35, load frames use: 4.77096390724s
step 9004/400000: train loss: 0.154144
epoch:195, batch36, load frames use: 4.65440297127s
step 9005/400000: train loss: 0.153333
epoch:195, batch37, load frames use: 4.70376586914s
step 9006/400000: train loss: 0.138839
epoch:195, batch38, load frames use: 4.38833093643s
step 9007/400000: train loss: 0.155506
epoch:195, batch39, load frames use: 5.19637393951s
step 9008/400000: train loss: 0.174265
epoch:195, batch40, load frames use: 4.88808989525s
step 9009/400000: train loss: 0.135744
epoch:195, batch41, load frames use: 4.31146907806s
step 9010/400000: train loss: 0.138892
epoch:195, batch42, load frames use: 4.60372805595s
step 9011/400000: train loss: 0.160063
epoch:195, batch43, load frames use: 4.82479476929s
step 9012/400000: train loss: 0.152324
epoch:195, batch44, load frames use: 4.40500593185s
step 9013/400000: train loss: 0.137035
epoch:195, batch45, load frames use: 4.43544411659s
step 9014/400000: train loss: 0.152114
epoch:195, batch46, load frames use: 4.67416596413s
step 9015/400000: train loss: 0.144650
epoch:196, batch1, load frames use: 4.90633201599s
step 9016/400000: train loss: 0.153436
epoch:196, batch2, load frames use: 4.25821900368s
step 9017/400000: train loss: 0.154563
epoch:196, batch3, load frames use: 5.42872190475s
step 9018/400000: train loss: 0.150790
epoch:196, batch4, load frames use: 4.89715003967s
step 9019/400000: train loss: 0.125091
epoch:196, batch5, load frames use: 4.67861008644s
step 9020/400000: train loss: 0.169801
epoch:196, batch6, load frames use: 4.79966902733s
step 9021/400000: train loss: 0.149280
epoch:196, batch7, load frames use: 4.67564105988s
step 9022/400000: train loss: 0.145393
epoch:196, batch8, load frames use: 4.97685790062s
step 9023/400000: train loss: 0.146387
epoch:196, batch9, load frames use: 4.64831805229s
step 9024/400000: train loss: 0.161512
epoch:196, batch10, load frames use: 5.01956605911s
step 9025/400000: train loss: 0.148678
epoch:196, batch11, load frames use: 4.5974779129s
step 9026/400000: train loss: 0.134757
epoch:196, batch12, load frames use: 4.51643109322s
step 9027/400000: train loss: 0.150548
epoch:196, batch13, load frames use: 4.68734693527s
step 9028/400000: train loss: 0.159175
epoch:196, batch14, load frames use: 4.06124591827s
step 9029/400000: train loss: 0.153572
epoch:196, batch15, load frames use: 4.56650805473s
step 9030/400000: train loss: 0.133299
epoch:196, batch16, load frames use: 4.22018098831s
step 9031/400000: train loss: 0.150882
epoch:196, batch17, load frames use: 4.57548713684s
step 9032/400000: train loss: 0.152576
epoch:196, batch18, load frames use: 4.36674118042s
step 9033/400000: train loss: 0.162046
epoch:196, batch19, load frames use: 4.81330895424s
step 9034/400000: train loss: 0.151495
epoch:196, batch20, load frames use: 4.63825011253s
step 9035/400000: train loss: 0.148222
epoch:196, batch21, load frames use: 4.66706609726s
step 9036/400000: train loss: 0.142557
epoch:196, batch22, load frames use: 4.65272188187s
step 9037/400000: train loss: 0.139090
epoch:196, batch23, load frames use: 4.26330685616s
step 9038/400000: train loss: 0.155352
epoch:196, batch24, load frames use: 4.73963809013s
step 9039/400000: train loss: 0.143601
epoch:196, batch25, load frames use: 3.96550583839s
step 9040/400000: train loss: 0.142187
epoch:196, batch26, load frames use: 4.41899204254s
step 9041/400000: train loss: 0.150250
epoch:196, batch27, load frames use: 4.54860091209s
step 9042/400000: train loss: 0.171214
epoch:196, batch28, load frames use: 4.42662620544s
step 9043/400000: train loss: 0.157337
epoch:196, batch29, load frames use: 4.40979790688s
step 9044/400000: train loss: 0.150338
epoch:196, batch30, load frames use: 4.99221420288s
step 9045/400000: train loss: 0.140447
epoch:196, batch31, load frames use: 4.94662809372s
step 9046/400000: train loss: 0.137597
epoch:196, batch32, load frames use: 4.49018287659s
step 9047/400000: train loss: 0.153224
epoch:196, batch33, load frames use: 4.47427487373s
step 9048/400000: train loss: 0.159094
epoch:196, batch34, load frames use: 4.49154806137s
step 9049/400000: train loss: 0.163789
epoch:196, batch35, load frames use: 4.53447389603s
step 9050/400000: train loss: 0.142520
epoch:196, batch36, load frames use: 4.55665683746s
step 9051/400000: train loss: 0.161767
epoch:196, batch37, load frames use: 4.18041610718s
step 9052/400000: train loss: 0.139121
epoch:196, batch38, load frames use: 3.94421100616s
step 9053/400000: train loss: 0.156755
epoch:196, batch39, load frames use: 4.82437801361s
step 9054/400000: train loss: 0.159831
epoch:196, batch40, load frames use: 4.21796703339s
step 9055/400000: train loss: 0.157835
epoch:196, batch41, load frames use: 4.39402103424s
step 9056/400000: train loss: 0.143089
epoch:196, batch42, load frames use: 5.05633187294s
step 9057/400000: train loss: 0.142993
epoch:196, batch43, load frames use: 4.57856893539s
step 9058/400000: train loss: 0.160474
epoch:196, batch44, load frames use: 5.14174795151s
step 9059/400000: train loss: 0.153328
epoch:196, batch45, load frames use: 4.3386490345s
step 9060/400000: train loss: 0.156317
epoch:196, batch46, load frames use: 4.82838487625s
step 9061/400000: train loss: 0.158109
epoch:197, batch1, load frames use: 4.54312896729s
step 9062/400000: train loss: 0.134611
epoch:197, batch2, load frames use: 4.31114602089s
step 9063/400000: train loss: 0.138656
epoch:197, batch3, load frames use: 4.77758312225s
step 9064/400000: train loss: 0.150538
epoch:197, batch4, load frames use: 4.53200292587s
step 9065/400000: train loss: 0.138780
epoch:197, batch5, load frames use: 4.89056897163s
step 9066/400000: train loss: 0.150996
epoch:197, batch6, load frames use: 4.23360109329s
step 9067/400000: train loss: 0.146705
epoch:197, batch7, load frames use: 4.87686109543s
step 9068/400000: train loss: 0.136012
epoch:197, batch8, load frames use: 4.82212805748s
step 9069/400000: train loss: 0.166075
epoch:197, batch9, load frames use: 4.78680181503s
step 9070/400000: train loss: 0.157180
epoch:197, batch10, load frames use: 5.0251660347s
step 9071/400000: train loss: 0.140883
epoch:197, batch11, load frames use: 5.42191195488s
step 9072/400000: train loss: 0.146413
epoch:197, batch12, load frames use: 4.27962803841s
step 9073/400000: train loss: 0.149266
epoch:197, batch13, load frames use: 4.48022294044s
step 9074/400000: train loss: 0.150687
epoch:197, batch14, load frames use: 4.74268198013s
step 9075/400000: train loss: 0.154311
epoch:197, batch15, load frames use: 4.67197990417s
step 9076/400000: train loss: 0.129194
epoch:197, batch16, load frames use: 4.97411990166s
step 9077/400000: train loss: 0.147480
epoch:197, batch17, load frames use: 5.04241800308s
step 9078/400000: train loss: 0.165748
epoch:197, batch18, load frames use: 4.24271202087s
step 9079/400000: train loss: 0.153596
epoch:197, batch19, load frames use: 4.61039400101s
step 9080/400000: train loss: 0.163844
epoch:197, batch20, load frames use: 4.39989113808s
step 9081/400000: train loss: 0.132916
epoch:197, batch21, load frames use: 4.13288092613s
step 9082/400000: train loss: 0.166374
epoch:197, batch22, load frames use: 4.10976004601s
step 9083/400000: train loss: 0.161815
epoch:197, batch23, load frames use: 5.35883498192s
step 9084/400000: train loss: 0.158942
epoch:197, batch24, load frames use: 4.93920207024s
step 9085/400000: train loss: 0.163102
epoch:197, batch25, load frames use: 4.82483506203s
step 9086/400000: train loss: 0.156935
epoch:197, batch26, load frames use: 5.46765398979s
step 9087/400000: train loss: 0.160529
epoch:197, batch27, load frames use: 4.09635400772s
step 9088/400000: train loss: 0.152156
epoch:197, batch28, load frames use: 5.34209799767s
step 9089/400000: train loss: 0.153501
epoch:197, batch29, load frames use: 4.02322697639s
step 9090/400000: train loss: 0.149326
epoch:197, batch30, load frames use: 4.50620388985s
step 9091/400000: train loss: 0.145521
epoch:197, batch31, load frames use: 4.3344399929s
step 9092/400000: train loss: 0.169351
epoch:197, batch32, load frames use: 5.29473805428s
step 9093/400000: train loss: 0.140491
epoch:197, batch33, load frames use: 4.72585105896s
step 9094/400000: train loss: 0.151909
epoch:197, batch34, load frames use: 4.76223897934s
step 9095/400000: train loss: 0.164677
epoch:197, batch35, load frames use: 4.94781017303s
step 9096/400000: train loss: 0.167428
epoch:197, batch36, load frames use: 5.29531216621s
step 9097/400000: train loss: 0.166732
epoch:197, batch37, load frames use: 5.04588007927s
step 9098/400000: train loss: 0.152650
epoch:197, batch38, load frames use: 4.70808792114s
step 9099/400000: train loss: 0.155903
epoch:197, batch39, load frames use: 5.54371500015s
step 9100/400000: train loss: 0.142681
step 9100/400000: validation loss: 0.139638
epoch:197, batch40, load frames use: 4.57536101341s
step 9101/400000: train loss: 0.156606
epoch:197, batch41, load frames use: 5.25355911255s
step 9102/400000: train loss: 0.154204
epoch:197, batch42, load frames use: 5.72418308258s
step 9103/400000: train loss: 0.159254
epoch:197, batch43, load frames use: 4.22031211853s
step 9104/400000: train loss: 0.168804
epoch:197, batch44, load frames use: 4.31653499603s
step 9105/400000: train loss: 0.142211
epoch:197, batch45, load frames use: 5.18999814987s
step 9106/400000: train loss: 0.168431
epoch:197, batch46, load frames use: 4.38877606392s
step 9107/400000: train loss: 0.156811
epoch:198, batch1, load frames use: 4.60760903358s
step 9108/400000: train loss: 0.167614
epoch:198, batch2, load frames use: 5.42121601105s
step 9109/400000: train loss: 0.149886
epoch:198, batch3, load frames use: 5.86927008629s
step 9110/400000: train loss: 0.148495
epoch:198, batch4, load frames use: 4.99202394485s
step 9111/400000: train loss: 0.137868
epoch:198, batch5, load frames use: 4.62122488022s
step 9112/400000: train loss: 0.144789
epoch:198, batch6, load frames use: 5.28360104561s
step 9113/400000: train loss: 0.134344
epoch:198, batch7, load frames use: 4.54465889931s
step 9114/400000: train loss: 0.148009
epoch:198, batch8, load frames use: 5.22590112686s
step 9115/400000: train loss: 0.149330
epoch:198, batch9, load frames use: 5.33918404579s
step 9116/400000: train loss: 0.147338
epoch:198, batch10, load frames use: 5.37798285484s
step 9117/400000: train loss: 0.156535
epoch:198, batch11, load frames use: 5.03259301186s
step 9118/400000: train loss: 0.154719
epoch:198, batch12, load frames use: 5.35737800598s
step 9119/400000: train loss: 0.163909
epoch:198, batch13, load frames use: 5.29035902023s
step 9120/400000: train loss: 0.153700
epoch:198, batch14, load frames use: 5.62572693825s
step 9121/400000: train loss: 0.165322
epoch:198, batch15, load frames use: 5.91234898567s
step 9122/400000: train loss: 0.138001
epoch:198, batch16, load frames use: 5.24278879166s
step 9123/400000: train loss: 0.151171
epoch:198, batch17, load frames use: 5.54192304611s
step 9124/400000: train loss: 0.140004
epoch:198, batch18, load frames use: 4.84442901611s
step 9125/400000: train loss: 0.157759
epoch:198, batch19, load frames use: 5.20272898674s
step 9126/400000: train loss: 0.139191
epoch:198, batch20, load frames use: 5.32479715347s
step 9127/400000: train loss: 0.149143
epoch:198, batch21, load frames use: 4.79074287415s
step 9128/400000: train loss: 0.139196
epoch:198, batch22, load frames use: 5.70329403877s
step 9129/400000: train loss: 0.143877
epoch:198, batch23, load frames use: 4.94124507904s
step 9130/400000: train loss: 0.143949
epoch:198, batch24, load frames use: 5.05648183823s
step 9131/400000: train loss: 0.151392
epoch:198, batch25, load frames use: 9.58310294151s
step 9132/400000: train loss: 0.152090
epoch:198, batch26, load frames use: 4.84714698792s
step 9133/400000: train loss: 0.135815
epoch:198, batch27, load frames use: 4.9761030674s
step 9134/400000: train loss: 0.158377
epoch:198, batch28, load frames use: 5.36671090126s
step 9135/400000: train loss: 0.162944
epoch:198, batch29, load frames use: 5.22085905075s
step 9136/400000: train loss: 0.151123
epoch:198, batch30, load frames use: 4.78637909889s
step 9137/400000: train loss: 0.152417
epoch:198, batch31, load frames use: 5.38825798035s
step 9138/400000: train loss: 0.169897
epoch:198, batch32, load frames use: 4.80717992783s
step 9139/400000: train loss: 0.169239
epoch:198, batch33, load frames use: 4.92098689079s
step 9140/400000: train loss: 0.148367
epoch:198, batch34, load frames use: 4.60137486458s
step 9141/400000: train loss: 0.152226
epoch:198, batch35, load frames use: 5.24841809273s
step 9142/400000: train loss: 0.137273
epoch:198, batch36, load frames use: 5.28192687035s
step 9143/400000: train loss: 0.163427
epoch:198, batch37, load frames use: 5.28628396988s
step 9144/400000: train loss: 0.148243
epoch:198, batch38, load frames use: 4.97827887535s
step 9145/400000: train loss: 0.163724
epoch:198, batch39, load frames use: 4.72222399712s
step 9146/400000: train loss: 0.164971
epoch:198, batch40, load frames use: 5.27071809769s
step 9147/400000: train loss: 0.163334
epoch:198, batch41, load frames use: 4.94726610184s
step 9148/400000: train loss: 0.179665
epoch:198, batch42, load frames use: 5.2240819931s
step 9149/400000: train loss: 0.155069
epoch:198, batch43, load frames use: 4.59285783768s
step 9150/400000: train loss: 0.154213
epoch:198, batch44, load frames use: 5.21974301338s
step 9151/400000: train loss: 0.144436
epoch:198, batch45, load frames use: 4.35733580589s
step 9152/400000: train loss: 0.142809
epoch:198, batch46, load frames use: 5.26325893402s
step 9153/400000: train loss: 0.163967
epoch:199, batch1, load frames use: 5.32865309715s
step 9154/400000: train loss: 0.161619
epoch:199, batch2, load frames use: 5.4117231369s
step 9155/400000: train loss: 0.132946
epoch:199, batch3, load frames use: 5.30338096619s
step 9156/400000: train loss: 0.143620
epoch:199, batch4, load frames use: 4.94503116608s
step 9157/400000: train loss: 0.134461
epoch:199, batch5, load frames use: 4.99805402756s
step 9158/400000: train loss: 0.118147
epoch:199, batch6, load frames use: 4.59386610985s
step 9159/400000: train loss: 0.141282
epoch:199, batch7, load frames use: 4.78180503845s
step 9160/400000: train loss: 0.143955
epoch:199, batch8, load frames use: 4.71066594124s
step 9161/400000: train loss: 0.159145
epoch:199, batch9, load frames use: 5.19006109238s
step 9162/400000: train loss: 0.134284
epoch:199, batch10, load frames use: 5.06512594223s
step 9163/400000: train loss: 0.164417
epoch:199, batch11, load frames use: 5.39464402199s
step 9164/400000: train loss: 0.142058
epoch:199, batch12, load frames use: 4.9002430439s
step 9165/400000: train loss: 0.148231
epoch:199, batch13, load frames use: 4.66074299812s
step 9166/400000: train loss: 0.139009
epoch:199, batch14, load frames use: 5.10165190697s
step 9167/400000: train loss: 0.134044
epoch:199, batch15, load frames use: 4.94692492485s
step 9168/400000: train loss: 0.122548
epoch:199, batch16, load frames use: 4.75443410873s
step 9169/400000: train loss: 0.150230
epoch:199, batch17, load frames use: 5.01393699646s
step 9170/400000: train loss: 0.156138
epoch:199, batch18, load frames use: 5.37203717232s
step 9171/400000: train loss: 0.144466
epoch:199, batch19, load frames use: 4.95235705376s
step 9172/400000: train loss: 0.146782
epoch:199, batch20, load frames use: 4.98169183731s
step 9173/400000: train loss: 0.151118
epoch:199, batch21, load frames use: 5.27271008492s
step 9174/400000: train loss: 0.150460
epoch:199, batch22, load frames use: 4.49963903427s
step 9175/400000: train loss: 0.151009
epoch:199, batch23, load frames use: 5.08210706711s
step 9176/400000: train loss: 0.142015
epoch:199, batch24, load frames use: 4.83081412315s
step 9177/400000: train loss: 0.148364
epoch:199, batch25, load frames use: 5.16870617867s
step 9178/400000: train loss: 0.148626
epoch:199, batch26, load frames use: 4.8707318306s
step 9179/400000: train loss: 0.143010
epoch:199, batch27, load frames use: 4.89604592323s
step 9180/400000: train loss: 0.139766
epoch:199, batch28, load frames use: 4.78897094727s
step 9181/400000: train loss: 0.132631
epoch:199, batch29, load frames use: 4.98678398132s
step 9182/400000: train loss: 0.156856
epoch:199, batch30, load frames use: 4.83814406395s
step 9183/400000: train loss: 0.160103
epoch:199, batch31, load frames use: 4.74261808395s
step 9184/400000: train loss: 0.149771
epoch:199, batch32, load frames use: 5.07899308205s
step 9185/400000: train loss: 0.131171
epoch:199, batch33, load frames use: 5.21657204628s
step 9186/400000: train loss: 0.146324
epoch:199, batch34, load frames use: 5.00170397758s
step 9187/400000: train loss: 0.148845
epoch:199, batch35, load frames use: 5.77064204216s
step 9188/400000: train loss: 0.166701
epoch:199, batch36, load frames use: 4.45903706551s
step 9189/400000: train loss: 0.141497
epoch:199, batch37, load frames use: 4.84045410156s
step 9190/400000: train loss: 0.150906
epoch:199, batch38, load frames use: 5.25660586357s
step 9191/400000: train loss: 0.152103
epoch:199, batch39, load frames use: 5.43791413307s
step 9192/400000: train loss: 0.134814
epoch:199, batch40, load frames use: 5.02240800858s
step 9193/400000: train loss: 0.159507
epoch:199, batch41, load frames use: 4.5095539093s
step 9194/400000: train loss: 0.154790
epoch:199, batch42, load frames use: 5.37503695488s
step 9195/400000: train loss: 0.159256
epoch:199, batch43, load frames use: 6.81043195724s
step 9196/400000: train loss: 0.153288
epoch:199, batch44, load frames use: 6.43042588234s
step 9197/400000: train loss: 0.149274
epoch:199, batch45, load frames use: 5.49787092209s
step 9198/400000: train loss: 0.147811
epoch:199, batch46, load frames use: 5.99067592621s
step 9199/400000: train loss: 0.159129
epoch:200, batch1, load frames use: 5.90828108788s
step 9200/400000: train loss: 0.134109
step 9200/400000: validation loss: 0.132915
epoch:200, batch2, load frames use: 6.54127812386s
step 9201/400000: train loss: 0.151852
epoch:200, batch3, load frames use: 4.90036797523s
step 9202/400000: train loss: 0.152230
epoch:200, batch4, load frames use: 4.20186400414s
step 9203/400000: train loss: 0.166200
epoch:200, batch5, load frames use: 4.47150397301s
step 9204/400000: train loss: 0.137087
epoch:200, batch6, load frames use: 4.45663785934s
step 9205/400000: train loss: 0.149261
epoch:200, batch7, load frames use: 5.04256701469s
step 9206/400000: train loss: 0.159406
epoch:200, batch8, load frames use: 5.03530097008s
step 9207/400000: train loss: 0.162301
epoch:200, batch9, load frames use: 5.54190993309s
step 9208/400000: train loss: 0.148172
epoch:200, batch10, load frames use: 4.94895911217s
step 9209/400000: train loss: 0.135604
epoch:200, batch11, load frames use: 4.56782793999s
step 9210/400000: train loss: 0.155340
epoch:200, batch12, load frames use: 4.73462891579s
step 9211/400000: train loss: 0.148091
epoch:200, batch13, load frames use: 4.982899189s
step 9212/400000: train loss: 0.146946
epoch:200, batch14, load frames use: 4.98184204102s
step 9213/400000: train loss: 0.118800
epoch:200, batch15, load frames use: 4.88539385796s
step 9214/400000: train loss: 0.128900
epoch:200, batch16, load frames use: 9.24891114235s
step 9215/400000: train loss: 0.148752
epoch:200, batch17, load frames use: 4.25141215324s
step 9216/400000: train loss: 0.171704
epoch:200, batch18, load frames use: 4.6582698822s
step 9217/400000: train loss: 0.157119
epoch:200, batch19, load frames use: 4.64392399788s
step 9218/400000: train loss: 0.161655
epoch:200, batch20, load frames use: 4.83802986145s
step 9219/400000: train loss: 0.147726
epoch:200, batch21, load frames use: 4.77138710022s
step 9220/400000: train loss: 0.147467
epoch:200, batch22, load frames use: 4.90338897705s
step 9221/400000: train loss: 0.145928
epoch:200, batch23, load frames use: 5.27492809296s
step 9222/400000: train loss: 0.140105
epoch:200, batch24, load frames use: 5.02858614922s
step 9223/400000: train loss: 0.144025
epoch:200, batch25, load frames use: 4.93278193474s
step 9224/400000: train loss: 0.144237
epoch:200, batch26, load frames use: 4.9485449791s
step 9225/400000: train loss: 0.151930
epoch:200, batch27, load frames use: 5.12650990486s
step 9226/400000: train loss: 0.169257
epoch:200, batch28, load frames use: 4.5220990181s
step 9227/400000: train loss: 0.173085
epoch:200, batch29, load frames use: 5.50724291801s
step 9228/400000: train loss: 0.162734
epoch:200, batch30, load frames use: 4.66686892509s
step 9229/400000: train loss: 0.157738
epoch:200, batch31, load frames use: 4.75065684319s
step 9230/400000: train loss: 0.146638
epoch:200, batch32, load frames use: 4.77063298225s
step 9231/400000: train loss: 0.160425
epoch:200, batch33, load frames use: 5.73747110367s
step 9232/400000: train loss: 0.144761
epoch:200, batch34, load frames use: 4.94000291824s
step 9233/400000: train loss: 0.150401
epoch:200, batch35, load frames use: 4.53268718719s
step 9234/400000: train loss: 0.143526
epoch:200, batch36, load frames use: 4.90312409401s
step 9235/400000: train loss: 0.160031
epoch:200, batch37, load frames use: 4.66904902458s
step 9236/400000: train loss: 0.155151
epoch:200, batch38, load frames use: 3.80322599411s
step 9237/400000: train loss: 0.136855
epoch:200, batch39, load frames use: 4.41135001183s
step 9238/400000: train loss: 0.139124
epoch:200, batch40, load frames use: 4.64737892151s
step 9239/400000: train loss: 0.138350
epoch:200, batch41, load frames use: 5.1588768959s
step 9240/400000: train loss: 0.154003
epoch:200, batch42, load frames use: 4.99107098579s
step 9241/400000: train loss: 0.153665
epoch:200, batch43, load frames use: 4.79701709747s
step 9242/400000: train loss: 0.149899
epoch:200, batch44, load frames use: 5.48398780823s
step 9243/400000: train loss: 0.166799
epoch:200, batch45, load frames use: 5.36883211136s
step 9244/400000: train loss: 0.140084
epoch:200, batch46, load frames use: 4.49380707741s
step 9245/400000: train loss: 0.165647
epoch:201, batch1, load frames use: 4.45160889626s
step 9246/400000: train loss: 0.156480
epoch:201, batch2, load frames use: 4.94352293015s
step 9247/400000: train loss: 0.166244
epoch:201, batch3, load frames use: 5.39683914185s
step 9248/400000: train loss: 0.149010
epoch:201, batch4, load frames use: 4.82259392738s
step 9249/400000: train loss: 0.155472
epoch:201, batch5, load frames use: 4.39887404442s
step 9250/400000: train loss: 0.113878
epoch:201, batch6, load frames use: 4.64111399651s
step 9251/400000: train loss: 0.128429
epoch:201, batch7, load frames use: 5.30156087875s
step 9252/400000: train loss: 0.144438
epoch:201, batch8, load frames use: 4.54740786552s
step 9253/400000: train loss: 0.151459
epoch:201, batch9, load frames use: 4.82922101021s
step 9254/400000: train loss: 0.164229
epoch:201, batch10, load frames use: 4.86304807663s
step 9255/400000: train loss: 0.142888
epoch:201, batch11, load frames use: 4.25092196465s
step 9256/400000: train loss: 0.142854
epoch:201, batch12, load frames use: 4.75944590569s
step 9257/400000: train loss: 0.168512
epoch:201, batch13, load frames use: 4.3782479763s
step 9258/400000: train loss: 0.163995
epoch:201, batch14, load frames use: 4.68754196167s
step 9259/400000: train loss: 0.128337
epoch:201, batch15, load frames use: 4.88724517822s
step 9260/400000: train loss: 0.146079
epoch:201, batch16, load frames use: 4.81459403038s
step 9261/400000: train loss: 0.131320
epoch:201, batch17, load frames use: 4.40138483047s
step 9262/400000: train loss: 0.157858
epoch:201, batch18, load frames use: 4.44840407372s
step 9263/400000: train loss: 0.153645
epoch:201, batch19, load frames use: 4.57337403297s
step 9264/400000: train loss: 0.156557
epoch:201, batch20, load frames use: 4.63325691223s
step 9265/400000: train loss: 0.152957
epoch:201, batch21, load frames use: 4.2796998024s
step 9266/400000: train loss: 0.160935
epoch:201, batch22, load frames use: 4.38263702393s
step 9267/400000: train loss: 0.148863
epoch:201, batch23, load frames use: 5.44032788277s
step 9268/400000: train loss: 0.140427
epoch:201, batch24, load frames use: 4.57511687279s
step 9269/400000: train loss: 0.135943
epoch:201, batch25, load frames use: 4.88485598564s
step 9270/400000: train loss: 0.148111
epoch:201, batch26, load frames use: 4.69417405128s
step 9271/400000: train loss: 0.163010
epoch:201, batch27, load frames use: 5.06415605545s
step 9272/400000: train loss: 0.155973
epoch:201, batch28, load frames use: 4.76823496819s
step 9273/400000: train loss: 0.152625
epoch:201, batch29, load frames use: 4.76546788216s
step 9274/400000: train loss: 0.162814
epoch:201, batch30, load frames use: 4.28415584564s
step 9275/400000: train loss: 0.157541
epoch:201, batch31, load frames use: 4.82352113724s
step 9276/400000: train loss: 0.151761
epoch:201, batch32, load frames use: 4.75870895386s
step 9277/400000: train loss: 0.167817
epoch:201, batch33, load frames use: 4.55042505264s
step 9278/400000: train loss: 0.161982
epoch:201, batch34, load frames use: 4.63292503357s
step 9279/400000: train loss: 0.169691
epoch:201, batch35, load frames use: 4.77561688423s
step 9280/400000: train loss: 0.145211
epoch:201, batch36, load frames use: 5.00323915482s
step 9281/400000: train loss: 0.144499
epoch:201, batch37, load frames use: 4.54724311829s
step 9282/400000: train loss: 0.141589
epoch:201, batch38, load frames use: 4.77339410782s
step 9283/400000: train loss: 0.155917
epoch:201, batch39, load frames use: 4.81500411034s
step 9284/400000: train loss: 0.155580
epoch:201, batch40, load frames use: 4.66262698174s
step 9285/400000: train loss: 0.145887
epoch:201, batch41, load frames use: 4.85657405853s
step 9286/400000: train loss: 0.156448
epoch:201, batch42, load frames use: 5.13178920746s
step 9287/400000: train loss: 0.153275
epoch:201, batch43, load frames use: 4.52424001694s
step 9288/400000: train loss: 0.151315
epoch:201, batch44, load frames use: 4.47877287865s
step 9289/400000: train loss: 0.149051
epoch:201, batch45, load frames use: 4.84607887268s
step 9290/400000: train loss: 0.155534
epoch:201, batch46, load frames use: 5.24438095093s
step 9291/400000: train loss: 0.131649
epoch:202, batch1, load frames use: 4.96157884598s
step 9292/400000: train loss: 0.131149
epoch:202, batch2, load frames use: 4.87435793877s
step 9293/400000: train loss: 0.149901
epoch:202, batch3, load frames use: 4.77245521545s
step 9294/400000: train loss: 0.144253
epoch:202, batch4, load frames use: 4.79565405846s
step 9295/400000: train loss: 0.155474
epoch:202, batch5, load frames use: 4.40671610832s
step 9296/400000: train loss: 0.117077
epoch:202, batch6, load frames use: 4.62758493423s
step 9297/400000: train loss: 0.163503
epoch:202, batch7, load frames use: 4.86055707932s
step 9298/400000: train loss: 0.148004
epoch:202, batch8, load frames use: 5.23081207275s
step 9299/400000: train loss: 0.144538
epoch:202, batch9, load frames use: 4.5431330204s
step 9300/400000: train loss: 0.155979
step 9300/400000: validation loss: 0.150185
epoch:202, batch10, load frames use: 5.42697405815s
step 9301/400000: train loss: 0.158654
epoch:202, batch11, load frames use: 5.52820706367s
step 9302/400000: train loss: 0.151595
epoch:202, batch12, load frames use: 4.60530281067s
step 9303/400000: train loss: 0.145637
epoch:202, batch13, load frames use: 4.92301082611s
step 9304/400000: train loss: 0.154727
epoch:202, batch14, load frames use: 4.60866212845s
step 9305/400000: train loss: 0.134013
epoch:202, batch15, load frames use: 4.51992297173s
step 9306/400000: train loss: 0.132720
epoch:202, batch16, load frames use: 5.3236181736s
step 9307/400000: train loss: 0.137928
epoch:202, batch17, load frames use: 4.47069406509s
step 9308/400000: train loss: 0.126716
epoch:202, batch18, load frames use: 4.47855997086s
step 9309/400000: train loss: 0.137114
epoch:202, batch19, load frames use: 4.61071896553s
step 9310/400000: train loss: 0.138734
epoch:202, batch20, load frames use: 5.24512195587s
step 9311/400000: train loss: 0.160816
epoch:202, batch21, load frames use: 4.39195299149s
step 9312/400000: train loss: 0.140653
epoch:202, batch22, load frames use: 4.48131990433s
step 9313/400000: train loss: 0.146572
epoch:202, batch23, load frames use: 4.40959501266s
step 9314/400000: train loss: 0.154154
epoch:202, batch24, load frames use: 4.93325614929s
step 9315/400000: train loss: 0.162559
epoch:202, batch25, load frames use: 4.56396317482s
step 9316/400000: train loss: 0.157328
epoch:202, batch26, load frames use: 4.98851585388s
step 9317/400000: train loss: 0.139613
epoch:202, batch27, load frames use: 4.70917701721s
step 9318/400000: train loss: 0.140892
epoch:202, batch28, load frames use: 4.95489597321s
step 9319/400000: train loss: 0.134529
epoch:202, batch29, load frames use: 4.43711900711s
step 9320/400000: train loss: 0.154372
epoch:202, batch30, load frames use: 4.34751796722s
step 9321/400000: train loss: 0.156852
epoch:202, batch31, load frames use: 4.82190608978s
step 9322/400000: train loss: 0.159240
epoch:202, batch32, load frames use: 4.67424201965s
step 9323/400000: train loss: 0.160941
epoch:202, batch33, load frames use: 4.57596683502s
step 9324/400000: train loss: 0.134489
epoch:202, batch34, load frames use: 5.35625600815s
step 9325/400000: train loss: 0.139365
epoch:202, batch35, load frames use: 5.35367202759s
step 9326/400000: train loss: 0.154993
epoch:202, batch36, load frames use: 4.89753007889s
step 9327/400000: train loss: 0.160127
epoch:202, batch37, load frames use: 4.67984986305s
step 9328/400000: train loss: 0.149525
epoch:202, batch38, load frames use: 5.46300411224s
step 9329/400000: train loss: 0.158683
epoch:202, batch39, load frames use: 4.89046406746s
step 9330/400000: train loss: 0.158154
epoch:202, batch40, load frames use: 4.48029589653s
step 9331/400000: train loss: 0.165221
epoch:202, batch41, load frames use: 4.88858509064s
step 9332/400000: train loss: 0.142507
epoch:202, batch42, load frames use: 4.68814086914s
step 9333/400000: train loss: 0.137246
epoch:202, batch43, load frames use: 4.75682997704s
step 9334/400000: train loss: 0.151116
epoch:202, batch44, load frames use: 4.51325297356s
step 9335/400000: train loss: 0.143385
epoch:202, batch45, load frames use: 4.53429913521s
step 9336/400000: train loss: 0.162855
epoch:202, batch46, load frames use: 5.03210401535s
step 9337/400000: train loss: 0.148940
epoch:203, batch1, load frames use: 5.21837210655s
step 9338/400000: train loss: 0.161827
epoch:203, batch2, load frames use: 4.65469002724s
step 9339/400000: train loss: 0.162356
epoch:203, batch3, load frames use: 4.53025889397s
step 9340/400000: train loss: 0.156689
epoch:203, batch4, load frames use: 4.82445502281s
step 9341/400000: train loss: 0.150025
epoch:203, batch5, load frames use: 4.49246811867s
step 9342/400000: train loss: 0.138064
epoch:203, batch6, load frames use: 4.81185388565s
step 9343/400000: train loss: 0.139545
epoch:203, batch7, load frames use: 5.11606287956s
step 9344/400000: train loss: 0.153721
epoch:203, batch8, load frames use: 4.62881994247s
step 9345/400000: train loss: 0.146101
epoch:203, batch9, load frames use: 4.70756697655s
step 9346/400000: train loss: 0.157937
epoch:203, batch10, load frames use: 4.69691705704s
step 9347/400000: train loss: 0.142627
epoch:203, batch11, load frames use: 4.12798595428s
step 9348/400000: train loss: 0.132951
epoch:203, batch12, load frames use: 4.84171390533s
step 9349/400000: train loss: 0.148810
epoch:203, batch13, load frames use: 4.91511702538s
step 9350/400000: train loss: 0.138466
epoch:203, batch14, load frames use: 5.40472698212s
step 9351/400000: train loss: 0.144736
epoch:203, batch15, load frames use: 5.14353203773s
step 9352/400000: train loss: 0.147334
epoch:203, batch16, load frames use: 4.51812386513s
step 9353/400000: train loss: 0.154658
epoch:203, batch17, load frames use: 4.95032787323s
step 9354/400000: train loss: 0.157694
epoch:203, batch18, load frames use: 4.74397706985s
step 9355/400000: train loss: 0.143527
epoch:203, batch19, load frames use: 5.70353102684s
step 9356/400000: train loss: 0.131768
epoch:203, batch20, load frames use: 4.63196611404s
step 9357/400000: train loss: 0.142856
epoch:203, batch21, load frames use: 4.87922096252s
step 9358/400000: train loss: 0.146381
epoch:203, batch22, load frames use: 4.8826611042s
step 9359/400000: train loss: 0.157178
epoch:203, batch23, load frames use: 4.86349511147s
step 9360/400000: train loss: 0.144466
epoch:203, batch24, load frames use: 4.56290602684s
step 9361/400000: train loss: 0.160349
epoch:203, batch25, load frames use: 4.79966211319s
step 9362/400000: train loss: 0.164403
epoch:203, batch26, load frames use: 4.47059893608s
step 9363/400000: train loss: 0.138802
epoch:203, batch27, load frames use: 4.65672016144s
step 9364/400000: train loss: 0.141378
epoch:203, batch28, load frames use: 4.28940200806s
step 9365/400000: train loss: 0.133698
epoch:203, batch29, load frames use: 4.86035609245s
step 9366/400000: train loss: 0.143749
epoch:203, batch30, load frames use: 4.77888894081s
step 9367/400000: train loss: 0.145183
epoch:203, batch31, load frames use: 5.28786802292s
step 9368/400000: train loss: 0.163460
epoch:203, batch32, load frames use: 4.55084395409s
step 9369/400000: train loss: 0.144395
epoch:203, batch33, load frames use: 5.05469894409s
step 9370/400000: train loss: 0.143039
epoch:203, batch34, load frames use: 5.10926580429s
step 9371/400000: train loss: 0.163922
epoch:203, batch35, load frames use: 4.39796805382s
step 9372/400000: train loss: 0.165015
epoch:203, batch36, load frames use: 4.7534070015s
step 9373/400000: train loss: 0.160880
epoch:203, batch37, load frames use: 4.57673788071s
step 9374/400000: train loss: 0.145374
epoch:203, batch38, load frames use: 4.57510209084s
step 9375/400000: train loss: 0.153803
epoch:203, batch39, load frames use: 5.17387223244s
step 9376/400000: train loss: 0.144294
epoch:203, batch40, load frames use: 5.04422497749s
step 9377/400000: train loss: 0.150033
epoch:203, batch41, load frames use: 5.09738206863s
step 9378/400000: train loss: 0.146527
epoch:203, batch42, load frames use: 4.68298316002s
step 9379/400000: train loss: 0.156253
epoch:203, batch43, load frames use: 4.66242098808s
step 9380/400000: train loss: 0.152995
epoch:203, batch44, load frames use: 5.08591198921s
step 9381/400000: train loss: 0.182195
epoch:203, batch45, load frames use: 4.740213871s
step 9382/400000: train loss: 0.135921
epoch:203, batch46, load frames use: 4.8474240303s
step 9383/400000: train loss: 0.140513
epoch:204, batch1, load frames use: 4.83545994759s
step 9384/400000: train loss: 0.152543
epoch:204, batch2, load frames use: 4.9232571125s
step 9385/400000: train loss: 0.161779
epoch:204, batch3, load frames use: 4.58401799202s
step 9386/400000: train loss: 0.121708
epoch:204, batch4, load frames use: 4.52197003365s
step 9387/400000: train loss: 0.159892
epoch:204, batch5, load frames use: 4.96125507355s
step 9388/400000: train loss: 0.142332
epoch:204, batch6, load frames use: 5.00937199593s
step 9389/400000: train loss: 0.164140
epoch:204, batch7, load frames use: 4.50062799454s
step 9390/400000: train loss: 0.141663
epoch:204, batch8, load frames use: 4.57249712944s
step 9391/400000: train loss: 0.134059
epoch:204, batch9, load frames use: 4.90889906883s
step 9392/400000: train loss: 0.154708
epoch:204, batch10, load frames use: 4.76403403282s
step 9393/400000: train loss: 0.150626
epoch:204, batch11, load frames use: 4.61707901955s
step 9394/400000: train loss: 0.143580
epoch:204, batch12, load frames use: 4.80930399895s
step 9395/400000: train loss: 0.147946
epoch:204, batch13, load frames use: 4.07322597504s
step 9396/400000: train loss: 0.145738
epoch:204, batch14, load frames use: 5.3966190815s
step 9397/400000: train loss: 0.149136
epoch:204, batch15, load frames use: 5.04909586906s
step 9398/400000: train loss: 0.143538
epoch:204, batch16, load frames use: 4.37094187737s
step 9399/400000: train loss: 0.143882
epoch:204, batch17, load frames use: 4.91148495674s
step 9400/400000: train loss: 0.141708
step 9400/400000: validation loss: 0.141806
epoch:204, batch18, load frames use: 5.11205220222s
step 9401/400000: train loss: 0.144987
epoch:204, batch19, load frames use: 4.96091103554s
step 9402/400000: train loss: 0.127917
epoch:204, batch20, load frames use: 4.77877688408s
step 9403/400000: train loss: 0.137747
epoch:204, batch21, load frames use: 4.8106238842s
step 9404/400000: train loss: 0.154089
epoch:204, batch22, load frames use: 4.92391610146s
step 9405/400000: train loss: 0.154490
epoch:204, batch23, load frames use: 4.63456583023s
step 9406/400000: train loss: 0.159100
epoch:204, batch24, load frames use: 4.44124007225s
step 9407/400000: train loss: 0.168312
epoch:204, batch25, load frames use: 4.52181696892s
step 9408/400000: train loss: 0.133656
epoch:204, batch26, load frames use: 4.87499189377s
step 9409/400000: train loss: 0.137828
epoch:204, batch27, load frames use: 4.92016983032s
step 9410/400000: train loss: 0.141858
epoch:204, batch28, load frames use: 4.48887991905s
step 9411/400000: train loss: 0.138722
epoch:204, batch29, load frames use: 4.96663188934s
step 9412/400000: train loss: 0.142973
epoch:204, batch30, load frames use: 5.34095001221s
step 9413/400000: train loss: 0.153461
epoch:204, batch31, load frames use: 4.92484092712s
step 9414/400000: train loss: 0.151207
epoch:204, batch32, load frames use: 4.91644406319s
step 9415/400000: train loss: 0.140355
epoch:204, batch33, load frames use: 4.6690530777s
step 9416/400000: train loss: 0.157415
epoch:204, batch34, load frames use: 4.60649299622s
step 9417/400000: train loss: 0.150357
epoch:204, batch35, load frames use: 4.58459997177s
step 9418/400000: train loss: 0.136911
epoch:204, batch36, load frames use: 5.05142807961s
step 9419/400000: train loss: 0.160304
epoch:204, batch37, load frames use: 4.7669968605s
step 9420/400000: train loss: 0.148151
epoch:204, batch38, load frames use: 4.82897186279s
step 9421/400000: train loss: 0.161931
epoch:204, batch39, load frames use: 3.90948796272s
step 9422/400000: train loss: 0.139132
epoch:204, batch40, load frames use: 4.28113698959s
step 9423/400000: train loss: 0.142779
epoch:204, batch41, load frames use: 4.74996113777s
step 9424/400000: train loss: 0.146216
epoch:204, batch42, load frames use: 4.47537088394s
step 9425/400000: train loss: 0.147572
epoch:204, batch43, load frames use: 4.10333514214s
step 9426/400000: train loss: 0.140447
epoch:204, batch44, load frames use: 4.47773504257s
step 9427/400000: train loss: 0.159260
epoch:204, batch45, load frames use: 4.44960403442s
step 9428/400000: train loss: 0.152606
epoch:204, batch46, load frames use: 4.39849615097s
step 9429/400000: train loss: 0.143390
epoch:205, batch1, load frames use: 4.96412801743s
step 9430/400000: train loss: 0.146945
epoch:205, batch2, load frames use: 4.74339008331s
step 9431/400000: train loss: 0.149642
epoch:205, batch3, load frames use: 4.64869713783s
step 9432/400000: train loss: 0.142431
epoch:205, batch4, load frames use: 4.75987100601s
step 9433/400000: train loss: 0.146655
epoch:205, batch5, load frames use: 4.1424779892s
step 9434/400000: train loss: 0.146659
epoch:205, batch6, load frames use: 4.08441400528s
step 9435/400000: train loss: 0.151179
epoch:205, batch7, load frames use: 4.74856805801s
step 9436/400000: train loss: 0.136666
epoch:205, batch8, load frames use: 4.68708109856s
step 9437/400000: train loss: 0.138528
epoch:205, batch9, load frames use: 5.26664614677s
step 9438/400000: train loss: 0.140228
epoch:205, batch10, load frames use: 5.09109401703s
step 9439/400000: train loss: 0.158652
epoch:205, batch11, load frames use: 5.02617001534s
step 9440/400000: train loss: 0.150392
epoch:205, batch12, load frames use: 5.41644787788s
step 9441/400000: train loss: 0.143418
epoch:205, batch13, load frames use: 5.12053394318s
step 9442/400000: train loss: 0.163262
epoch:205, batch14, load frames use: 5.05069303513s
step 9443/400000: train loss: 0.132062
epoch:205, batch15, load frames use: 5.52856397629s
step 9444/400000: train loss: 0.156202
epoch:205, batch16, load frames use: 5.12163496017s
step 9445/400000: train loss: 0.140169
epoch:205, batch17, load frames use: 4.62079596519s
step 9446/400000: train loss: 0.162143
epoch:205, batch18, load frames use: 4.89662003517s
step 9447/400000: train loss: 0.139204
epoch:205, batch19, load frames use: 5.25485706329s
step 9448/400000: train loss: 0.142482
epoch:205, batch20, load frames use: 4.55183911324s
step 9449/400000: train loss: 0.149288
epoch:205, batch21, load frames use: 3.90600895882s
step 9450/400000: train loss: 0.153059
epoch:205, batch22, load frames use: 5.32738399506s
step 9451/400000: train loss: 0.144297
epoch:205, batch23, load frames use: 5.45706605911s
step 9452/400000: train loss: 0.146241
epoch:205, batch24, load frames use: 5.37429904938s
step 9453/400000: train loss: 0.151897
epoch:205, batch25, load frames use: 5.15890312195s
step 9454/400000: train loss: 0.133073
epoch:205, batch26, load frames use: 4.85823512077s
step 9455/400000: train loss: 0.146155
epoch:205, batch27, load frames use: 4.87415409088s
step 9456/400000: train loss: 0.146044
epoch:205, batch28, load frames use: 4.77534008026s
step 9457/400000: train loss: 0.151169
epoch:205, batch29, load frames use: 4.95740795135s
step 9458/400000: train loss: 0.138166
epoch:205, batch30, load frames use: 4.18509697914s
step 9459/400000: train loss: 0.154120
epoch:205, batch31, load frames use: 5.32284498215s
step 9460/400000: train loss: 0.150193
epoch:205, batch32, load frames use: 4.94503498077s
step 9461/400000: train loss: 0.143501
epoch:205, batch33, load frames use: 4.20574092865s
step 9462/400000: train loss: 0.151624
epoch:205, batch34, load frames use: 5.33670282364s
step 9463/400000: train loss: 0.145164
epoch:205, batch35, load frames use: 5.21705508232s
step 9464/400000: train loss: 0.151236
epoch:205, batch36, load frames use: 5.08208489418s
step 9465/400000: train loss: 0.150921
epoch:205, batch37, load frames use: 4.98348689079s
step 9466/400000: train loss: 0.146681
epoch:205, batch38, load frames use: 4.40630412102s
step 9467/400000: train loss: 0.169582
epoch:205, batch39, load frames use: 5.32465910912s
step 9468/400000: train loss: 0.143309
epoch:205, batch40, load frames use: 5.18091201782s
step 9469/400000: train loss: 0.144137
epoch:205, batch41, load frames use: 4.1983230114s
step 9470/400000: train loss: 0.154761
epoch:205, batch42, load frames use: 6.12329101562s
step 9471/400000: train loss: 0.149489
epoch:205, batch43, load frames use: 4.59221196175s
step 9472/400000: train loss: 0.127024
epoch:205, batch44, load frames use: 5.21191692352s
step 9473/400000: train loss: 0.167380
epoch:205, batch45, load frames use: 5.23596596718s
step 9474/400000: train loss: 0.146349
epoch:205, batch46, load frames use: 4.79405093193s
step 9475/400000: train loss: 0.158422
epoch:206, batch1, load frames use: 3.84662413597s
step 9476/400000: train loss: 0.137940
epoch:206, batch2, load frames use: 4.65228796005s
step 9477/400000: train loss: 0.144040
epoch:206, batch3, load frames use: 4.6548781395s
step 9478/400000: train loss: 0.139588
epoch:206, batch4, load frames use: 4.86832499504s
step 9479/400000: train loss: 0.133006
epoch:206, batch5, load frames use: 5.0452671051s
step 9480/400000: train loss: 0.135201
epoch:206, batch6, load frames use: 4.88401198387s
step 9481/400000: train loss: 0.136618
epoch:206, batch7, load frames use: 5.18970704079s
step 9482/400000: train loss: 0.149139
epoch:206, batch8, load frames use: 5.38076686859s
step 9483/400000: train loss: 0.130913
epoch:206, batch9, load frames use: 5.56634402275s
step 9484/400000: train loss: 0.150994
epoch:206, batch10, load frames use: 4.99333810806s
step 9485/400000: train loss: 0.141016
epoch:206, batch11, load frames use: 6.00972008705s
step 9486/400000: train loss: 0.150549
epoch:206, batch12, load frames use: 5.30834794044s
step 9487/400000: train loss: 0.142923
epoch:206, batch13, load frames use: 5.05481410027s
step 9488/400000: train loss: 0.149515
epoch:206, batch14, load frames use: 4.97010684013s
step 9489/400000: train loss: 0.145544
epoch:206, batch15, load frames use: 5.2599029541s
step 9490/400000: train loss: 0.145250
epoch:206, batch16, load frames use: 5.29069900513s
step 9491/400000: train loss: 0.140833
epoch:206, batch17, load frames use: 5.34578490257s
step 9492/400000: train loss: 0.155633
epoch:206, batch18, load frames use: 4.82996606827s
step 9493/400000: train loss: 0.136895
epoch:206, batch19, load frames use: 4.81336116791s
step 9494/400000: train loss: 0.139785
epoch:206, batch20, load frames use: 5.38370323181s
step 9495/400000: train loss: 0.145335
epoch:206, batch21, load frames use: 5.97317099571s
step 9496/400000: train loss: 0.163752
epoch:206, batch22, load frames use: 5.20411491394s
step 9497/400000: train loss: 0.146528
epoch:206, batch23, load frames use: 4.92702198029s
step 9498/400000: train loss: 0.159336
epoch:206, batch24, load frames use: 5.29463601112s
step 9499/400000: train loss: 0.160695
epoch:206, batch25, load frames use: 4.18908405304s
step 9500/400000: train loss: 0.162454
step 9500/400000: validation loss: 0.148469
epoch:206, batch26, load frames use: 4.83879303932s
step 9501/400000: train loss: 0.140910
epoch:206, batch27, load frames use: 5.10972809792s
step 9502/400000: train loss: 0.155187
epoch:206, batch28, load frames use: 4.57138299942s
step 9503/400000: train loss: 0.151918
epoch:206, batch29, load frames use: 5.48605895042s
step 9504/400000: train loss: 0.163599
epoch:206, batch30, load frames use: 5.40271687508s
step 9505/400000: train loss: 0.148825
epoch:206, batch31, load frames use: 4.9975271225s
step 9506/400000: train loss: 0.153102
epoch:206, batch32, load frames use: 5.45025801659s
step 9507/400000: train loss: 0.148782
epoch:206, batch33, load frames use: 5.18998599052s
step 9508/400000: train loss: 0.140567
epoch:206, batch34, load frames use: 4.79684090614s
step 9509/400000: train loss: 0.143584
epoch:206, batch35, load frames use: 5.08112215996s
step 9510/400000: train loss: 0.166717
epoch:206, batch36, load frames use: 5.39300608635s
step 9511/400000: train loss: 0.154679
epoch:206, batch37, load frames use: 5.56301283836s
step 9512/400000: train loss: 0.151603
epoch:206, batch38, load frames use: 5.14126300812s
step 9513/400000: train loss: 0.150719
epoch:206, batch39, load frames use: 5.21770501137s
step 9514/400000: train loss: 0.150553
epoch:206, batch40, load frames use: 5.20285606384s
step 9515/400000: train loss: 0.159312
epoch:206, batch41, load frames use: 5.44407200813s
step 9516/400000: train loss: 0.128550
epoch:206, batch42, load frames use: 4.75768399239s
step 9517/400000: train loss: 0.161868
epoch:206, batch43, load frames use: 4.51036095619s
step 9518/400000: train loss: 0.166609
epoch:206, batch44, load frames use: 4.84228801727s
step 9519/400000: train loss: 0.154198
epoch:206, batch45, load frames use: 5.16642308235s
step 9520/400000: train loss: 0.145304
epoch:206, batch46, load frames use: 4.49820804596s
step 9521/400000: train loss: 0.137638
epoch:207, batch1, load frames use: 5.14450716972s
step 9522/400000: train loss: 0.130571
epoch:207, batch2, load frames use: 5.26504588127s
step 9523/400000: train loss: 0.144797
epoch:207, batch3, load frames use: 6.67770314217s
step 9524/400000: train loss: 0.132367
epoch:207, batch4, load frames use: 4.77099013329s
step 9525/400000: train loss: 0.140766
epoch:207, batch5, load frames use: 5.34131503105s
step 9526/400000: train loss: 0.121654
epoch:207, batch6, load frames use: 4.83201599121s
step 9527/400000: train loss: 0.145974
epoch:207, batch7, load frames use: 4.46558403969s
step 9528/400000: train loss: 0.158943
epoch:207, batch8, load frames use: 5.60111403465s
step 9529/400000: train loss: 0.140087
epoch:207, batch9, load frames use: 5.14328598976s
step 9530/400000: train loss: 0.143811
epoch:207, batch10, load frames use: 5.02631902695s
step 9531/400000: train loss: 0.157389
epoch:207, batch11, load frames use: 6.32450985909s
step 9532/400000: train loss: 0.143054
epoch:207, batch12, load frames use: 4.19727611542s
step 9533/400000: train loss: 0.148649
epoch:207, batch13, load frames use: 4.44818806648s
step 9534/400000: train loss: 0.140111
epoch:207, batch14, load frames use: 4.79054403305s
step 9535/400000: train loss: 0.152961
epoch:207, batch15, load frames use: 5.57577991486s
step 9536/400000: train loss: 0.140050
epoch:207, batch16, load frames use: 6.08832883835s
step 9537/400000: train loss: 0.134782
epoch:207, batch17, load frames use: 5.52165293694s
step 9538/400000: train loss: 0.154645
epoch:207, batch18, load frames use: 5.20084118843s
step 9539/400000: train loss: 0.157463
epoch:207, batch19, load frames use: 5.21739697456s
step 9540/400000: train loss: 0.164629
epoch:207, batch20, load frames use: 5.63005113602s
step 9541/400000: train loss: 0.154670
epoch:207, batch21, load frames use: 5.11618304253s
step 9542/400000: train loss: 0.140265
epoch:207, batch22, load frames use: 5.4251139164s
step 9543/400000: train loss: 0.138688
epoch:207, batch23, load frames use: 5.18803191185s
step 9544/400000: train loss: 0.148889
epoch:207, batch24, load frames use: 5.1537539959s
step 9545/400000: train loss: 0.159256
epoch:207, batch25, load frames use: 5.14451599121s
step 9546/400000: train loss: 0.151527
epoch:207, batch26, load frames use: 4.97690987587s
step 9547/400000: train loss: 0.143862
epoch:207, batch27, load frames use: 5.05696606636s
step 9548/400000: train loss: 0.145654
epoch:207, batch28, load frames use: 5.64233493805s
step 9549/400000: train loss: 0.141612
epoch:207, batch29, load frames use: 5.76737618446s
step 9550/400000: train loss: 0.150997
epoch:207, batch30, load frames use: 4.33350300789s
step 9551/400000: train loss: 0.154326
epoch:207, batch31, load frames use: 5.26025104523s
step 9552/400000: train loss: 0.130634
epoch:207, batch32, load frames use: 5.01509380341s
step 9553/400000: train loss: 0.145131
epoch:207, batch33, load frames use: 5.78786802292s
step 9554/400000: train loss: 0.136053
epoch:207, batch34, load frames use: 5.13894510269s
step 9555/400000: train loss: 0.144254
epoch:207, batch35, load frames use: 5.28910398483s
step 9556/400000: train loss: 0.138526
epoch:207, batch36, load frames use: 4.973539114s
step 9557/400000: train loss: 0.135354
epoch:207, batch37, load frames use: 17.7893309593s
step 9558/400000: train loss: 0.152709
epoch:207, batch38, load frames use: 4.77826404572s
step 9559/400000: train loss: 0.156697
epoch:207, batch39, load frames use: 5.21238684654s
step 9560/400000: train loss: 0.135771
epoch:207, batch40, load frames use: 4.99983000755s
step 9561/400000: train loss: 0.158817
epoch:207, batch41, load frames use: 4.81608200073s
step 9562/400000: train loss: 0.133026
epoch:207, batch42, load frames use: 5.27341485023s
step 9563/400000: train loss: 0.143838
epoch:207, batch43, load frames use: 5.60934495926s
step 9564/400000: train loss: 0.160355
epoch:207, batch44, load frames use: 4.78201913834s
step 9565/400000: train loss: 0.152325
epoch:207, batch45, load frames use: 5.50476980209s
step 9566/400000: train loss: 0.134173
epoch:207, batch46, load frames use: 5.22789382935s
step 9567/400000: train loss: 0.167602
epoch:208, batch1, load frames use: 5.05093598366s
step 9568/400000: train loss: 0.142339
epoch:208, batch2, load frames use: 5.50009799004s
step 9569/400000: train loss: 0.153630
epoch:208, batch3, load frames use: 5.02874422073s
step 9570/400000: train loss: 0.142272
epoch:208, batch4, load frames use: 4.49873900414s
step 9571/400000: train loss: 0.138973
epoch:208, batch5, load frames use: 5.80278992653s
step 9572/400000: train loss: 0.156494
epoch:208, batch6, load frames use: 5.26918983459s
step 9573/400000: train loss: 0.138157
epoch:208, batch7, load frames use: 5.12655711174s
step 9574/400000: train loss: 0.141422
epoch:208, batch8, load frames use: 4.98497891426s
step 9575/400000: train loss: 0.156955
epoch:208, batch9, load frames use: 5.09050798416s
step 9576/400000: train loss: 0.150698
epoch:208, batch10, load frames use: 4.18938422203s
step 9577/400000: train loss: 0.145201
epoch:208, batch11, load frames use: 5.02103996277s
step 9578/400000: train loss: 0.134205
epoch:208, batch12, load frames use: 5.24133300781s
step 9579/400000: train loss: 0.142759
epoch:208, batch13, load frames use: 5.29460692406s
step 9580/400000: train loss: 0.129973
epoch:208, batch14, load frames use: 5.63968515396s
step 9581/400000: train loss: 0.133038
epoch:208, batch15, load frames use: 5.23057293892s
step 9582/400000: train loss: 0.140291
epoch:208, batch16, load frames use: 5.43295693398s
step 9583/400000: train loss: 0.161781
epoch:208, batch17, load frames use: 5.15261816978s
step 9584/400000: train loss: 0.133131
epoch:208, batch18, load frames use: 4.46227002144s
step 9585/400000: train loss: 0.147730
epoch:208, batch19, load frames use: 4.92452192307s
step 9586/400000: train loss: 0.147768
epoch:208, batch20, load frames use: 4.8563990593s
step 9587/400000: train loss: 0.142242
epoch:208, batch21, load frames use: 4.91450500488s
step 9588/400000: train loss: 0.137244
epoch:208, batch22, load frames use: 5.23501300812s
step 9589/400000: train loss: 0.139518
epoch:208, batch23, load frames use: 5.32833695412s
step 9590/400000: train loss: 0.141257
epoch:208, batch24, load frames use: 5.01629805565s
step 9591/400000: train loss: 0.139580
epoch:208, batch25, load frames use: 5.44571590424s
step 9592/400000: train loss: 0.162089
epoch:208, batch26, load frames use: 4.78533887863s
step 9593/400000: train loss: 0.150627
epoch:208, batch27, load frames use: 4.96345901489s
step 9594/400000: train loss: 0.125567
epoch:208, batch28, load frames use: 5.19545102119s
step 9595/400000: train loss: 0.155315
epoch:208, batch29, load frames use: 5.07653594017s
step 9596/400000: train loss: 0.138892
epoch:208, batch30, load frames use: 4.67306900024s
step 9597/400000: train loss: 0.141581
epoch:208, batch31, load frames use: 5.21819090843s
step 9598/400000: train loss: 0.145370
epoch:208, batch32, load frames use: 4.60197496414s
step 9599/400000: train loss: 0.124395
epoch:208, batch33, load frames use: 5.32994508743s
step 9600/400000: train loss: 0.142697
step 9600/400000: validation loss: 0.143667
epoch:208, batch34, load frames use: 4.67211890221s
step 9601/400000: train loss: 0.150354
epoch:208, batch35, load frames use: 4.15965986252s
step 9602/400000: train loss: 0.141949
epoch:208, batch36, load frames use: 5.55030107498s
step 9603/400000: train loss: 0.138165
epoch:208, batch37, load frames use: 5.55145597458s
step 9604/400000: train loss: 0.133300
epoch:208, batch38, load frames use: 5.3667280674s
step 9605/400000: train loss: 0.146454
epoch:208, batch39, load frames use: 5.29382395744s
step 9606/400000: train loss: 0.151055
epoch:208, batch40, load frames use: 5.09056401253s
step 9607/400000: train loss: 0.133032
epoch:208, batch41, load frames use: 4.93246102333s
step 9608/400000: train loss: 0.134615
epoch:208, batch42, load frames use: 5.21582579613s
step 9609/400000: train loss: 0.150110
epoch:208, batch43, load frames use: 5.65742707253s
step 9610/400000: train loss: 0.148524
epoch:208, batch44, load frames use: 5.11938905716s
step 9611/400000: train loss: 0.158020
epoch:208, batch45, load frames use: 5.46024799347s
step 9612/400000: train loss: 0.132744
epoch:208, batch46, load frames use: 5.4285261631s
step 9613/400000: train loss: 0.165194
epoch:209, batch1, load frames use: 5.46882987022s
step 9614/400000: train loss: 0.136014
epoch:209, batch2, load frames use: 5.30810785294s
step 9615/400000: train loss: 0.143376
epoch:209, batch3, load frames use: 4.83258008957s
step 9616/400000: train loss: 0.133667
epoch:209, batch4, load frames use: 4.90430116653s
step 9617/400000: train loss: 0.157739
epoch:209, batch5, load frames use: 4.95151281357s
step 9618/400000: train loss: 0.157698
epoch:209, batch6, load frames use: 4.74559092522s
step 9619/400000: train loss: 0.163051
epoch:209, batch7, load frames use: 6.11751103401s
step 9620/400000: train loss: 0.143339
epoch:209, batch8, load frames use: 5.2411468029s
step 9621/400000: train loss: 0.150184
epoch:209, batch9, load frames use: 4.77259111404s
step 9622/400000: train loss: 0.133026
epoch:209, batch10, load frames use: 5.29264378548s
step 9623/400000: train loss: 0.160506
epoch:209, batch11, load frames use: 5.07262396812s
step 9624/400000: train loss: 0.145246
epoch:209, batch12, load frames use: 5.48706793785s
step 9625/400000: train loss: 0.145049
epoch:209, batch13, load frames use: 4.71918606758s
step 9626/400000: train loss: 0.141499
epoch:209, batch14, load frames use: 5.31010198593s
step 9627/400000: train loss: 0.157347
epoch:209, batch15, load frames use: 4.95758104324s
step 9628/400000: train loss: 0.158793
epoch:209, batch16, load frames use: 5.50876116753s
step 9629/400000: train loss: 0.145115
epoch:209, batch17, load frames use: 4.91929006577s
step 9630/400000: train loss: 0.170996
epoch:209, batch18, load frames use: 5.04678201675s
step 9631/400000: train loss: 0.156114
epoch:209, batch19, load frames use: 5.7009768486s
step 9632/400000: train loss: 0.149104
epoch:209, batch20, load frames use: 5.09249591827s
step 9633/400000: train loss: 0.160891
epoch:209, batch21, load frames use: 5.31820511818s
step 9634/400000: train loss: 0.137542
epoch:209, batch22, load frames use: 5.83236289024s
step 9635/400000: train loss: 0.151787
epoch:209, batch23, load frames use: 5.6160428524s
step 9636/400000: train loss: 0.140701
epoch:209, batch24, load frames use: 5.09318780899s
step 9637/400000: train loss: 0.147021
epoch:209, batch25, load frames use: 4.92886781693s
step 9638/400000: train loss: 0.147899
epoch:209, batch26, load frames use: 5.68067789078s
step 9639/400000: train loss: 0.147593
epoch:209, batch27, load frames use: 6.84704780579s
step 9640/400000: train loss: 0.146476
epoch:209, batch28, load frames use: 5.62460303307s
step 9641/400000: train loss: 0.168758
epoch:209, batch29, load frames use: 6.02822589874s
step 9642/400000: train loss: 0.156326
epoch:209, batch30, load frames use: 5.94845986366s
step 9643/400000: train loss: 0.126889
epoch:209, batch31, load frames use: 4.97270107269s
step 9644/400000: train loss: 0.140606
epoch:209, batch32, load frames use: 5.79994797707s
step 9645/400000: train loss: 0.145652
epoch:209, batch33, load frames use: 5.10346198082s
step 9646/400000: train loss: 0.154547
epoch:209, batch34, load frames use: 4.99450516701s
step 9647/400000: train loss: 0.149017
epoch:209, batch35, load frames use: 5.52883601189s
step 9648/400000: train loss: 0.148059
epoch:209, batch36, load frames use: 5.39607095718s
step 9649/400000: train loss: 0.141982
epoch:209, batch37, load frames use: 6.05977892876s
step 9650/400000: train loss: 0.174011
epoch:209, batch38, load frames use: 5.4191300869s
step 9651/400000: train loss: 0.157557
epoch:209, batch39, load frames use: 4.8117120266s
step 9652/400000: train loss: 0.130501
epoch:209, batch40, load frames use: 5.3537671566s
step 9653/400000: train loss: 0.141632
epoch:209, batch41, load frames use: 5.63889193535s
step 9654/400000: train loss: 0.144777
epoch:209, batch42, load frames use: 5.12683486938s
step 9655/400000: train loss: 0.166935
epoch:209, batch43, load frames use: 5.10411906242s
step 9656/400000: train loss: 0.147150
epoch:209, batch44, load frames use: 5.60642695427s
step 9657/400000: train loss: 0.147174
epoch:209, batch45, load frames use: 4.93282985687s
step 9658/400000: train loss: 0.149923
epoch:209, batch46, load frames use: 5.42185091972s
step 9659/400000: train loss: 0.157506
epoch:210, batch1, load frames use: 4.63239192963s
step 9660/400000: train loss: 0.153231
epoch:210, batch2, load frames use: 5.28438401222s
step 9661/400000: train loss: 0.145958
epoch:210, batch3, load frames use: 6.13480591774s
step 9662/400000: train loss: 0.156087
epoch:210, batch4, load frames use: 5.34923481941s
step 9663/400000: train loss: 0.153801
epoch:210, batch5, load frames use: 4.93589401245s
step 9664/400000: train loss: 0.148481
epoch:210, batch6, load frames use: 5.18490004539s
step 9665/400000: train loss: 0.136907
epoch:210, batch7, load frames use: 5.62009692192s
step 9666/400000: train loss: 0.147205
epoch:210, batch8, load frames use: 5.71727609634s
step 9667/400000: train loss: 0.153437
epoch:210, batch9, load frames use: 5.69225883484s
step 9668/400000: train loss: 0.155651
epoch:210, batch10, load frames use: 6.08583402634s
step 9669/400000: train loss: 0.134774
epoch:210, batch11, load frames use: 4.8544011116s
step 9670/400000: train loss: 0.154906
epoch:210, batch12, load frames use: 5.67733597755s
step 9671/400000: train loss: 0.152234
epoch:210, batch13, load frames use: 5.52045297623s
step 9672/400000: train loss: 0.138702
epoch:210, batch14, load frames use: 5.74814391136s
step 9673/400000: train loss: 0.145320
epoch:210, batch15, load frames use: 10.0097949505s
step 9674/400000: train loss: 0.135680
epoch:210, batch16, load frames use: 4.53112697601s
step 9675/400000: train loss: 0.134344
epoch:210, batch17, load frames use: 5.2492890358s
step 9676/400000: train loss: 0.145867
epoch:210, batch18, load frames use: 6.52554988861s
step 9677/400000: train loss: 0.135615
epoch:210, batch19, load frames use: 5.3439719677s
step 9678/400000: train loss: 0.155107
epoch:210, batch20, load frames use: 5.57090806961s
step 9679/400000: train loss: 0.168393
epoch:210, batch21, load frames use: 5.1146709919s
step 9680/400000: train loss: 0.135009
epoch:210, batch22, load frames use: 5.60639095306s
step 9681/400000: train loss: 0.137207
epoch:210, batch23, load frames use: 5.07043409348s
step 9682/400000: train loss: 0.131254
epoch:210, batch24, load frames use: 4.90796303749s
step 9683/400000: train loss: 0.144913
epoch:210, batch25, load frames use: 5.42506504059s
step 9684/400000: train loss: 0.149888
epoch:210, batch26, load frames use: 4.59304499626s
step 9685/400000: train loss: 0.148782
epoch:210, batch27, load frames use: 5.20361089706s
step 9686/400000: train loss: 0.141124
epoch:210, batch28, load frames use: 5.52991318703s
step 9687/400000: train loss: 0.148062
epoch:210, batch29, load frames use: 5.33039903641s
step 9688/400000: train loss: 0.134698
epoch:210, batch30, load frames use: 5.70514798164s
step 9689/400000: train loss: 0.136932
epoch:210, batch31, load frames use: 5.56894397736s
step 9690/400000: train loss: 0.145726
epoch:210, batch32, load frames use: 5.25618505478s
step 9691/400000: train loss: 0.155963
epoch:210, batch33, load frames use: 5.29413604736s
step 9692/400000: train loss: 0.158685
epoch:210, batch34, load frames use: 8.7568590641s
step 9693/400000: train loss: 0.166405
epoch:210, batch35, load frames use: 5.66538000107s
step 9694/400000: train loss: 0.156970
epoch:210, batch36, load frames use: 5.4424829483s
step 9695/400000: train loss: 0.161144
epoch:210, batch37, load frames use: 5.6908929348s
step 9696/400000: train loss: 0.153426
epoch:210, batch38, load frames use: 5.38189387321s
step 9697/400000: train loss: 0.135181
epoch:210, batch39, load frames use: 5.59173583984s
step 9698/400000: train loss: 0.146642
epoch:210, batch40, load frames use: 4.96974802017s
step 9699/400000: train loss: 0.159191
epoch:210, batch41, load frames use: 5.34159994125s
step 9700/400000: train loss: 0.144561
step 9700/400000: validation loss: 0.144927
epoch:210, batch42, load frames use: 5.11769485474s
step 9701/400000: train loss: 0.152954
epoch:210, batch43, load frames use: 5.31732106209s
step 9702/400000: train loss: 0.149768
epoch:210, batch44, load frames use: 4.80824589729s
step 9703/400000: train loss: 0.141829
epoch:210, batch45, load frames use: 5.64597201347s
step 9704/400000: train loss: 0.135919
epoch:210, batch46, load frames use: 4.760160923s
step 9705/400000: train loss: 0.143080
epoch:211, batch1, load frames use: 5.94647908211s
step 9706/400000: train loss: 0.151529
epoch:211, batch2, load frames use: 5.82111001015s
step 9707/400000: train loss: 0.141284
epoch:211, batch3, load frames use: 5.52200293541s
step 9708/400000: train loss: 0.127192
epoch:211, batch4, load frames use: 5.45662903786s
step 9709/400000: train loss: 0.142631
epoch:211, batch5, load frames use: 4.76172399521s
step 9710/400000: train loss: 0.140839
epoch:211, batch6, load frames use: 5.76766109467s
step 9711/400000: train loss: 0.134895
epoch:211, batch7, load frames use: 5.73085689545s
step 9712/400000: train loss: 0.144757
epoch:211, batch8, load frames use: 5.50136590004s
step 9713/400000: train loss: 0.138113
epoch:211, batch9, load frames use: 5.11704206467s
step 9714/400000: train loss: 0.132779
epoch:211, batch10, load frames use: 5.24150705338s
step 9715/400000: train loss: 0.154024
epoch:211, batch11, load frames use: 5.22758483887s
step 9716/400000: train loss: 0.150265
epoch:211, batch12, load frames use: 4.56931185722s
step 9717/400000: train loss: 0.144749
epoch:211, batch13, load frames use: 4.5184071064s
step 9718/400000: train loss: 0.159834
epoch:211, batch14, load frames use: 5.13920807838s
step 9719/400000: train loss: 0.145552
epoch:211, batch15, load frames use: 5.52753400803s
step 9720/400000: train loss: 0.134499
epoch:211, batch16, load frames use: 4.51950502396s
step 9721/400000: train loss: 0.134283
epoch:211, batch17, load frames use: 5.25505304337s
step 9722/400000: train loss: 0.121400
epoch:211, batch18, load frames use: 4.96152806282s
step 9723/400000: train loss: 0.135558
epoch:211, batch19, load frames use: 5.40040206909s
step 9724/400000: train loss: 0.139769
epoch:211, batch20, load frames use: 5.12038207054s
step 9725/400000: train loss: 0.147665
epoch:211, batch21, load frames use: 4.98589205742s
step 9726/400000: train loss: 0.138178
epoch:211, batch22, load frames use: 5.4122440815s
step 9727/400000: train loss: 0.143304
epoch:211, batch23, load frames use: 5.14330911636s
step 9728/400000: train loss: 0.133400
epoch:211, batch24, load frames use: 6.5214009285s
step 9729/400000: train loss: 0.128755
epoch:211, batch25, load frames use: 4.8383898735s
step 9730/400000: train loss: 0.136511
epoch:211, batch26, load frames use: 4.7835559845s
step 9731/400000: train loss: 0.140758
epoch:211, batch27, load frames use: 5.15664410591s
step 9732/400000: train loss: 0.136672
epoch:211, batch28, load frames use: 4.74825406075s
step 9733/400000: train loss: 0.160318
epoch:211, batch29, load frames use: 4.98231387138s
step 9734/400000: train loss: 0.170451
epoch:211, batch30, load frames use: 4.78117609024s
step 9735/400000: train loss: 0.146923
epoch:211, batch31, load frames use: 5.09089398384s
step 9736/400000: train loss: 0.151429
epoch:211, batch32, load frames use: 5.25462508202s
step 9737/400000: train loss: 0.137392
epoch:211, batch33, load frames use: 4.92608714104s
step 9738/400000: train loss: 0.135681
epoch:211, batch34, load frames use: 5.34233593941s
step 9739/400000: train loss: 0.151075
epoch:211, batch35, load frames use: 5.65603804588s
step 9740/400000: train loss: 0.132979
epoch:211, batch36, load frames use: 4.99427890778s
step 9741/400000: train loss: 0.151366
epoch:211, batch37, load frames use: 4.78672909737s
step 9742/400000: train loss: 0.149492
epoch:211, batch38, load frames use: 5.2483291626s
step 9743/400000: train loss: 0.145967
epoch:211, batch39, load frames use: 5.49527001381s
step 9744/400000: train loss: 0.158962
epoch:211, batch40, load frames use: 5.45762491226s
step 9745/400000: train loss: 0.159032
epoch:211, batch41, load frames use: 5.17971301079s
step 9746/400000: train loss: 0.154877
epoch:211, batch42, load frames use: 5.07037901878s
step 9747/400000: train loss: 0.153085
epoch:211, batch43, load frames use: 5.08019685745s
step 9748/400000: train loss: 0.163774
epoch:211, batch44, load frames use: 5.47176098824s
step 9749/400000: train loss: 0.134630
epoch:211, batch45, load frames use: 5.05430102348s
step 9750/400000: train loss: 0.141179
epoch:211, batch46, load frames use: 5.79876899719s
step 9751/400000: train loss: 0.152555
epoch:212, batch1, load frames use: 5.4765958786s
step 9752/400000: train loss: 0.148793
epoch:212, batch2, load frames use: 5.3104801178s
step 9753/400000: train loss: 0.147676
epoch:212, batch3, load frames use: 5.13952803612s
step 9754/400000: train loss: 0.148028
epoch:212, batch4, load frames use: 5.08968710899s
step 9755/400000: train loss: 0.125206
epoch:212, batch5, load frames use: 5.60170292854s
step 9756/400000: train loss: 0.146071
epoch:212, batch6, load frames use: 5.3052740097s
step 9757/400000: train loss: 0.139433
epoch:212, batch7, load frames use: 4.43462610245s
step 9758/400000: train loss: 0.139317
epoch:212, batch8, load frames use: 5.45297598839s
step 9759/400000: train loss: 0.131712
epoch:212, batch9, load frames use: 4.89178419113s
step 9760/400000: train loss: 0.142766
epoch:212, batch10, load frames use: 4.92368292809s
step 9761/400000: train loss: 0.149444
epoch:212, batch11, load frames use: 4.90971803665s
step 9762/400000: train loss: 0.139313
epoch:212, batch12, load frames use: 5.10101795197s
step 9763/400000: train loss: 0.137139
epoch:212, batch13, load frames use: 5.13325786591s
step 9764/400000: train loss: 0.141420
epoch:212, batch14, load frames use: 4.81681609154s
step 9765/400000: train loss: 0.132421
epoch:212, batch15, load frames use: 5.0795519352s
step 9766/400000: train loss: 0.140880
epoch:212, batch16, load frames use: 5.2037229538s
step 9767/400000: train loss: 0.153614
epoch:212, batch17, load frames use: 5.19112706184s
step 9768/400000: train loss: 0.133899
epoch:212, batch18, load frames use: 4.31280303001s
step 9769/400000: train loss: 0.135157
epoch:212, batch19, load frames use: 4.98787403107s
step 9770/400000: train loss: 0.146848
epoch:212, batch20, load frames use: 5.0070168972s
step 9771/400000: train loss: 0.146349
epoch:212, batch21, load frames use: 5.07876014709s
step 9772/400000: train loss: 0.127908
epoch:212, batch22, load frames use: 4.98413801193s
step 9773/400000: train loss: 0.146268
epoch:212, batch23, load frames use: 5.11449289322s
step 9774/400000: train loss: 0.149981
epoch:212, batch24, load frames use: 4.97482609749s
step 9775/400000: train loss: 0.133705
epoch:212, batch25, load frames use: 5.35845398903s
step 9776/400000: train loss: 0.142144
epoch:212, batch26, load frames use: 5.186825037s
step 9777/400000: train loss: 0.154935
epoch:212, batch27, load frames use: 5.72859191895s
step 9778/400000: train loss: 0.152950
epoch:212, batch28, load frames use: 5.03135800362s
step 9779/400000: train loss: 0.143099
epoch:212, batch29, load frames use: 4.64809799194s
step 9780/400000: train loss: 0.140506
epoch:212, batch30, load frames use: 5.23073410988s
step 9781/400000: train loss: 0.145959
epoch:212, batch31, load frames use: 5.54058408737s
step 9782/400000: train loss: 0.165996
epoch:212, batch32, load frames use: 5.22124695778s
step 9783/400000: train loss: 0.139815
epoch:212, batch33, load frames use: 5.07192778587s
step 9784/400000: train loss: 0.139003
epoch:212, batch34, load frames use: 4.81960701942s
step 9785/400000: train loss: 0.150820
epoch:212, batch35, load frames use: 5.57978415489s
step 9786/400000: train loss: 0.141676
epoch:212, batch36, load frames use: 5.0443751812s
step 9787/400000: train loss: 0.155017
epoch:212, batch37, load frames use: 4.89441990852s
step 9788/400000: train loss: 0.147536
epoch:212, batch38, load frames use: 5.55640292168s
step 9789/400000: train loss: 0.148204
epoch:212, batch39, load frames use: 4.72491598129s
step 9790/400000: train loss: 0.137410
epoch:212, batch40, load frames use: 5.51822280884s
step 9791/400000: train loss: 0.152764
epoch:212, batch41, load frames use: 4.59513020515s
step 9792/400000: train loss: 0.145647
epoch:212, batch42, load frames use: 5.0866420269s
step 9793/400000: train loss: 0.143256
epoch:212, batch43, load frames use: 5.35915088654s
step 9794/400000: train loss: 0.149035
epoch:212, batch44, load frames use: 4.48751592636s
step 9795/400000: train loss: 0.141914
epoch:212, batch45, load frames use: 5.36489295959s
step 9796/400000: train loss: 0.136820
epoch:212, batch46, load frames use: 4.64147281647s
step 9797/400000: train loss: 0.151095
epoch:213, batch1, load frames use: 5.54644703865s
step 9798/400000: train loss: 0.128394
epoch:213, batch2, load frames use: 5.20685982704s
step 9799/400000: train loss: 0.147127
epoch:213, batch3, load frames use: 5.18321585655s
step 9800/400000: train loss: 0.146917
step 9800/400000: validation loss: 0.132892
epoch:213, batch4, load frames use: 5.57679605484s
step 9801/400000: train loss: 0.137547
epoch:213, batch5, load frames use: 5.05011415482s
step 9802/400000: train loss: 0.149537
epoch:213, batch6, load frames use: 5.74227595329s
step 9803/400000: train loss: 0.139954
epoch:213, batch7, load frames use: 5.08924603462s
step 9804/400000: train loss: 0.142596
epoch:213, batch8, load frames use: 5.33956980705s
step 9805/400000: train loss: 0.133594
epoch:213, batch9, load frames use: 5.16843008995s
step 9806/400000: train loss: 0.120790
epoch:213, batch10, load frames use: 5.19690489769s
step 9807/400000: train loss: 0.148504
epoch:213, batch11, load frames use: 5.01472210884s
step 9808/400000: train loss: 0.145794
epoch:213, batch12, load frames use: 5.60267400742s
step 9809/400000: train loss: 0.155388
epoch:213, batch13, load frames use: 5.21700692177s
step 9810/400000: train loss: 0.135092
epoch:213, batch14, load frames use: 4.78049516678s
step 9811/400000: train loss: 0.132601
epoch:213, batch15, load frames use: 5.59581398964s
step 9812/400000: train loss: 0.157248
epoch:213, batch16, load frames use: 5.27540493011s
step 9813/400000: train loss: 0.141290
epoch:213, batch17, load frames use: 4.98499083519s
step 9814/400000: train loss: 0.161518
epoch:213, batch18, load frames use: 4.55029082298s
step 9815/400000: train loss: 0.145031
epoch:213, batch19, load frames use: 5.06268310547s
step 9816/400000: train loss: 0.151117
epoch:213, batch20, load frames use: 6.00410199165s
step 9817/400000: train loss: 0.156088
epoch:213, batch21, load frames use: 4.80947709084s
step 9818/400000: train loss: 0.150373
epoch:213, batch22, load frames use: 5.20017695427s
step 9819/400000: train loss: 0.151697
epoch:213, batch23, load frames use: 4.75432085991s
step 9820/400000: train loss: 0.143421
epoch:213, batch24, load frames use: 5.6046538353s
step 9821/400000: train loss: 0.143670
epoch:213, batch25, load frames use: 4.58306121826s
step 9822/400000: train loss: 0.160940
epoch:213, batch26, load frames use: 5.20734882355s
step 9823/400000: train loss: 0.138143
epoch:213, batch27, load frames use: 5.25864696503s
step 9824/400000: train loss: 0.132561
epoch:213, batch28, load frames use: 4.75250792503s
step 9825/400000: train loss: 0.134378
epoch:213, batch29, load frames use: 4.96560406685s
step 9826/400000: train loss: 0.151075
epoch:213, batch30, load frames use: 5.12660908699s
step 9827/400000: train loss: 0.121197
epoch:213, batch31, load frames use: 4.65601110458s
step 9828/400000: train loss: 0.137448
epoch:213, batch32, load frames use: 5.68251419067s
step 9829/400000: train loss: 0.150128
epoch:213, batch33, load frames use: 6.46099591255s
step 9830/400000: train loss: 0.146232
epoch:213, batch34, load frames use: 5.3269469738s
step 9831/400000: train loss: 0.143176
epoch:213, batch35, load frames use: 4.94041609764s
step 9832/400000: train loss: 0.136691
epoch:213, batch36, load frames use: 5.2256629467s
step 9833/400000: train loss: 0.146983
epoch:213, batch37, load frames use: 5.17844390869s
step 9834/400000: train loss: 0.161724
epoch:213, batch38, load frames use: 5.40163087845s
step 9835/400000: train loss: 0.137006
epoch:213, batch39, load frames use: 6.83409690857s
step 9836/400000: train loss: 0.150718
epoch:213, batch40, load frames use: 5.3395409584s
step 9837/400000: train loss: 0.144935
epoch:213, batch41, load frames use: 5.18973112106s
step 9838/400000: train loss: 0.158388
epoch:213, batch42, load frames use: 5.44918012619s
step 9839/400000: train loss: 0.159077
epoch:213, batch43, load frames use: 4.97074604034s
step 9840/400000: train loss: 0.126156
epoch:213, batch44, load frames use: 4.92407894135s
step 9841/400000: train loss: 0.164044
epoch:213, batch45, load frames use: 6.29470396042s
step 9842/400000: train loss: 0.156539
epoch:213, batch46, load frames use: 4.11254692078s
step 9843/400000: train loss: 0.161717
epoch:214, batch1, load frames use: 5.14626312256s
step 9844/400000: train loss: 0.149390
epoch:214, batch2, load frames use: 5.07356595993s
step 9845/400000: train loss: 0.153149
epoch:214, batch3, load frames use: 4.82387208939s
step 9846/400000: train loss: 0.137435
epoch:214, batch4, load frames use: 5.29419016838s
step 9847/400000: train loss: 0.136157
epoch:214, batch5, load frames use: 4.96088480949s
step 9848/400000: train loss: 0.155317
epoch:214, batch6, load frames use: 5.14581513405s
step 9849/400000: train loss: 0.168089
epoch:214, batch7, load frames use: 4.96948289871s
step 9850/400000: train loss: 0.151249
epoch:214, batch8, load frames use: 4.89209198952s
step 9851/400000: train loss: 0.152908
epoch:214, batch9, load frames use: 5.20676112175s
step 9852/400000: train loss: 0.146279
epoch:214, batch10, load frames use: 4.97728204727s
step 9853/400000: train loss: 0.154594
epoch:214, batch11, load frames use: 5.19846487045s
step 9854/400000: train loss: 0.141372
epoch:214, batch12, load frames use: 5.15397596359s
step 9855/400000: train loss: 0.150896
epoch:214, batch13, load frames use: 5.44566297531s
step 9856/400000: train loss: 0.137143
epoch:214, batch14, load frames use: 4.93939805031s
step 9857/400000: train loss: 0.137414
epoch:214, batch15, load frames use: 5.07025504112s
step 9858/400000: train loss: 0.137454
epoch:214, batch16, load frames use: 5.2060918808s
step 9859/400000: train loss: 0.161522
epoch:214, batch17, load frames use: 5.93004012108s
step 9860/400000: train loss: 0.166132
epoch:214, batch18, load frames use: 4.92610311508s
step 9861/400000: train loss: 0.150794
epoch:214, batch19, load frames use: 6.09757208824s
step 9862/400000: train loss: 0.142418
epoch:214, batch20, load frames use: 5.01236009598s
step 9863/400000: train loss: 0.147538
epoch:214, batch21, load frames use: 5.85402202606s
step 9864/400000: train loss: 0.151400
epoch:214, batch22, load frames use: 6.09453105927s
step 9865/400000: train loss: 0.158941
epoch:214, batch23, load frames use: 5.6771709919s
step 9866/400000: train loss: 0.157429
epoch:214, batch24, load frames use: 5.54905295372s
step 9867/400000: train loss: 0.158632
epoch:214, batch25, load frames use: 5.23017215729s
step 9868/400000: train loss: 0.131030
epoch:214, batch26, load frames use: 6.15884399414s
step 9869/400000: train loss: 0.163886
epoch:214, batch27, load frames use: 5.36447000504s
step 9870/400000: train loss: 0.152136
epoch:214, batch28, load frames use: 5.21208190918s
step 9871/400000: train loss: 0.137259
epoch:214, batch29, load frames use: 5.51268696785s
step 9872/400000: train loss: 0.151912
epoch:214, batch30, load frames use: 6.68370103836s
step 9873/400000: train loss: 0.147212
epoch:214, batch31, load frames use: 4.50254106522s
step 9874/400000: train loss: 0.142432
epoch:214, batch32, load frames use: 4.5618019104s
step 9875/400000: train loss: 0.146610
epoch:214, batch33, load frames use: 4.98235678673s
step 9876/400000: train loss: 0.157310
epoch:214, batch34, load frames use: 5.27159500122s
step 9877/400000: train loss: 0.148949
epoch:214, batch35, load frames use: 5.40900492668s
step 9878/400000: train loss: 0.147742
epoch:214, batch36, load frames use: 4.93101406097s
step 9879/400000: train loss: 0.153974
epoch:214, batch37, load frames use: 4.97913098335s
step 9880/400000: train loss: 0.150336
epoch:214, batch38, load frames use: 5.45085811615s
step 9881/400000: train loss: 0.160643
epoch:214, batch39, load frames use: 5.28410410881s
step 9882/400000: train loss: 0.151841
epoch:214, batch40, load frames use: 5.16959309578s
step 9883/400000: train loss: 0.137379
epoch:214, batch41, load frames use: 4.76346182823s
step 9884/400000: train loss: 0.149839
epoch:214, batch42, load frames use: 5.34235095978s
step 9885/400000: train loss: 0.153168
epoch:214, batch43, load frames use: 5.20149588585s
step 9886/400000: train loss: 0.151099
epoch:214, batch44, load frames use: 5.73489713669s
step 9887/400000: train loss: 0.165102
epoch:214, batch45, load frames use: 4.82951402664s
step 9888/400000: train loss: 0.151041
epoch:214, batch46, load frames use: 5.42116999626s
step 9889/400000: train loss: 0.152462
epoch:215, batch1, load frames use: 5.17355918884s
step 9890/400000: train loss: 0.140594
epoch:215, batch2, load frames use: 5.44714093208s
step 9891/400000: train loss: 0.141696
epoch:215, batch3, load frames use: 5.41166496277s
step 9892/400000: train loss: 0.143106
epoch:215, batch4, load frames use: 5.07559108734s
step 9893/400000: train loss: 0.138972
epoch:215, batch5, load frames use: 5.49727892876s
step 9894/400000: train loss: 0.145908
epoch:215, batch6, load frames use: 5.06066203117s
step 9895/400000: train loss: 0.143081
epoch:215, batch7, load frames use: 5.34850096703s
step 9896/400000: train loss: 0.141120
epoch:215, batch8, load frames use: 5.24859905243s
step 9897/400000: train loss: 0.146595
epoch:215, batch9, load frames use: 5.05589604378s
step 9898/400000: train loss: 0.149526
epoch:215, batch10, load frames use: 5.54738903046s
step 9899/400000: train loss: 0.127599
epoch:215, batch11, load frames use: 5.12487101555s
step 9900/400000: train loss: 0.129517
step 9900/400000: validation loss: 0.120400
epoch:215, batch12, load frames use: 5.03259396553s
step 9901/400000: train loss: 0.137572
epoch:215, batch13, load frames use: 5.82887196541s
step 9902/400000: train loss: 0.142821
epoch:215, batch14, load frames use: 5.17730998993s
step 9903/400000: train loss: 0.132458
epoch:215, batch15, load frames use: 5.16263890266s
step 9904/400000: train loss: 0.142313
epoch:215, batch16, load frames use: 5.46705102921s
step 9905/400000: train loss: 0.164407
epoch:215, batch17, load frames use: 4.98714780807s
step 9906/400000: train loss: 0.145572
epoch:215, batch18, load frames use: 5.59016394615s
step 9907/400000: train loss: 0.138384
epoch:215, batch19, load frames use: 5.23993706703s
step 9908/400000: train loss: 0.130391
epoch:215, batch20, load frames use: 5.57075500488s
step 9909/400000: train loss: 0.136546
epoch:215, batch21, load frames use: 5.58392095566s
step 9910/400000: train loss: 0.155855
epoch:215, batch22, load frames use: 5.36135482788s
step 9911/400000: train loss: 0.147001
epoch:215, batch23, load frames use: 5.09728097916s
step 9912/400000: train loss: 0.146998
epoch:215, batch24, load frames use: 5.32171416283s
step 9913/400000: train loss: 0.142542
epoch:215, batch25, load frames use: 5.16997098923s
step 9914/400000: train loss: 0.137887
epoch:215, batch26, load frames use: 5.23514413834s
step 9915/400000: train loss: 0.149168
epoch:215, batch27, load frames use: 5.84596896172s
step 9916/400000: train loss: 0.161712
epoch:215, batch28, load frames use: 5.32930207253s
step 9917/400000: train loss: 0.159193
epoch:215, batch29, load frames use: 5.37469291687s
step 9918/400000: train loss: 0.147957
epoch:215, batch30, load frames use: 5.40570902824s
step 9919/400000: train loss: 0.145801
epoch:215, batch31, load frames use: 5.04621887207s
step 9920/400000: train loss: 0.157989
epoch:215, batch32, load frames use: 6.24865603447s
step 9921/400000: train loss: 0.143706
epoch:215, batch33, load frames use: 4.67371201515s
step 9922/400000: train loss: 0.139275
epoch:215, batch34, load frames use: 5.02989411354s
step 9923/400000: train loss: 0.157741
epoch:215, batch35, load frames use: 5.2926170826s
step 9924/400000: train loss: 0.148921
epoch:215, batch36, load frames use: 5.13480710983s
step 9925/400000: train loss: 0.129668
epoch:215, batch37, load frames use: 5.50870895386s
step 9926/400000: train loss: 0.142697
epoch:215, batch38, load frames use: 5.04634189606s
step 9927/400000: train loss: 0.148997
epoch:215, batch39, load frames use: 5.18552589417s
step 9928/400000: train loss: 0.164477
epoch:215, batch40, load frames use: 5.71365904808s
step 9929/400000: train loss: 0.152865
epoch:215, batch41, load frames use: 5.75576210022s
step 9930/400000: train loss: 0.131873
epoch:215, batch42, load frames use: 4.56627321243s
step 9931/400000: train loss: 0.135977
epoch:215, batch43, load frames use: 5.29473304749s
step 9932/400000: train loss: 0.153404
epoch:215, batch44, load frames use: 5.13287305832s
step 9933/400000: train loss: 0.146229
epoch:215, batch45, load frames use: 5.1949441433s
step 9934/400000: train loss: 0.148538
epoch:215, batch46, load frames use: 5.29668784142s
step 9935/400000: train loss: 0.143700
epoch:216, batch1, load frames use: 4.78847098351s
step 9936/400000: train loss: 0.132259
epoch:216, batch2, load frames use: 5.26810598373s
step 9937/400000: train loss: 0.130739
epoch:216, batch3, load frames use: 4.98481202126s
step 9938/400000: train loss: 0.138551
epoch:216, batch4, load frames use: 5.57525491714s
step 9939/400000: train loss: 0.144296
epoch:216, batch5, load frames use: 4.95904707909s
step 9940/400000: train loss: 0.131529
epoch:216, batch6, load frames use: 5.44204592705s
step 9941/400000: train loss: 0.155048
epoch:216, batch7, load frames use: 5.17805099487s
step 9942/400000: train loss: 0.144101
epoch:216, batch8, load frames use: 5.03994297981s
step 9943/400000: train loss: 0.142629
epoch:216, batch9, load frames use: 5.05976986885s
step 9944/400000: train loss: 0.134197
epoch:216, batch10, load frames use: 5.33711314201s
step 9945/400000: train loss: 0.171138
epoch:216, batch11, load frames use: 5.51930880547s
step 9946/400000: train loss: 0.142232
epoch:216, batch12, load frames use: 5.27138495445s
step 9947/400000: train loss: 0.157336
epoch:216, batch13, load frames use: 5.53677392006s
step 9948/400000: train loss: 0.147646
epoch:216, batch14, load frames use: 5.62077713013s
step 9949/400000: train loss: 0.153912
epoch:216, batch15, load frames use: 5.22397899628s
step 9950/400000: train loss: 0.135656
epoch:216, batch16, load frames use: 5.55339217186s
step 9951/400000: train loss: 0.129230
epoch:216, batch17, load frames use: 5.55068898201s
step 9952/400000: train loss: 0.147206
epoch:216, batch18, load frames use: 6.66112518311s
step 9953/400000: train loss: 0.149302
epoch:216, batch19, load frames use: 5.38449788094s
step 9954/400000: train loss: 0.160767
epoch:216, batch20, load frames use: 5.66028380394s
step 9955/400000: train loss: 0.142992
epoch:216, batch21, load frames use: 5.27074217796s
step 9956/400000: train loss: 0.144045
epoch:216, batch22, load frames use: 6.39111399651s
step 9957/400000: train loss: 0.156607
epoch:216, batch23, load frames use: 5.27624797821s
step 9958/400000: train loss: 0.137034
epoch:216, batch24, load frames use: 5.02728700638s
step 9959/400000: train loss: 0.154815
epoch:216, batch25, load frames use: 5.18853116035s
step 9960/400000: train loss: 0.158080
epoch:216, batch26, load frames use: 4.90986609459s
step 9961/400000: train loss: 0.146028
epoch:216, batch27, load frames use: 5.17234182358s
step 9962/400000: train loss: 0.171204
epoch:216, batch28, load frames use: 5.20475387573s
step 9963/400000: train loss: 0.173545
epoch:216, batch29, load frames use: 4.76688313484s
step 9964/400000: train loss: 0.150373
epoch:216, batch30, load frames use: 5.49907398224s
step 9965/400000: train loss: 0.147295
epoch:216, batch31, load frames use: 5.54710197449s
step 9966/400000: train loss: 0.131749
epoch:216, batch32, load frames use: 4.56283211708s
step 9967/400000: train loss: 0.154637
epoch:216, batch33, load frames use: 5.29969596863s
step 9968/400000: train loss: 0.169309
epoch:216, batch34, load frames use: 5.65843009949s
step 9969/400000: train loss: 0.140489
epoch:216, batch35, load frames use: 5.51720499992s
step 9970/400000: train loss: 0.141496
epoch:216, batch36, load frames use: 5.84622502327s
step 9971/400000: train loss: 0.150191
epoch:216, batch37, load frames use: 5.0782430172s
step 9972/400000: train loss: 0.150887
epoch:216, batch38, load frames use: 5.12681484222s
step 9973/400000: train loss: 0.152952
epoch:216, batch39, load frames use: 5.42549395561s
step 9974/400000: train loss: 0.142074
epoch:216, batch40, load frames use: 5.7349319458s
step 9975/400000: train loss: 0.154869
epoch:216, batch41, load frames use: 5.21762514114s
step 9976/400000: train loss: 0.155648
epoch:216, batch42, load frames use: 4.91669893265s
step 9977/400000: train loss: 0.139627
epoch:216, batch43, load frames use: 4.88196706772s
step 9978/400000: train loss: 0.158400
epoch:216, batch44, load frames use: 5.64938378334s
step 9979/400000: train loss: 0.165937
epoch:216, batch45, load frames use: 5.32020998001s
step 9980/400000: train loss: 0.149604
epoch:216, batch46, load frames use: 5.01295995712s
step 9981/400000: train loss: 0.131850
epoch:217, batch1, load frames use: 6.01860117912s
step 9982/400000: train loss: 0.138025
epoch:217, batch2, load frames use: 5.80932307243s
step 9983/400000: train loss: 0.142703
epoch:217, batch3, load frames use: 5.37344408035s
step 9984/400000: train loss: 0.150707
epoch:217, batch4, load frames use: 5.18699502945s
step 9985/400000: train loss: 0.129554
epoch:217, batch5, load frames use: 5.73746299744s
step 9986/400000: train loss: 0.126703
epoch:217, batch6, load frames use: 5.48998403549s
step 9987/400000: train loss: 0.143621
epoch:217, batch7, load frames use: 5.22703003883s
step 9988/400000: train loss: 0.138782
epoch:217, batch8, load frames use: 4.8432469368s
step 9989/400000: train loss: 0.156125
epoch:217, batch9, load frames use: 5.40172505379s
step 9990/400000: train loss: 0.129536
epoch:217, batch10, load frames use: 5.35169816017s
step 9991/400000: train loss: 0.140712
epoch:217, batch11, load frames use: 4.99741101265s
step 9992/400000: train loss: 0.132231
epoch:217, batch12, load frames use: 4.93555903435s
step 9993/400000: train loss: 0.142394
epoch:217, batch13, load frames use: 4.73857402802s
step 9994/400000: train loss: 0.126449
epoch:217, batch14, load frames use: 5.80757284164s
step 9995/400000: train loss: 0.149664
epoch:217, batch15, load frames use: 5.22140598297s
step 9996/400000: train loss: 0.173336
epoch:217, batch16, load frames use: 4.79612612724s
step 9997/400000: train loss: 0.165210
epoch:217, batch17, load frames use: 5.29168605804s
step 9998/400000: train loss: 0.146357
epoch:217, batch18, load frames use: 5.05881214142s
step 9999/400000: train loss: 0.170600
epoch:217, batch19, load frames use: 5.11070585251s
step 10000/400000: train loss: 0.157086
step 10000/400000: validation loss: 0.151490
epoch:217, batch20, load frames use: 5.22222614288s
step 10001/400000: train loss: 0.157662
epoch:217, batch21, load frames use: 6.33923316002s
step 10002/400000: train loss: 0.148557
epoch:217, batch22, load frames use: 3.95111203194s
step 10003/400000: train loss: 0.145689
epoch:217, batch23, load frames use: 5.70741510391s
step 10004/400000: train loss: 0.160741
epoch:217, batch24, load frames use: 5.71248412132s
step 10005/400000: train loss: 0.129835
epoch:217, batch25, load frames use: 5.19504594803s
step 10006/400000: train loss: 0.140169
epoch:217, batch26, load frames use: 4.64987802505s
step 10007/400000: train loss: 0.138930
epoch:217, batch27, load frames use: 4.78683686256s
step 10008/400000: train loss: 0.139806
epoch:217, batch28, load frames use: 5.00342392921s
step 10009/400000: train loss: 0.134316
epoch:217, batch29, load frames use: 4.73620009422s
step 10010/400000: train loss: 0.158054
epoch:217, batch30, load frames use: 5.23497581482s
step 10011/400000: train loss: 0.142894
epoch:217, batch31, load frames use: 5.02254605293s
step 10012/400000: train loss: 0.149237
epoch:217, batch32, load frames use: 5.62937092781s
step 10013/400000: train loss: 0.167694
epoch:217, batch33, load frames use: 5.24406194687s
step 10014/400000: train loss: 0.138814
epoch:217, batch34, load frames use: 5.6646130085s
step 10015/400000: train loss: 0.146242
epoch:217, batch35, load frames use: 5.43821620941s
step 10016/400000: train loss: 0.137321
epoch:217, batch36, load frames use: 6.22551298141s
step 10017/400000: train loss: 0.149158
epoch:217, batch37, load frames use: 5.16615915298s
step 10018/400000: train loss: 0.148395
epoch:217, batch38, load frames use: 5.48790311813s
step 10019/400000: train loss: 0.135982
epoch:217, batch39, load frames use: 5.00122690201s
step 10020/400000: train loss: 0.159906
epoch:217, batch40, load frames use: 5.33190107346s
step 10021/400000: train loss: 0.145214
epoch:217, batch41, load frames use: 5.51236200333s
step 10022/400000: train loss: 0.137534
epoch:217, batch42, load frames use: 5.49297785759s
step 10023/400000: train loss: 0.143781
epoch:217, batch43, load frames use: 5.03264093399s
step 10024/400000: train loss: 0.152594
epoch:217, batch44, load frames use: 5.30643892288s
step 10025/400000: train loss: 0.150662
epoch:217, batch45, load frames use: 5.44785785675s
step 10026/400000: train loss: 0.141793
epoch:217, batch46, load frames use: 5.25982093811s
step 10027/400000: train loss: 0.134965
epoch:218, batch1, load frames use: 4.82751703262s
step 10028/400000: train loss: 0.127818
epoch:218, batch2, load frames use: 5.35218405724s
step 10029/400000: train loss: 0.133530
epoch:218, batch3, load frames use: 4.82968521118s
step 10030/400000: train loss: 0.138291
epoch:218, batch4, load frames use: 5.33425307274s
step 10031/400000: train loss: 0.128320
epoch:218, batch5, load frames use: 5.44763684273s
step 10032/400000: train loss: 0.144668
epoch:218, batch6, load frames use: 5.2177760601s
step 10033/400000: train loss: 0.125384
epoch:218, batch7, load frames use: 5.51655912399s
step 10034/400000: train loss: 0.131821
epoch:218, batch8, load frames use: 5.78187513351s
step 10035/400000: train loss: 0.128900
epoch:218, batch9, load frames use: 5.84402012825s
step 10036/400000: train loss: 0.148490
epoch:218, batch10, load frames use: 5.13761091232s
step 10037/400000: train loss: 0.163202
epoch:218, batch11, load frames use: 5.37569189072s
step 10038/400000: train loss: 0.152843
epoch:218, batch12, load frames use: 5.52864384651s
step 10039/400000: train loss: 0.144546
epoch:218, batch13, load frames use: 5.87264800072s
step 10040/400000: train loss: 0.137547
epoch:218, batch14, load frames use: 5.68245816231s
step 10041/400000: train loss: 0.137999
epoch:218, batch15, load frames use: 5.71778988838s
step 10042/400000: train loss: 0.131852
epoch:218, batch16, load frames use: 5.21112203598s
step 10043/400000: train loss: 0.125766
epoch:218, batch17, load frames use: 6.08750200272s
step 10044/400000: train loss: 0.137057
epoch:218, batch18, load frames use: 5.62600708008s
step 10045/400000: train loss: 0.139978
epoch:218, batch19, load frames use: 5.16119289398s
step 10046/400000: train loss: 0.140510
epoch:218, batch20, load frames use: 5.10796904564s
step 10047/400000: train loss: 0.151282
epoch:218, batch21, load frames use: 5.42225909233s
step 10048/400000: train loss: 0.151944
epoch:218, batch22, load frames use: 5.29717707634s
step 10049/400000: train loss: 0.141243
epoch:218, batch23, load frames use: 5.47965216637s
step 10050/400000: train loss: 0.146408
epoch:218, batch24, load frames use: 4.86328792572s
step 10051/400000: train loss: 0.131663
epoch:218, batch25, load frames use: 5.19925403595s
step 10052/400000: train loss: 0.147288
epoch:218, batch26, load frames use: 4.95479607582s
step 10053/400000: train loss: 0.143411
epoch:218, batch27, load frames use: 5.47266387939s
step 10054/400000: train loss: 0.142087
epoch:218, batch28, load frames use: 5.08069086075s
step 10055/400000: train loss: 0.130520
epoch:218, batch29, load frames use: 6.12639904022s
step 10056/400000: train loss: 0.142738
epoch:218, batch30, load frames use: 5.36785793304s
step 10057/400000: train loss: 0.150143
epoch:218, batch31, load frames use: 5.19497394562s
step 10058/400000: train loss: 0.148049
epoch:218, batch32, load frames use: 5.23640704155s
step 10059/400000: train loss: 0.160080
epoch:218, batch33, load frames use: 5.19828605652s
step 10060/400000: train loss: 0.135740
epoch:218, batch34, load frames use: 5.37079787254s
step 10061/400000: train loss: 0.144067
epoch:218, batch35, load frames use: 4.95690608025s
step 10062/400000: train loss: 0.153758
epoch:218, batch36, load frames use: 5.19234681129s
step 10063/400000: train loss: 0.145633
epoch:218, batch37, load frames use: 5.10821795464s
step 10064/400000: train loss: 0.158829
epoch:218, batch38, load frames use: 4.89337491989s
step 10065/400000: train loss: 0.142124
epoch:218, batch39, load frames use: 4.46777009964s
step 10066/400000: train loss: 0.134481
epoch:218, batch40, load frames use: 5.4824821949s
step 10067/400000: train loss: 0.145175
epoch:218, batch41, load frames use: 4.9943010807s
step 10068/400000: train loss: 0.134090
epoch:218, batch42, load frames use: 5.32642292976s
step 10069/400000: train loss: 0.171694
epoch:218, batch43, load frames use: 5.14602398872s
step 10070/400000: train loss: 0.137880
epoch:218, batch44, load frames use: 5.2533390522s
step 10071/400000: train loss: 0.154714
epoch:218, batch45, load frames use: 5.37904500961s
step 10072/400000: train loss: 0.138993
epoch:218, batch46, load frames use: 5.11664700508s
step 10073/400000: train loss: 0.144227
epoch:219, batch1, load frames use: 5.25170707703s
step 10074/400000: train loss: 0.135062
epoch:219, batch2, load frames use: 5.64871716499s
step 10075/400000: train loss: 0.148605
epoch:219, batch3, load frames use: 5.32038807869s
step 10076/400000: train loss: 0.143779
epoch:219, batch4, load frames use: 5.11662602425s
step 10077/400000: train loss: 0.131943
epoch:219, batch5, load frames use: 5.36378383636s
step 10078/400000: train loss: 0.157167
epoch:219, batch6, load frames use: 4.74041986465s
step 10079/400000: train loss: 0.142188
epoch:219, batch7, load frames use: 5.01641893387s
step 10080/400000: train loss: 0.154257
epoch:219, batch8, load frames use: 5.00239706039s
step 10081/400000: train loss: 0.140369
epoch:219, batch9, load frames use: 5.17362618446s
step 10082/400000: train loss: 0.145578
epoch:219, batch10, load frames use: 5.53843593597s
step 10083/400000: train loss: 0.139619
epoch:219, batch11, load frames use: 5.15878891945s
step 10084/400000: train loss: 0.139601
epoch:219, batch12, load frames use: 5.41113710403s
step 10085/400000: train loss: 0.144648
epoch:219, batch13, load frames use: 5.20033311844s
step 10086/400000: train loss: 0.130786
epoch:219, batch14, load frames use: 5.15945792198s
step 10087/400000: train loss: 0.136950
epoch:219, batch15, load frames use: 5.33650803566s
step 10088/400000: train loss: 0.136184
epoch:219, batch16, load frames use: 5.42400312424s
step 10089/400000: train loss: 0.132987
epoch:219, batch17, load frames use: 5.36018705368s
step 10090/400000: train loss: 0.137979
epoch:219, batch18, load frames use: 5.6376979351s
step 10091/400000: train loss: 0.138204
epoch:219, batch19, load frames use: 5.10296010971s
step 10092/400000: train loss: 0.140761
epoch:219, batch20, load frames use: 5.5919880867s
step 10093/400000: train loss: 0.147903
epoch:219, batch21, load frames use: 5.773209095s
step 10094/400000: train loss: 0.152642
epoch:219, batch22, load frames use: 5.13039922714s
step 10095/400000: train loss: 0.129282
epoch:219, batch23, load frames use: 5.01742100716s
step 10096/400000: train loss: 0.134755
epoch:219, batch24, load frames use: 5.47606706619s
step 10097/400000: train loss: 0.132726
epoch:219, batch25, load frames use: 4.56136202812s
step 10098/400000: train loss: 0.156319
epoch:219, batch26, load frames use: 5.38259196281s
step 10099/400000: train loss: 0.139124
epoch:219, batch27, load frames use: 5.77850198746s
step 10100/400000: train loss: 0.133526
step 10100/400000: validation loss: 0.126910
epoch:219, batch28, load frames use: 5.52298998833s
step 10101/400000: train loss: 0.142926
epoch:219, batch29, load frames use: 5.82849597931s
step 10102/400000: train loss: 0.169630
epoch:219, batch30, load frames use: 5.69146990776s
step 10103/400000: train loss: 0.130614
epoch:219, batch31, load frames use: 5.12539815903s
step 10104/400000: train loss: 0.147471
epoch:219, batch32, load frames use: 5.73034405708s
step 10105/400000: train loss: 0.152536
epoch:219, batch33, load frames use: 5.05357384682s
step 10106/400000: train loss: 0.144246
epoch:219, batch34, load frames use: 5.22024703026s
step 10107/400000: train loss: 0.135407
epoch:219, batch35, load frames use: 5.31207799911s
step 10108/400000: train loss: 0.141132
epoch:219, batch36, load frames use: 5.1003241539s
step 10109/400000: train loss: 0.133514
epoch:219, batch37, load frames use: 5.44936394691s
step 10110/400000: train loss: 0.132323
epoch:219, batch38, load frames use: 5.06253600121s
step 10111/400000: train loss: 0.134212
epoch:219, batch39, load frames use: 5.71873402596s
step 10112/400000: train loss: 0.153596
epoch:219, batch40, load frames use: 5.48187303543s
step 10113/400000: train loss: 0.154934
epoch:219, batch41, load frames use: 4.89315199852s
step 10114/400000: train loss: 0.127059
epoch:219, batch42, load frames use: 4.97984910011s
step 10115/400000: train loss: 0.143617
epoch:219, batch43, load frames use: 5.23969292641s
step 10116/400000: train loss: 0.150795
epoch:219, batch44, load frames use: 6.02427101135s
step 10117/400000: train loss: 0.129662
epoch:219, batch45, load frames use: 5.30179691315s
step 10118/400000: train loss: 0.147836
epoch:219, batch46, load frames use: 4.84269881248s
step 10119/400000: train loss: 0.157213
epoch:220, batch1, load frames use: 5.00624394417s
step 10120/400000: train loss: 0.142793
epoch:220, batch2, load frames use: 5.79713892937s
step 10121/400000: train loss: 0.167667
epoch:220, batch3, load frames use: 5.44266581535s
step 10122/400000: train loss: 0.155355
epoch:220, batch4, load frames use: 5.58740091324s
step 10123/400000: train loss: 0.146351
epoch:220, batch5, load frames use: 5.35478997231s
step 10124/400000: train loss: 0.149516
epoch:220, batch6, load frames use: 4.99247384071s
step 10125/400000: train loss: 0.144730
epoch:220, batch7, load frames use: 5.62519288063s
step 10126/400000: train loss: 0.137812
epoch:220, batch8, load frames use: 5.83236098289s
step 10127/400000: train loss: 0.135287
epoch:220, batch9, load frames use: 5.04329705238s
step 10128/400000: train loss: 0.130427
epoch:220, batch10, load frames use: 5.87334990501s
step 10129/400000: train loss: 0.141323
epoch:220, batch11, load frames use: 5.7956829071s
step 10130/400000: train loss: 0.139700
epoch:220, batch12, load frames use: 5.29233384132s
step 10131/400000: train loss: 0.135024
epoch:220, batch13, load frames use: 5.86972880363s
step 10132/400000: train loss: 0.142160
epoch:220, batch14, load frames use: 5.04433703423s
step 10133/400000: train loss: 0.146302
epoch:220, batch15, load frames use: 4.90711116791s
step 10134/400000: train loss: 0.139400
epoch:220, batch16, load frames use: 5.84764313698s
step 10135/400000: train loss: 0.132008
epoch:220, batch17, load frames use: 5.14681601524s
step 10136/400000: train loss: 0.137647
epoch:220, batch18, load frames use: 5.54992198944s
step 10137/400000: train loss: 0.151122
epoch:220, batch19, load frames use: 5.45230913162s
step 10138/400000: train loss: 0.139451
epoch:220, batch20, load frames use: 5.30237007141s
step 10139/400000: train loss: 0.133364
epoch:220, batch21, load frames use: 5.32144403458s
step 10140/400000: train loss: 0.149931
epoch:220, batch22, load frames use: 5.26893901825s
step 10141/400000: train loss: 0.144982
epoch:220, batch23, load frames use: 5.47618699074s
step 10142/400000: train loss: 0.138436
epoch:220, batch24, load frames use: 5.45076704025s
step 10143/400000: train loss: 0.138812
epoch:220, batch25, load frames use: 5.53798389435s
step 10144/400000: train loss: 0.142307
epoch:220, batch26, load frames use: 5.34940481186s
step 10145/400000: train loss: 0.139742
epoch:220, batch27, load frames use: 5.88156914711s
step 10146/400000: train loss: 0.130242
epoch:220, batch28, load frames use: 5.06529808044s
step 10147/400000: train loss: 0.150565
epoch:220, batch29, load frames use: 5.6223180294s
step 10148/400000: train loss: 0.131928
epoch:220, batch30, load frames use: 5.08633589745s
step 10149/400000: train loss: 0.147822
epoch:220, batch31, load frames use: 6.11750793457s
step 10150/400000: train loss: 0.152598
epoch:220, batch32, load frames use: 5.57116007805s
step 10151/400000: train loss: 0.159241
epoch:220, batch33, load frames use: 5.36800003052s
step 10152/400000: train loss: 0.148983
epoch:220, batch34, load frames use: 5.07553100586s
step 10153/400000: train loss: 0.157009
epoch:220, batch35, load frames use: 5.59051990509s
step 10154/400000: train loss: 0.132249
epoch:220, batch36, load frames use: 4.77651405334s
step 10155/400000: train loss: 0.158135
epoch:220, batch37, load frames use: 5.74692201614s
step 10156/400000: train loss: 0.149887
epoch:220, batch38, load frames use: 5.12029409409s
step 10157/400000: train loss: 0.155014
epoch:220, batch39, load frames use: 5.63609004021s
step 10158/400000: train loss: 0.137511
epoch:220, batch40, load frames use: 5.18496704102s
step 10159/400000: train loss: 0.160447
epoch:220, batch41, load frames use: 5.21419906616s
step 10160/400000: train loss: 0.151109
epoch:220, batch42, load frames use: 5.69684100151s
step 10161/400000: train loss: 0.139591
epoch:220, batch43, load frames use: 4.72313499451s
step 10162/400000: train loss: 0.150937
epoch:220, batch44, load frames use: 5.50659394264s
step 10163/400000: train loss: 0.147493
epoch:220, batch45, load frames use: 5.62229704857s
step 10164/400000: train loss: 0.140457
epoch:220, batch46, load frames use: 5.50954914093s
step 10165/400000: train loss: 0.143584
epoch:221, batch1, load frames use: 5.51551389694s
step 10166/400000: train loss: 0.148403
epoch:221, batch2, load frames use: 5.75495886803s
step 10167/400000: train loss: 0.159523
epoch:221, batch3, load frames use: 5.84737586975s
step 10168/400000: train loss: 0.123890
epoch:221, batch4, load frames use: 5.55034399033s
step 10169/400000: train loss: 0.140028
epoch:221, batch5, load frames use: 5.33091592789s
step 10170/400000: train loss: 0.127696
epoch:221, batch6, load frames use: 6.23036003113s
step 10171/400000: train loss: 0.146788
epoch:221, batch7, load frames use: 5.23569989204s
step 10172/400000: train loss: 0.148366
epoch:221, batch8, load frames use: 4.76001310349s
step 10173/400000: train loss: 0.120709
epoch:221, batch9, load frames use: 5.80282783508s
step 10174/400000: train loss: 0.128068
epoch:221, batch10, load frames use: 5.49880695343s
step 10175/400000: train loss: 0.134379
epoch:221, batch11, load frames use: 5.44714713097s
step 10176/400000: train loss: 0.144948
epoch:221, batch12, load frames use: 5.41739702225s
step 10177/400000: train loss: 0.158321
epoch:221, batch13, load frames use: 5.40688920021s
step 10178/400000: train loss: 0.137762
epoch:221, batch14, load frames use: 5.8452899456s
step 10179/400000: train loss: 0.136946
epoch:221, batch15, load frames use: 5.46407198906s
step 10180/400000: train loss: 0.136850
epoch:221, batch16, load frames use: 6.21073698997s
step 10181/400000: train loss: 0.153618
epoch:221, batch17, load frames use: 4.64632105827s
step 10182/400000: train loss: 0.148300
epoch:221, batch18, load frames use: 5.55967092514s
step 10183/400000: train loss: 0.161597
epoch:221, batch19, load frames use: 5.71350693703s
step 10184/400000: train loss: 0.145952
epoch:221, batch20, load frames use: 6.03451704979s
step 10185/400000: train loss: 0.125571
epoch:221, batch21, load frames use: 5.42706799507s
step 10186/400000: train loss: 0.149409
epoch:221, batch22, load frames use: 5.54851508141s
step 10187/400000: train loss: 0.139203
epoch:221, batch23, load frames use: 6.04662299156s
step 10188/400000: train loss: 0.147119
epoch:221, batch24, load frames use: 5.44555306435s
step 10189/400000: train loss: 0.161438
epoch:221, batch25, load frames use: 5.31838011742s
step 10190/400000: train loss: 0.147442
epoch:221, batch26, load frames use: 5.85711193085s
step 10191/400000: train loss: 0.124338
epoch:221, batch27, load frames use: 4.79605221748s
step 10192/400000: train loss: 0.144868
epoch:221, batch28, load frames use: 5.44161891937s
step 10193/400000: train loss: 0.138593
epoch:221, batch29, load frames use: 5.62081384659s
step 10194/400000: train loss: 0.129224
epoch:221, batch30, load frames use: 5.2035150528s
step 10195/400000: train loss: 0.148800
epoch:221, batch31, load frames use: 5.28871488571s
step 10196/400000: train loss: 0.160551
epoch:221, batch32, load frames use: 5.77112603188s
step 10197/400000: train loss: 0.144932
epoch:221, batch33, load frames use: 5.33947300911s
step 10198/400000: train loss: 0.137112
epoch:221, batch34, load frames use: 5.83124995232s
step 10199/400000: train loss: 0.141489
epoch:221, batch35, load frames use: 5.22247505188s
step 10200/400000: train loss: 0.149636
step 10200/400000: validation loss: 0.140382
epoch:221, batch36, load frames use: 5.34727811813s
step 10201/400000: train loss: 0.137419
epoch:221, batch37, load frames use: 5.98269510269s
step 10202/400000: train loss: 0.149168
epoch:221, batch38, load frames use: 5.33843803406s
step 10203/400000: train loss: 0.157188
epoch:221, batch39, load frames use: 5.55017495155s
step 10204/400000: train loss: 0.131319
epoch:221, batch40, load frames use: 4.73574614525s
step 10205/400000: train loss: 0.119411
epoch:221, batch41, load frames use: 5.53199791908s
step 10206/400000: train loss: 0.142941
epoch:221, batch42, load frames use: 5.11131691933s
step 10207/400000: train loss: 0.148831
epoch:221, batch43, load frames use: 5.38691997528s
step 10208/400000: train loss: 0.135743
epoch:221, batch44, load frames use: 5.39109611511s
step 10209/400000: train loss: 0.138432
epoch:221, batch45, load frames use: 5.24630904198s
step 10210/400000: train loss: 0.137612
epoch:221, batch46, load frames use: 5.08439612389s
step 10211/400000: train loss: 0.132003
epoch:222, batch1, load frames use: 5.30553603172s
step 10212/400000: train loss: 0.123577
epoch:222, batch2, load frames use: 5.63840293884s
step 10213/400000: train loss: 0.131026
epoch:222, batch3, load frames use: 5.07491207123s
step 10214/400000: train loss: 0.123969
epoch:222, batch4, load frames use: 5.45816683769s
step 10215/400000: train loss: 0.133774
epoch:222, batch5, load frames use: 5.42789793015s
step 10216/400000: train loss: 0.129999
epoch:222, batch6, load frames use: 4.93917608261s
step 10217/400000: train loss: 0.134702
epoch:222, batch7, load frames use: 4.44578289986s
step 10218/400000: train loss: 0.141112
epoch:222, batch8, load frames use: 4.96633005142s
step 10219/400000: train loss: 0.135087
epoch:222, batch9, load frames use: 4.70056605339s
step 10220/400000: train loss: 0.139896
epoch:222, batch10, load frames use: 5.22937893867s
step 10221/400000: train loss: 0.145545
epoch:222, batch11, load frames use: 4.88399600983s
step 10222/400000: train loss: 0.133563
epoch:222, batch12, load frames use: 5.23069405556s
step 10223/400000: train loss: 0.136079
epoch:222, batch13, load frames use: 5.25345802307s
step 10224/400000: train loss: 0.137029
epoch:222, batch14, load frames use: 4.8765501976s
step 10225/400000: train loss: 0.141780
epoch:222, batch15, load frames use: 4.89047217369s
step 10226/400000: train loss: 0.126562
epoch:222, batch16, load frames use: 5.40269207954s
step 10227/400000: train loss: 0.134619
epoch:222, batch17, load frames use: 5.33667302132s
step 10228/400000: train loss: 0.136590
epoch:222, batch18, load frames use: 5.08665299416s
step 10229/400000: train loss: 0.131254
epoch:222, batch19, load frames use: 5.48165416718s
step 10230/400000: train loss: 0.152852
epoch:222, batch20, load frames use: 5.51296615601s
step 10231/400000: train loss: 0.158344
epoch:222, batch21, load frames use: 5.29082298279s
step 10232/400000: train loss: 0.128251
epoch:222, batch22, load frames use: 5.16040086746s
step 10233/400000: train loss: 0.130523
epoch:222, batch23, load frames use: 5.48614120483s
step 10234/400000: train loss: 0.123209
epoch:222, batch24, load frames use: 5.43925690651s
step 10235/400000: train loss: 0.145201
epoch:222, batch25, load frames use: 5.68576097488s
step 10236/400000: train loss: 0.135975
epoch:222, batch26, load frames use: 5.62555003166s
step 10237/400000: train loss: 0.141476
epoch:222, batch27, load frames use: 5.56389188766s
step 10238/400000: train loss: 0.139380
epoch:222, batch28, load frames use: 5.48565816879s
step 10239/400000: train loss: 0.159907
epoch:222, batch29, load frames use: 5.60940003395s
step 10240/400000: train loss: 0.121103
epoch:222, batch30, load frames use: 5.69094991684s
step 10241/400000: train loss: 0.148771
epoch:222, batch31, load frames use: 5.33878087997s
step 10242/400000: train loss: 0.158647
epoch:222, batch32, load frames use: 6.00355195999s
step 10243/400000: train loss: 0.142891
epoch:222, batch33, load frames use: 4.66512608528s
step 10244/400000: train loss: 0.129858
epoch:222, batch34, load frames use: 5.59621596336s
step 10245/400000: train loss: 0.141289
epoch:222, batch35, load frames use: 5.38165688515s
step 10246/400000: train loss: 0.159067
epoch:222, batch36, load frames use: 4.53664708138s
step 10247/400000: train loss: 0.148442
epoch:222, batch37, load frames use: 5.04183197021s
step 10248/400000: train loss: 0.138637
epoch:222, batch38, load frames use: 5.203125s
step 10249/400000: train loss: 0.140703
epoch:222, batch39, load frames use: 5.20658993721s
step 10250/400000: train loss: 0.130578
epoch:222, batch40, load frames use: 5.48559808731s
step 10251/400000: train loss: 0.145251
epoch:222, batch41, load frames use: 5.78953123093s
step 10252/400000: train loss: 0.143429
epoch:222, batch42, load frames use: 5.69155287743s
step 10253/400000: train loss: 0.151143
epoch:222, batch43, load frames use: 5.49244308472s
step 10254/400000: train loss: 0.150390
epoch:222, batch44, load frames use: 6.14612388611s
step 10255/400000: train loss: 0.153935
epoch:222, batch45, load frames use: 5.64222407341s
step 10256/400000: train loss: 0.144604
epoch:222, batch46, load frames use: 5.57065391541s
step 10257/400000: train loss: 0.146740
epoch:223, batch1, load frames use: 4.99722909927s
step 10258/400000: train loss: 0.150991
epoch:223, batch2, load frames use: 5.3306210041s
step 10259/400000: train loss: 0.134331
epoch:223, batch3, load frames use: 5.47298407555s
step 10260/400000: train loss: 0.133374
epoch:223, batch4, load frames use: 5.27838778496s
step 10261/400000: train loss: 0.128397
epoch:223, batch5, load frames use: 5.5247399807s
step 10262/400000: train loss: 0.125787
epoch:223, batch6, load frames use: 5.69931912422s
step 10263/400000: train loss: 0.123709
epoch:223, batch7, load frames use: 4.82761907578s
step 10264/400000: train loss: 0.131564
epoch:223, batch8, load frames use: 6.00266599655s
step 10265/400000: train loss: 0.142896
epoch:223, batch9, load frames use: 4.96779894829s
step 10266/400000: train loss: 0.134434
epoch:223, batch10, load frames use: 5.81640720367s
step 10267/400000: train loss: 0.146212
epoch:223, batch11, load frames use: 5.28934001923s
step 10268/400000: train loss: 0.138366
epoch:223, batch12, load frames use: 5.20542216301s
step 10269/400000: train loss: 0.146036
epoch:223, batch13, load frames use: 5.2471280098s
step 10270/400000: train loss: 0.133610
epoch:223, batch14, load frames use: 5.77342891693s
step 10271/400000: train loss: 0.129363
epoch:223, batch15, load frames use: 4.8528740406s
step 10272/400000: train loss: 0.131454
epoch:223, batch16, load frames use: 5.72574806213s
step 10273/400000: train loss: 0.144380
epoch:223, batch17, load frames use: 5.63679289818s
step 10274/400000: train loss: 0.135806
epoch:223, batch18, load frames use: 5.59292984009s
step 10275/400000: train loss: 0.149398
epoch:223, batch19, load frames use: 5.43919897079s
step 10276/400000: train loss: 0.137142
epoch:223, batch20, load frames use: 5.37399291992s
step 10277/400000: train loss: 0.148731
epoch:223, batch21, load frames use: 5.73608207703s
step 10278/400000: train loss: 0.134920
epoch:223, batch22, load frames use: 5.38248920441s
step 10279/400000: train loss: 0.129084
epoch:223, batch23, load frames use: 5.26526999474s
step 10280/400000: train loss: 0.140527
epoch:223, batch24, load frames use: 4.81452584267s
step 10281/400000: train loss: 0.139740
epoch:223, batch25, load frames use: 5.39313197136s
step 10282/400000: train loss: 0.138039
epoch:223, batch26, load frames use: 5.35773801804s
step 10283/400000: train loss: 0.132578
epoch:223, batch27, load frames use: 5.61123108864s
step 10284/400000: train loss: 0.152945
epoch:223, batch28, load frames use: 5.77550601959s
step 10285/400000: train loss: 0.154248
epoch:223, batch29, load frames use: 5.40407013893s
step 10286/400000: train loss: 0.133962
epoch:223, batch30, load frames use: 5.85601210594s
step 10287/400000: train loss: 0.141287
epoch:223, batch31, load frames use: 5.40476703644s
step 10288/400000: train loss: 0.149308
epoch:223, batch32, load frames use: 5.19063997269s
step 10289/400000: train loss: 0.141020
epoch:223, batch33, load frames use: 5.67416191101s
step 10290/400000: train loss: 0.133628
epoch:223, batch34, load frames use: 5.48881602287s
step 10291/400000: train loss: 0.160188
epoch:223, batch35, load frames use: 5.15707993507s
step 10292/400000: train loss: 0.165210
epoch:223, batch36, load frames use: 5.45420002937s
step 10293/400000: train loss: 0.136136
epoch:223, batch37, load frames use: 4.57788300514s
step 10294/400000: train loss: 0.151926
epoch:223, batch38, load frames use: 5.38897800446s
step 10295/400000: train loss: 0.138083
epoch:223, batch39, load frames use: 5.64888501167s
step 10296/400000: train loss: 0.174343
epoch:223, batch40, load frames use: 5.17774510384s
step 10297/400000: train loss: 0.137022
epoch:223, batch41, load frames use: 5.4579410553s
step 10298/400000: train loss: 0.140096
epoch:223, batch42, load frames use: 5.0924680233s
step 10299/400000: train loss: 0.139700
epoch:223, batch43, load frames use: 4.91750192642s
step 10300/400000: train loss: 0.143218
step 10300/400000: validation loss: 0.139367
epoch:223, batch44, load frames use: 5.55181717873s
step 10301/400000: train loss: 0.168981
epoch:223, batch45, load frames use: 5.13037204742s
step 10302/400000: train loss: 0.145804
epoch:223, batch46, load frames use: 5.33843016624s
step 10303/400000: train loss: 0.160719
epoch:224, batch1, load frames use: 5.20750594139s
step 10304/400000: train loss: 0.143316
epoch:224, batch2, load frames use: 5.19690203667s
step 10305/400000: train loss: 0.130428
epoch:224, batch3, load frames use: 5.24572706223s
step 10306/400000: train loss: 0.128961
epoch:224, batch4, load frames use: 5.51615190506s
step 10307/400000: train loss: 0.141682
epoch:224, batch5, load frames use: 5.14195203781s
step 10308/400000: train loss: 0.136260
epoch:224, batch6, load frames use: 5.35825204849s
step 10309/400000: train loss: 0.134866
epoch:224, batch7, load frames use: 5.17849802971s
step 10310/400000: train loss: 0.149123
epoch:224, batch8, load frames use: 5.4217839241s
step 10311/400000: train loss: 0.149200
epoch:224, batch9, load frames use: 5.29750204086s
step 10312/400000: train loss: 0.136256
epoch:224, batch10, load frames use: 5.40805912018s
step 10313/400000: train loss: 0.145610
epoch:224, batch11, load frames use: 5.41213989258s
step 10314/400000: train loss: 0.138171
epoch:224, batch12, load frames use: 5.49427103996s
step 10315/400000: train loss: 0.148125
epoch:224, batch13, load frames use: 4.5838830471s
step 10316/400000: train loss: 0.138526
epoch:224, batch14, load frames use: 5.47646284103s
step 10317/400000: train loss: 0.143258
epoch:224, batch15, load frames use: 5.38455986977s
step 10318/400000: train loss: 0.134114
epoch:224, batch16, load frames use: 5.69411587715s
step 10319/400000: train loss: 0.137905
epoch:224, batch17, load frames use: 5.13364887238s
step 10320/400000: train loss: 0.166608
epoch:224, batch18, load frames use: 5.30589413643s
step 10321/400000: train loss: 0.151531
epoch:224, batch19, load frames use: 4.68180918694s
step 10322/400000: train loss: 0.148670
epoch:224, batch20, load frames use: 5.75123000145s
step 10323/400000: train loss: 0.131645
epoch:224, batch21, load frames use: 4.76888489723s
step 10324/400000: train loss: 0.133197
epoch:224, batch22, load frames use: 5.63163113594s
step 10325/400000: train loss: 0.132361
epoch:224, batch23, load frames use: 5.19353508949s
step 10326/400000: train loss: 0.144187
epoch:224, batch24, load frames use: 5.22478795052s
step 10327/400000: train loss: 0.157022
epoch:224, batch25, load frames use: 4.90744400024s
step 10328/400000: train loss: 0.135053
epoch:224, batch26, load frames use: 5.72793483734s
step 10329/400000: train loss: 0.135645
epoch:224, batch27, load frames use: 5.74446892738s
step 10330/400000: train loss: 0.155952
epoch:224, batch28, load frames use: 5.17336988449s
step 10331/400000: train loss: 0.133774
epoch:224, batch29, load frames use: 6.26274013519s
step 10332/400000: train loss: 0.127860
epoch:224, batch30, load frames use: 5.69567394257s
step 10333/400000: train loss: 0.138868
epoch:224, batch31, load frames use: 5.28683400154s
step 10334/400000: train loss: 0.153261
epoch:224, batch32, load frames use: 5.51607084274s
step 10335/400000: train loss: 0.141699
epoch:224, batch33, load frames use: 5.28624105453s
step 10336/400000: train loss: 0.144897
epoch:224, batch34, load frames use: 5.95218896866s
step 10337/400000: train loss: 0.141017
epoch:224, batch35, load frames use: 5.34207582474s
step 10338/400000: train loss: 0.127430
epoch:224, batch36, load frames use: 5.35380983353s
step 10339/400000: train loss: 0.140996
epoch:224, batch37, load frames use: 5.24907684326s
step 10340/400000: train loss: 0.143716
epoch:224, batch38, load frames use: 5.38391304016s
step 10341/400000: train loss: 0.149201
epoch:224, batch39, load frames use: 5.49686098099s
step 10342/400000: train loss: 0.141906
epoch:224, batch40, load frames use: 6.13282299042s
step 10343/400000: train loss: 0.135434
epoch:224, batch41, load frames use: 4.91000699997s
step 10344/400000: train loss: 0.129891
epoch:224, batch42, load frames use: 5.06653499603s
step 10345/400000: train loss: 0.148936
epoch:224, batch43, load frames use: 5.228992939s
step 10346/400000: train loss: 0.119445
epoch:224, batch44, load frames use: 5.017993927s
step 10347/400000: train loss: 0.127568
epoch:224, batch45, load frames use: 4.91112494469s
step 10348/400000: train loss: 0.147023
epoch:224, batch46, load frames use: 5.69653487206s
step 10349/400000: train loss: 0.136339
epoch:225, batch1, load frames use: 4.81896591187s
step 10350/400000: train loss: 0.142105
epoch:225, batch2, load frames use: 4.82315611839s
step 10351/400000: train loss: 0.138659
epoch:225, batch3, load frames use: 4.90521001816s
step 10352/400000: train loss: 0.141527
epoch:225, batch4, load frames use: 5.64686799049s
step 10353/400000: train loss: 0.128352
epoch:225, batch5, load frames use: 5.63396883011s
step 10354/400000: train loss: 0.136865
epoch:225, batch6, load frames use: 5.10462784767s
step 10355/400000: train loss: 0.148613
epoch:225, batch7, load frames use: 5.59844303131s
step 10356/400000: train loss: 0.136035
epoch:225, batch8, load frames use: 5.37123203278s
step 10357/400000: train loss: 0.146087
epoch:225, batch9, load frames use: 5.3447830677s
step 10358/400000: train loss: 0.136704
epoch:225, batch10, load frames use: 5.78966093063s
step 10359/400000: train loss: 0.152896
epoch:225, batch11, load frames use: 5.51898598671s
step 10360/400000: train loss: 0.134595
epoch:225, batch12, load frames use: 6.19629192352s
step 10361/400000: train loss: 0.143007
epoch:225, batch13, load frames use: 5.14626216888s
step 10362/400000: train loss: 0.134001
epoch:225, batch14, load frames use: 5.58995819092s
step 10363/400000: train loss: 0.131692
epoch:225, batch15, load frames use: 5.13367509842s
step 10364/400000: train loss: 0.135962
epoch:225, batch16, load frames use: 5.08753800392s
step 10365/400000: train loss: 0.149887
epoch:225, batch17, load frames use: 5.0140080452s
step 10366/400000: train loss: 0.133085
epoch:225, batch18, load frames use: 5.24026107788s
step 10367/400000: train loss: 0.136067
epoch:225, batch19, load frames use: 4.98525190353s
step 10368/400000: train loss: 0.139895
epoch:225, batch20, load frames use: 5.19835495949s
step 10369/400000: train loss: 0.152720
epoch:225, batch21, load frames use: 5.37919998169s
step 10370/400000: train loss: 0.155591
epoch:225, batch22, load frames use: 5.62194085121s
step 10371/400000: train loss: 0.144748
epoch:225, batch23, load frames use: 5.08992195129s
step 10372/400000: train loss: 0.130101
epoch:225, batch24, load frames use: 4.83491492271s
step 10373/400000: train loss: 0.124424
epoch:225, batch25, load frames use: 4.97581195831s
step 10374/400000: train loss: 0.130963
epoch:225, batch26, load frames use: 6.07295799255s
step 10375/400000: train loss: 0.128353
epoch:225, batch27, load frames use: 5.447961092s
step 10376/400000: train loss: 0.140758
epoch:225, batch28, load frames use: 4.57620191574s
step 10377/400000: train loss: 0.142994
epoch:225, batch29, load frames use: 5.30874490738s
step 10378/400000: train loss: 0.150485
epoch:225, batch30, load frames use: 5.90785503387s
step 10379/400000: train loss: 0.142288
epoch:225, batch31, load frames use: 5.06915783882s
step 10380/400000: train loss: 0.136721
epoch:225, batch32, load frames use: 5.27807497978s
step 10381/400000: train loss: 0.143884
epoch:225, batch33, load frames use: 5.07503414154s
step 10382/400000: train loss: 0.136447
epoch:225, batch34, load frames use: 4.75727796555s
step 10383/400000: train loss: 0.154698
epoch:225, batch35, load frames use: 5.81151103973s
step 10384/400000: train loss: 0.142076
epoch:225, batch36, load frames use: 5.4945628643s
step 10385/400000: train loss: 0.136903
epoch:225, batch37, load frames use: 5.54161500931s
step 10386/400000: train loss: 0.155089
epoch:225, batch38, load frames use: 5.88185691833s
step 10387/400000: train loss: 0.137797
epoch:225, batch39, load frames use: 4.64766788483s
step 10388/400000: train loss: 0.147224
epoch:225, batch40, load frames use: 4.77490711212s
step 10389/400000: train loss: 0.127589
epoch:225, batch41, load frames use: 5.46632790565s
step 10390/400000: train loss: 0.134430
epoch:225, batch42, load frames use: 5.4337220192s
step 10391/400000: train loss: 0.135224
epoch:225, batch43, load frames use: 4.75273108482s
step 10392/400000: train loss: 0.147638
epoch:225, batch44, load frames use: 5.28943610191s
step 10393/400000: train loss: 0.145206
epoch:225, batch45, load frames use: 6.177090168s
step 10394/400000: train loss: 0.143521
epoch:225, batch46, load frames use: 5.01936817169s
step 10395/400000: train loss: 0.155883
epoch:226, batch1, load frames use: 5.06517696381s
step 10396/400000: train loss: 0.120509
epoch:226, batch2, load frames use: 5.13044404984s
step 10397/400000: train loss: 0.133652
epoch:226, batch3, load frames use: 4.8103120327s
step 10398/400000: train loss: 0.134180
epoch:226, batch4, load frames use: 5.04928207397s
step 10399/400000: train loss: 0.129607
epoch:226, batch5, load frames use: 5.14141488075s
step 10400/400000: train loss: 0.143464
step 10400/400000: validation loss: 0.128666
epoch:226, batch6, load frames use: 5.05947709084s
step 10401/400000: train loss: 0.140710
epoch:226, batch7, load frames use: 6.0182929039s
step 10402/400000: train loss: 0.128535
epoch:226, batch8, load frames use: 5.10196208954s
step 10403/400000: train loss: 0.137320
epoch:226, batch9, load frames use: 5.99318003654s
step 10404/400000: train loss: 0.144273
epoch:226, batch10, load frames use: 5.44684004784s
step 10405/400000: train loss: 0.138332
epoch:226, batch11, load frames use: 4.7752468586s
step 10406/400000: train loss: 0.148961
epoch:226, batch12, load frames use: 4.97021412849s
step 10407/400000: train loss: 0.133007
epoch:226, batch13, load frames use: 5.07627415657s
step 10408/400000: train loss: 0.130382
epoch:226, batch14, load frames use: 4.99186086655s
step 10409/400000: train loss: 0.125102
epoch:226, batch15, load frames use: 5.13612103462s
step 10410/400000: train loss: 0.139048
epoch:226, batch16, load frames use: 5.48992609978s
step 10411/400000: train loss: 0.134513
epoch:226, batch17, load frames use: 5.02526783943s
step 10412/400000: train loss: 0.144572
epoch:226, batch18, load frames use: 5.14913320541s
step 10413/400000: train loss: 0.151994
epoch:226, batch19, load frames use: 4.95051407814s
step 10414/400000: train loss: 0.129564
epoch:226, batch20, load frames use: 5.26105999947s
step 10415/400000: train loss: 0.123965
epoch:226, batch21, load frames use: 5.30760097504s
step 10416/400000: train loss: 0.134905
epoch:226, batch22, load frames use: 5.86525297165s
step 10417/400000: train loss: 0.144774
epoch:226, batch23, load frames use: 5.75578188896s
step 10418/400000: train loss: 0.134968
epoch:226, batch24, load frames use: 5.37532711029s
step 10419/400000: train loss: 0.126663
epoch:226, batch25, load frames use: 5.08735895157s
step 10420/400000: train loss: 0.123700
epoch:226, batch26, load frames use: 5.03626894951s
step 10421/400000: train loss: 0.140430
epoch:226, batch27, load frames use: 5.47435688972s
step 10422/400000: train loss: 0.149360
epoch:226, batch28, load frames use: 4.65339899063s
step 10423/400000: train loss: 0.157084
epoch:226, batch29, load frames use: 5.75800585747s
step 10424/400000: train loss: 0.141902
epoch:226, batch30, load frames use: 5.71337819099s
step 10425/400000: train loss: 0.144436
epoch:226, batch31, load frames use: 5.58217787743s
step 10426/400000: train loss: 0.152886
epoch:226, batch32, load frames use: 5.5727751255s
step 10427/400000: train loss: 0.133983
epoch:226, batch33, load frames use: 5.16929221153s
step 10428/400000: train loss: 0.132384
epoch:226, batch34, load frames use: 5.73783397675s
step 10429/400000: train loss: 0.148638
epoch:226, batch35, load frames use: 4.87125706673s
step 10430/400000: train loss: 0.157297
epoch:226, batch36, load frames use: 6.43743085861s
step 10431/400000: train loss: 0.139183
epoch:226, batch37, load frames use: 5.23295807838s
step 10432/400000: train loss: 0.139735
epoch:226, batch38, load frames use: 5.08221316338s
step 10433/400000: train loss: 0.158725
epoch:226, batch39, load frames use: 5.2047419548s
step 10434/400000: train loss: 0.122600
epoch:226, batch40, load frames use: 5.54754185677s
step 10435/400000: train loss: 0.147879
epoch:226, batch41, load frames use: 4.89666199684s
step 10436/400000: train loss: 0.150724
epoch:226, batch42, load frames use: 5.23404407501s
step 10437/400000: train loss: 0.140808
epoch:226, batch43, load frames use: 5.45248103142s
step 10438/400000: train loss: 0.144630
epoch:226, batch44, load frames use: 5.52382516861s
step 10439/400000: train loss: 0.158033
epoch:226, batch45, load frames use: 5.48742008209s
step 10440/400000: train loss: 0.137611
epoch:226, batch46, load frames use: 5.13922214508s
step 10441/400000: train loss: 0.147726
epoch:227, batch1, load frames use: 5.46816515923s
step 10442/400000: train loss: 0.136927
epoch:227, batch2, load frames use: 7.05904698372s
step 10443/400000: train loss: 0.144208
epoch:227, batch3, load frames use: 4.74149608612s
step 10444/400000: train loss: 0.134480
epoch:227, batch4, load frames use: 5.63331198692s
step 10445/400000: train loss: 0.142492
epoch:227, batch5, load frames use: 5.13572096825s
step 10446/400000: train loss: 0.135498
epoch:227, batch6, load frames use: 5.05089497566s
step 10447/400000: train loss: 0.131985
epoch:227, batch7, load frames use: 4.57996201515s
step 10448/400000: train loss: 0.140426
epoch:227, batch8, load frames use: 5.2786359787s
step 10449/400000: train loss: 0.150762
epoch:227, batch9, load frames use: 5.07770490646s
step 10450/400000: train loss: 0.155509
epoch:227, batch10, load frames use: 5.38628482819s
step 10451/400000: train loss: 0.138675
epoch:227, batch11, load frames use: 5.03164601326s
step 10452/400000: train loss: 0.159144
epoch:227, batch12, load frames use: 5.07613706589s
step 10453/400000: train loss: 0.130900
epoch:227, batch13, load frames use: 5.81749892235s
step 10454/400000: train loss: 0.134743
epoch:227, batch14, load frames use: 4.90934586525s
step 10455/400000: train loss: 0.136452
epoch:227, batch15, load frames use: 5.21918702126s
step 10456/400000: train loss: 0.131363
epoch:227, batch16, load frames use: 4.89998197556s
step 10457/400000: train loss: 0.127401
epoch:227, batch17, load frames use: 5.51004505157s
step 10458/400000: train loss: 0.135178
epoch:227, batch18, load frames use: 5.44151806831s
step 10459/400000: train loss: 0.134454
epoch:227, batch19, load frames use: 4.49213290215s
step 10460/400000: train loss: 0.157260
epoch:227, batch20, load frames use: 4.46264600754s
step 10461/400000: train loss: 0.135434
epoch:227, batch21, load frames use: 5.01459598541s
step 10462/400000: train loss: 0.142531
epoch:227, batch22, load frames use: 5.39607405663s
step 10463/400000: train loss: 0.140659
epoch:227, batch23, load frames use: 5.22687506676s
step 10464/400000: train loss: 0.146366
epoch:227, batch24, load frames use: 5.65358710289s
step 10465/400000: train loss: 0.120946
epoch:227, batch25, load frames use: 5.4082608223s
step 10466/400000: train loss: 0.121600
epoch:227, batch26, load frames use: 5.01443004608s
step 10467/400000: train loss: 0.132130
epoch:227, batch27, load frames use: 4.98706007004s
step 10468/400000: train loss: 0.138905
epoch:227, batch28, load frames use: 5.24209094048s
step 10469/400000: train loss: 0.149199
epoch:227, batch29, load frames use: 4.95480084419s
step 10470/400000: train loss: 0.128227
epoch:227, batch30, load frames use: 5.4785490036s
step 10471/400000: train loss: 0.132695
epoch:227, batch31, load frames use: 5.34807705879s
step 10472/400000: train loss: 0.127259
epoch:227, batch32, load frames use: 5.13052296638s
step 10473/400000: train loss: 0.144856
epoch:227, batch33, load frames use: 4.83629393578s
step 10474/400000: train loss: 0.138317
epoch:227, batch34, load frames use: 5.34954309464s
step 10475/400000: train loss: 0.153793
epoch:227, batch35, load frames use: 5.28196382523s
step 10476/400000: train loss: 0.137525
epoch:227, batch36, load frames use: 5.06606411934s
step 10477/400000: train loss: 0.153487
epoch:227, batch37, load frames use: 5.80512189865s
step 10478/400000: train loss: 0.126006
epoch:227, batch38, load frames use: 4.7433359623s
step 10479/400000: train loss: 0.126833
epoch:227, batch39, load frames use: 6.23856496811s
step 10480/400000: train loss: 0.139052
epoch:227, batch40, load frames use: 5.21288394928s
step 10481/400000: train loss: 0.140344
epoch:227, batch41, load frames use: 7.33997917175s
step 10482/400000: train loss: 0.120929
epoch:227, batch42, load frames use: 5.43811511993s
step 10483/400000: train loss: 0.148748
epoch:227, batch43, load frames use: 5.01371502876s
step 10484/400000: train loss: 0.143832
epoch:227, batch44, load frames use: 5.24153399467s
step 10485/400000: train loss: 0.133430
epoch:227, batch45, load frames use: 5.96466588974s
step 10486/400000: train loss: 0.150703
epoch:227, batch46, load frames use: 5.26737499237s
step 10487/400000: train loss: 0.164401
epoch:228, batch1, load frames use: 5.62797403336s
step 10488/400000: train loss: 0.159432
epoch:228, batch2, load frames use: 5.38532209396s
step 10489/400000: train loss: 0.143788
epoch:228, batch3, load frames use: 6.15370392799s
step 10490/400000: train loss: 0.132518
epoch:228, batch4, load frames use: 6.19341993332s
step 10491/400000: train loss: 0.153375
epoch:228, batch5, load frames use: 4.9233648777s
step 10492/400000: train loss: 0.126886
epoch:228, batch6, load frames use: 5.27905106544s
step 10493/400000: train loss: 0.139920
epoch:228, batch7, load frames use: 5.6029779911s
step 10494/400000: train loss: 0.153416
epoch:228, batch8, load frames use: 5.75876903534s
step 10495/400000: train loss: 0.153542
epoch:228, batch9, load frames use: 5.40121603012s
step 10496/400000: train loss: 0.132746
epoch:228, batch10, load frames use: 5.31205391884s
step 10497/400000: train loss: 0.148439
epoch:228, batch11, load frames use: 5.09605288506s
step 10498/400000: train loss: 0.118313
epoch:228, batch12, load frames use: 5.7429690361s
step 10499/400000: train loss: 0.141111
epoch:228, batch13, load frames use: 5.30999493599s
step 10500/400000: train loss: 0.146128
step 10500/400000: validation loss: 0.135836
epoch:228, batch14, load frames use: 5.42181491852s
step 10501/400000: train loss: 0.144284
epoch:228, batch15, load frames use: 5.31985402107s
step 10502/400000: train loss: 0.133938
epoch:228, batch16, load frames use: 5.02163696289s
step 10503/400000: train loss: 0.131157
epoch:228, batch17, load frames use: 5.45463609695s
step 10504/400000: train loss: 0.148811
epoch:228, batch18, load frames use: 5.09960794449s
step 10505/400000: train loss: 0.146708
epoch:228, batch19, load frames use: 6.34198307991s
step 10506/400000: train loss: 0.132913
epoch:228, batch20, load frames use: 5.70702719688s
step 10507/400000: train loss: 0.141677
epoch:228, batch21, load frames use: 5.52611517906s
step 10508/400000: train loss: 0.148121
epoch:228, batch22, load frames use: 5.20547890663s
step 10509/400000: train loss: 0.140944
epoch:228, batch23, load frames use: 5.45110106468s
step 10510/400000: train loss: 0.146360
epoch:228, batch24, load frames use: 5.3731970787s
step 10511/400000: train loss: 0.147711
epoch:228, batch25, load frames use: 4.81384420395s
step 10512/400000: train loss: 0.138748
epoch:228, batch26, load frames use: 5.15032482147s
step 10513/400000: train loss: 0.135182
epoch:228, batch27, load frames use: 5.25273299217s
step 10514/400000: train loss: 0.132509
epoch:228, batch28, load frames use: 5.16838002205s
step 10515/400000: train loss: 0.120518
epoch:228, batch29, load frames use: 5.16012001038s
step 10516/400000: train loss: 0.138131
epoch:228, batch30, load frames use: 6.06507110596s
step 10517/400000: train loss: 0.144278
epoch:228, batch31, load frames use: 5.2520699501s
step 10518/400000: train loss: 0.143761
epoch:228, batch32, load frames use: 5.17425489426s
step 10519/400000: train loss: 0.147607
epoch:228, batch33, load frames use: 5.0998609066s
step 10520/400000: train loss: 0.142422
epoch:228, batch34, load frames use: 5.87044000626s
step 10521/400000: train loss: 0.133924
epoch:228, batch35, load frames use: 5.59090614319s
step 10522/400000: train loss: 0.130687
epoch:228, batch36, load frames use: 5.29370498657s
step 10523/400000: train loss: 0.131230
epoch:228, batch37, load frames use: 5.39794588089s
step 10524/400000: train loss: 0.132016
epoch:228, batch38, load frames use: 5.65158700943s
step 10525/400000: train loss: 0.133797
epoch:228, batch39, load frames use: 5.34192800522s
step 10526/400000: train loss: 0.126212
epoch:228, batch40, load frames use: 5.2552728653s
step 10527/400000: train loss: 0.144573
epoch:228, batch41, load frames use: 5.4202580452s
step 10528/400000: train loss: 0.141445
epoch:228, batch42, load frames use: 5.36713886261s
step 10529/400000: train loss: 0.139546
epoch:228, batch43, load frames use: 5.36631298065s
step 10530/400000: train loss: 0.155373
epoch:228, batch44, load frames use: 5.58534598351s
step 10531/400000: train loss: 0.156179
epoch:228, batch45, load frames use: 5.44678401947s
step 10532/400000: train loss: 0.136244
epoch:228, batch46, load frames use: 5.84771895409s
step 10533/400000: train loss: 0.134063
epoch:229, batch1, load frames use: 5.18868398666s
step 10534/400000: train loss: 0.130282
epoch:229, batch2, load frames use: 5.23644304276s
step 10535/400000: train loss: 0.147123
epoch:229, batch3, load frames use: 5.1490240097s
step 10536/400000: train loss: 0.125042
epoch:229, batch4, load frames use: 5.52593994141s
step 10537/400000: train loss: 0.144807
epoch:229, batch5, load frames use: 5.3007478714s
step 10538/400000: train loss: 0.138967
epoch:229, batch6, load frames use: 7.05993318558s
step 10539/400000: train loss: 0.138173
epoch:229, batch7, load frames use: 5.44552707672s
step 10540/400000: train loss: 0.136574
epoch:229, batch8, load frames use: 5.46720194817s
step 10541/400000: train loss: 0.140058
epoch:229, batch9, load frames use: 5.33137392998s
step 10542/400000: train loss: 0.127879
epoch:229, batch10, load frames use: 5.40446400642s
step 10543/400000: train loss: 0.145053
epoch:229, batch11, load frames use: 5.67106294632s
step 10544/400000: train loss: 0.147087
epoch:229, batch12, load frames use: 5.27619504929s
step 10545/400000: train loss: 0.130386
epoch:229, batch13, load frames use: 5.34487700462s
step 10546/400000: train loss: 0.135382
epoch:229, batch14, load frames use: 5.16148900986s
step 10547/400000: train loss: 0.134639
epoch:229, batch15, load frames use: 5.22880578041s
step 10548/400000: train loss: 0.138785
epoch:229, batch16, load frames use: 5.37190079689s
step 10549/400000: train loss: 0.156744
epoch:229, batch17, load frames use: 5.93809008598s
step 10550/400000: train loss: 0.131175
epoch:229, batch18, load frames use: 5.84262895584s
step 10551/400000: train loss: 0.135579
epoch:229, batch19, load frames use: 5.87274694443s
step 10552/400000: train loss: 0.122797
epoch:229, batch20, load frames use: 5.06494212151s
step 10553/400000: train loss: 0.135070
epoch:229, batch21, load frames use: 5.19774079323s
step 10554/400000: train loss: 0.137071
epoch:229, batch22, load frames use: 4.97619390488s
step 10555/400000: train loss: 0.141756
epoch:229, batch23, load frames use: 5.11920714378s
step 10556/400000: train loss: 0.137484
epoch:229, batch24, load frames use: 5.28258085251s
step 10557/400000: train loss: 0.170374
epoch:229, batch25, load frames use: 5.14559388161s
step 10558/400000: train loss: 0.124554
epoch:229, batch26, load frames use: 5.56562805176s
step 10559/400000: train loss: 0.139352
epoch:229, batch27, load frames use: 5.55567312241s
step 10560/400000: train loss: 0.121165
epoch:229, batch28, load frames use: 5.62987399101s
step 10561/400000: train loss: 0.131304
epoch:229, batch29, load frames use: 6.52146911621s
step 10562/400000: train loss: 0.147106
epoch:229, batch30, load frames use: 5.60815811157s
step 10563/400000: train loss: 0.149830
epoch:229, batch31, load frames use: 6.59597992897s
step 10564/400000: train loss: 0.161047
epoch:229, batch32, load frames use: 5.61814188957s
step 10565/400000: train loss: 0.149036
epoch:229, batch33, load frames use: 5.2856938839s
step 10566/400000: train loss: 0.130522
epoch:229, batch34, load frames use: 5.96947312355s
step 10567/400000: train loss: 0.159316
epoch:229, batch35, load frames use: 5.64617991447s
step 10568/400000: train loss: 0.145513
epoch:229, batch36, load frames use: 5.54600906372s
step 10569/400000: train loss: 0.131068
epoch:229, batch37, load frames use: 5.27926802635s
step 10570/400000: train loss: 0.157714
epoch:229, batch38, load frames use: 5.94007301331s
step 10571/400000: train loss: 0.147153
epoch:229, batch39, load frames use: 5.77807998657s
step 10572/400000: train loss: 0.146533
epoch:229, batch40, load frames use: 5.66681694984s
step 10573/400000: train loss: 0.142129
epoch:229, batch41, load frames use: 6.32872891426s
step 10574/400000: train loss: 0.140895
epoch:229, batch42, load frames use: 4.68205785751s
step 10575/400000: train loss: 0.167775
epoch:229, batch43, load frames use: 5.91146612167s
step 10576/400000: train loss: 0.143440
epoch:229, batch44, load frames use: 6.34388399124s
step 10577/400000: train loss: 0.140915
epoch:229, batch45, load frames use: 5.12793517113s
step 10578/400000: train loss: 0.133920
epoch:229, batch46, load frames use: 5.74126315117s
step 10579/400000: train loss: 0.155261
epoch:230, batch1, load frames use: 6.76171207428s
step 10580/400000: train loss: 0.140301
epoch:230, batch2, load frames use: 5.34810185432s
step 10581/400000: train loss: 0.136052
epoch:230, batch3, load frames use: 5.38772797585s
step 10582/400000: train loss: 0.132026
epoch:230, batch4, load frames use: 5.13803005219s
step 10583/400000: train loss: 0.129123
epoch:230, batch5, load frames use: 5.47469687462s
step 10584/400000: train loss: 0.139094
epoch:230, batch6, load frames use: 6.3207218647s
step 10585/400000: train loss: 0.127073
epoch:230, batch7, load frames use: 7.9113111496s
step 10586/400000: train loss: 0.129618
epoch:230, batch8, load frames use: 6.81254386902s
step 10587/400000: train loss: 0.130368
epoch:230, batch9, load frames use: 5.13027095795s
step 10588/400000: train loss: 0.115703
epoch:230, batch10, load frames use: 5.16599798203s
step 10589/400000: train loss: 0.123577
epoch:230, batch11, load frames use: 5.67784881592s
step 10590/400000: train loss: 0.140215
epoch:230, batch12, load frames use: 6.88761901855s
step 10591/400000: train loss: 0.147281
epoch:230, batch13, load frames use: 5.8889939785s
step 10592/400000: train loss: 0.125904
epoch:230, batch14, load frames use: 6.23215889931s
step 10593/400000: train loss: 0.134426
epoch:230, batch15, load frames use: 5.60196685791s
step 10594/400000: train loss: 0.143411
epoch:230, batch16, load frames use: 5.48721790314s
step 10595/400000: train loss: 0.139205
epoch:230, batch17, load frames use: 6.06144213676s
step 10596/400000: train loss: 0.137178
epoch:230, batch18, load frames use: 6.2935731411s
step 10597/400000: train loss: 0.136487
epoch:230, batch19, load frames use: 6.23169517517s
step 10598/400000: train loss: 0.130400
epoch:230, batch20, load frames use: 6.03426599503s
step 10599/400000: train loss: 0.155214
epoch:230, batch21, load frames use: 6.04692482948s
step 10600/400000: train loss: 0.172434
step 10600/400000: validation loss: 0.161163
epoch:230, batch22, load frames use: 5.62411999702s
step 10601/400000: train loss: 0.137631
epoch:230, batch23, load frames use: 6.9773888588s
step 10602/400000: train loss: 0.155631
epoch:230, batch24, load frames use: 6.47960495949s
step 10603/400000: train loss: 0.146607
epoch:230, batch25, load frames use: 5.96438694s
step 10604/400000: train loss: 0.148149
epoch:230, batch26, load frames use: 6.23104691505s
step 10605/400000: train loss: 0.146828
epoch:230, batch27, load frames use: 5.64274597168s
step 10606/400000: train loss: 0.147465
epoch:230, batch28, load frames use: 5.71716785431s
step 10607/400000: train loss: 0.122091
epoch:230, batch29, load frames use: 5.48960995674s
step 10608/400000: train loss: 0.155530
epoch:230, batch30, load frames use: 6.68411111832s
step 10609/400000: train loss: 0.151060
epoch:230, batch31, load frames use: 5.84416890144s
step 10610/400000: train loss: 0.124588
epoch:230, batch32, load frames use: 5.4198679924s
step 10611/400000: train loss: 0.129192
epoch:230, batch33, load frames use: 5.82792592049s
step 10612/400000: train loss: 0.136845
epoch:230, batch34, load frames use: 8.39212393761s
step 10613/400000: train loss: 0.137492
epoch:230, batch35, load frames use: 5.76495218277s
step 10614/400000: train loss: 0.133968
epoch:230, batch36, load frames use: 6.81562900543s
step 10615/400000: train loss: 0.146876
epoch:230, batch37, load frames use: 5.70058894157s
step 10616/400000: train loss: 0.133019
epoch:230, batch38, load frames use: 5.3106610775s
step 10617/400000: train loss: 0.132195
epoch:230, batch39, load frames use: 6.06792402267s
step 10618/400000: train loss: 0.149902
epoch:230, batch40, load frames use: 6.92679381371s
step 10619/400000: train loss: 0.146738
epoch:230, batch41, load frames use: 6.36796808243s
step 10620/400000: train loss: 0.144827
epoch:230, batch42, load frames use: 5.87954592705s
step 10621/400000: train loss: 0.134746
epoch:230, batch43, load frames use: 5.35699009895s
step 10622/400000: train loss: 0.151255
epoch:230, batch44, load frames use: 5.53613901138s
step 10623/400000: train loss: 0.137038
epoch:230, batch45, load frames use: 6.892152071s
step 10624/400000: train loss: 0.161916
epoch:230, batch46, load frames use: 4.80382490158s
step 10625/400000: train loss: 0.150265
epoch:231, batch1, load frames use: 5.87469601631s
step 10626/400000: train loss: 0.144365
epoch:231, batch2, load frames use: 5.61543989182s
step 10627/400000: train loss: 0.144541
epoch:231, batch3, load frames use: 5.65558004379s
step 10628/400000: train loss: 0.132271
epoch:231, batch4, load frames use: 4.89921283722s
step 10629/400000: train loss: 0.131364
epoch:231, batch5, load frames use: 6.40448999405s
step 10630/400000: train loss: 0.159394
epoch:231, batch6, load frames use: 6.51815986633s
step 10631/400000: train loss: 0.153470
epoch:231, batch7, load frames use: 6.2861199379s
step 10632/400000: train loss: 0.137620
epoch:231, batch8, load frames use: 5.36245989799s
step 10633/400000: train loss: 0.121394
epoch:231, batch9, load frames use: 5.26404809952s
step 10634/400000: train loss: 0.148336
epoch:231, batch10, load frames use: 6.0225250721s
step 10635/400000: train loss: 0.136336
epoch:231, batch11, load frames use: 5.98224401474s
step 10636/400000: train loss: 0.130302
epoch:231, batch12, load frames use: 6.22708201408s
step 10637/400000: train loss: 0.129798
epoch:231, batch13, load frames use: 5.54688000679s
step 10638/400000: train loss: 0.136384
epoch:231, batch14, load frames use: 5.68116307259s
step 10639/400000: train loss: 0.125468
epoch:231, batch15, load frames use: 8.51082491875s
step 10640/400000: train loss: 0.125005
epoch:231, batch16, load frames use: 6.44443893433s
step 10641/400000: train loss: 0.136844
epoch:231, batch17, load frames use: 5.36708712578s
step 10642/400000: train loss: 0.135364
epoch:231, batch18, load frames use: 6.46181201935s
step 10643/400000: train loss: 0.140435
epoch:231, batch19, load frames use: 5.48825001717s
step 10644/400000: train loss: 0.142036
epoch:231, batch20, load frames use: 5.85457110405s
step 10645/400000: train loss: 0.147422
epoch:231, batch21, load frames use: 5.59033298492s
step 10646/400000: train loss: 0.134961
epoch:231, batch22, load frames use: 5.60578298569s
step 10647/400000: train loss: 0.136913
epoch:231, batch23, load frames use: 5.98053002357s
step 10648/400000: train loss: 0.152266
epoch:231, batch24, load frames use: 5.46384000778s
step 10649/400000: train loss: 0.158203
epoch:231, batch25, load frames use: 5.87662386894s
step 10650/400000: train loss: 0.133303
epoch:231, batch26, load frames use: 8.55809187889s
step 10651/400000: train loss: 0.135526
epoch:231, batch27, load frames use: 5.82698512077s
step 10652/400000: train loss: 0.147114
epoch:231, batch28, load frames use: 5.13855004311s
step 10653/400000: train loss: 0.134130
epoch:231, batch29, load frames use: 5.16167998314s
step 10654/400000: train loss: 0.158324
epoch:231, batch30, load frames use: 5.96620798111s
step 10655/400000: train loss: 0.143783
epoch:231, batch31, load frames use: 5.6541531086s
step 10656/400000: train loss: 0.134037
epoch:231, batch32, load frames use: 6.53465390205s
step 10657/400000: train loss: 0.141289
epoch:231, batch33, load frames use: 5.79511809349s
step 10658/400000: train loss: 0.141871
epoch:231, batch34, load frames use: 6.15116095543s
step 10659/400000: train loss: 0.148787
epoch:231, batch35, load frames use: 5.39353013039s
step 10660/400000: train loss: 0.129827
epoch:231, batch36, load frames use: 5.57585310936s
step 10661/400000: train loss: 0.138730
epoch:231, batch37, load frames use: 6.12529802322s
step 10662/400000: train loss: 0.151763
epoch:231, batch38, load frames use: 5.51703500748s
step 10663/400000: train loss: 0.152224
epoch:231, batch39, load frames use: 6.03115105629s
step 10664/400000: train loss: 0.158231
epoch:231, batch40, load frames use: 5.577917099s
step 10665/400000: train loss: 0.148740
epoch:231, batch41, load frames use: 6.61038088799s
step 10666/400000: train loss: 0.142585
epoch:231, batch42, load frames use: 6.15654611588s
step 10667/400000: train loss: 0.147536
epoch:231, batch43, load frames use: 6.27801299095s
step 10668/400000: train loss: 0.141923
epoch:231, batch44, load frames use: 6.32838511467s
step 10669/400000: train loss: 0.133412
epoch:231, batch45, load frames use: 6.93676304817s
step 10670/400000: train loss: 0.127950
epoch:231, batch46, load frames use: 5.27613401413s
step 10671/400000: train loss: 0.142329
epoch:232, batch1, load frames use: 6.4508190155s
step 10672/400000: train loss: 0.139810
epoch:232, batch2, load frames use: 8.1232919693s
step 10673/400000: train loss: 0.133083
epoch:232, batch3, load frames use: 6.65294504166s
step 10674/400000: train loss: 0.137505
epoch:232, batch4, load frames use: 6.14710783958s
step 10675/400000: train loss: 0.138308
epoch:232, batch5, load frames use: 6.23501992226s
step 10676/400000: train loss: 0.145915
epoch:232, batch6, load frames use: 6.1734058857s
step 10677/400000: train loss: 0.138564
epoch:232, batch7, load frames use: 5.68835306168s
step 10678/400000: train loss: 0.140340
epoch:232, batch8, load frames use: 5.4137699604s
step 10679/400000: train loss: 0.138058
epoch:232, batch9, load frames use: 5.5208530426s
step 10680/400000: train loss: 0.136563
epoch:232, batch10, load frames use: 5.1456079483s
step 10681/400000: train loss: 0.143188
epoch:232, batch11, load frames use: 5.24396491051s
step 10682/400000: train loss: 0.130611
epoch:232, batch12, load frames use: 5.91044306755s
step 10683/400000: train loss: 0.139323
epoch:232, batch13, load frames use: 7.81182408333s
step 10684/400000: train loss: 0.146321
epoch:232, batch14, load frames use: 5.36507701874s
step 10685/400000: train loss: 0.136629
epoch:232, batch15, load frames use: 5.43533611298s
step 10686/400000: train loss: 0.154126
epoch:232, batch16, load frames use: 4.98846101761s
step 10687/400000: train loss: 0.128632
epoch:232, batch17, load frames use: 5.46818399429s
step 10688/400000: train loss: 0.139775
epoch:232, batch18, load frames use: 5.23329210281s
step 10689/400000: train loss: 0.130965
epoch:232, batch19, load frames use: 6.74020791054s
step 10690/400000: train loss: 0.147101
epoch:232, batch20, load frames use: 6.267578125s
step 10691/400000: train loss: 0.134738
epoch:232, batch21, load frames use: 6.52252197266s
step 10692/400000: train loss: 0.125964
epoch:232, batch22, load frames use: 6.65341520309s
step 10693/400000: train loss: 0.153620
epoch:232, batch23, load frames use: 6.26409983635s
step 10694/400000: train loss: 0.138914
epoch:232, batch24, load frames use: 7.46189785004s
step 10695/400000: train loss: 0.142797
epoch:232, batch25, load frames use: 6.09089303017s
step 10696/400000: train loss: 0.138676
epoch:232, batch26, load frames use: 5.61370110512s
step 10697/400000: train loss: 0.123875
epoch:232, batch27, load frames use: 6.0770368576s
step 10698/400000: train loss: 0.160153
epoch:232, batch28, load frames use: 6.82151293755s
step 10699/400000: train loss: 0.140983
epoch:232, batch29, load frames use: 5.60885381699s
step 10700/400000: train loss: 0.133711
step 10700/400000: validation loss: 0.127204
epoch:232, batch30, load frames use: 5.62414813042s
step 10701/400000: train loss: 0.128885
epoch:232, batch31, load frames use: 6.44619011879s
step 10702/400000: train loss: 0.136250
epoch:232, batch32, load frames use: 5.52415418625s
step 10703/400000: train loss: 0.130106
epoch:232, batch33, load frames use: 5.44647288322s
step 10704/400000: train loss: 0.140613
epoch:232, batch34, load frames use: 6.32756280899s
step 10705/400000: train loss: 0.164446
epoch:232, batch35, load frames use: 7.14271092415s
step 10706/400000: train loss: 0.146846
epoch:232, batch36, load frames use: 5.61182618141s
step 10707/400000: train loss: 0.133047
epoch:232, batch37, load frames use: 6.09110093117s
step 10708/400000: train loss: 0.151580
epoch:232, batch38, load frames use: 6.78331303596s
step 10709/400000: train loss: 0.155835
epoch:232, batch39, load frames use: 5.40587496758s
step 10710/400000: train loss: 0.143063
epoch:232, batch40, load frames use: 6.48515796661s
step 10711/400000: train loss: 0.134924
epoch:232, batch41, load frames use: 6.15885901451s
step 10712/400000: train loss: 0.149764
epoch:232, batch42, load frames use: 6.01206588745s
step 10713/400000: train loss: 0.136823
epoch:232, batch43, load frames use: 6.09482502937s
step 10714/400000: train loss: 0.139288
epoch:232, batch44, load frames use: 6.09219288826s
step 10715/400000: train loss: 0.136800
epoch:232, batch45, load frames use: 5.86030888557s
step 10716/400000: train loss: 0.147194
epoch:232, batch46, load frames use: 6.50851678848s
step 10717/400000: train loss: 0.143313
epoch:233, batch1, load frames use: 5.40543603897s
step 10718/400000: train loss: 0.145896
epoch:233, batch2, load frames use: 6.14594197273s
step 10719/400000: train loss: 0.123057
epoch:233, batch3, load frames use: 5.0191681385s
step 10720/400000: train loss: 0.164862
epoch:233, batch4, load frames use: 6.98642921448s
step 10721/400000: train loss: 0.151307
epoch:233, batch5, load frames use: 5.69886803627s
step 10722/400000: train loss: 0.136824
epoch:233, batch6, load frames use: 6.2014670372s
step 10723/400000: train loss: 0.149397
epoch:233, batch7, load frames use: 6.02626609802s
step 10724/400000: train loss: 0.138334
epoch:233, batch8, load frames use: 6.12221503258s
step 10725/400000: train loss: 0.149511
epoch:233, batch9, load frames use: 5.61764407158s
step 10726/400000: train loss: 0.144803
epoch:233, batch10, load frames use: 8.67107009888s
step 10727/400000: train loss: 0.148548
epoch:233, batch11, load frames use: 5.85365414619s
step 10728/400000: train loss: 0.153546
epoch:233, batch12, load frames use: 5.47060799599s
step 10729/400000: train loss: 0.145326
epoch:233, batch13, load frames use: 6.07108712196s
step 10730/400000: train loss: 0.151817
epoch:233, batch14, load frames use: 4.98538708687s
step 10731/400000: train loss: 0.143264
epoch:233, batch15, load frames use: 5.25514483452s
step 10732/400000: train loss: 0.169516
epoch:233, batch16, load frames use: 5.59792995453s
step 10733/400000: train loss: 0.143449
epoch:233, batch17, load frames use: 5.23376202583s
step 10734/400000: train loss: 0.150675
epoch:233, batch18, load frames use: 5.18508791924s
step 10735/400000: train loss: 0.144697
epoch:233, batch19, load frames use: 5.36979007721s
step 10736/400000: train loss: 0.135020
epoch:233, batch20, load frames use: 5.28661799431s
step 10737/400000: train loss: 0.150995
epoch:233, batch21, load frames use: 4.61114215851s
step 10738/400000: train loss: 0.134272
epoch:233, batch22, load frames use: 5.85763812065s
step 10739/400000: train loss: 0.141164
epoch:233, batch23, load frames use: 5.595703125s
step 10740/400000: train loss: 0.144782
epoch:233, batch24, load frames use: 5.98598384857s
step 10741/400000: train loss: 0.137061
epoch:233, batch25, load frames use: 5.84487819672s
step 10742/400000: train loss: 0.142806
epoch:233, batch26, load frames use: 5.9140958786s
step 10743/400000: train loss: 0.147721
epoch:233, batch27, load frames use: 6.22757601738s
step 10744/400000: train loss: 0.130201
epoch:233, batch28, load frames use: 5.39765691757s
step 10745/400000: train loss: 0.168480
epoch:233, batch29, load frames use: 5.70930099487s
step 10746/400000: train loss: 0.134660
epoch:233, batch30, load frames use: 5.98853206635s
step 10747/400000: train loss: 0.136045
epoch:233, batch31, load frames use: 5.67524600029s
step 10748/400000: train loss: 0.140802
epoch:233, batch32, load frames use: 5.07079815865s
step 10749/400000: train loss: 0.155498
epoch:233, batch33, load frames use: 7.96400809288s
step 10750/400000: train loss: 0.140721
epoch:233, batch34, load frames use: 5.51783394814s
step 10751/400000: train loss: 0.151414
epoch:233, batch35, load frames use: 5.23530197144s
step 10752/400000: train loss: 0.160588
epoch:233, batch36, load frames use: 6.35343408585s
step 10753/400000: train loss: 0.150631
epoch:233, batch37, load frames use: 6.23042106628s
step 10754/400000: train loss: 0.146978
epoch:233, batch38, load frames use: 5.51450204849s
step 10755/400000: train loss: 0.154262
epoch:233, batch39, load frames use: 5.11091709137s
step 10756/400000: train loss: 0.162744
epoch:233, batch40, load frames use: 4.84096598625s
step 10757/400000: train loss: 0.141952
epoch:233, batch41, load frames use: 4.89540195465s
step 10758/400000: train loss: 0.164298
epoch:233, batch42, load frames use: 5.48161005974s
step 10759/400000: train loss: 0.160104
epoch:233, batch43, load frames use: 5.35840606689s
step 10760/400000: train loss: 0.159146
epoch:233, batch44, load frames use: 5.04192900658s
step 10761/400000: train loss: 0.162172
epoch:233, batch45, load frames use: 5.27677893639s
step 10762/400000: train loss: 0.135430
epoch:233, batch46, load frames use: 5.419921875s
step 10763/400000: train loss: 0.147039
epoch:234, batch1, load frames use: 5.73245501518s
step 10764/400000: train loss: 0.135559
epoch:234, batch2, load frames use: 5.37079310417s
step 10765/400000: train loss: 0.139811
epoch:234, batch3, load frames use: 5.41710686684s
step 10766/400000: train loss: 0.136730
epoch:234, batch4, load frames use: 5.94493603706s
step 10767/400000: train loss: 0.134100
epoch:234, batch5, load frames use: 6.89501404762s
step 10768/400000: train loss: 0.136464
epoch:234, batch6, load frames use: 5.73102998734s
step 10769/400000: train loss: 0.143848
epoch:234, batch7, load frames use: 5.68207287788s
step 10770/400000: train loss: 0.162383
epoch:234, batch8, load frames use: 6.63192200661s
step 10771/400000: train loss: 0.124952
epoch:234, batch9, load frames use: 6.57752513885s
step 10772/400000: train loss: 0.139540
epoch:234, batch10, load frames use: 6.32981991768s
step 10773/400000: train loss: 0.151478
epoch:234, batch11, load frames use: 6.72765398026s
step 10774/400000: train loss: 0.136430
epoch:234, batch12, load frames use: 6.15108013153s
step 10775/400000: train loss: 0.128181
epoch:234, batch13, load frames use: 5.60665297508s
step 10776/400000: train loss: 0.133832
epoch:234, batch14, load frames use: 6.41559696198s
step 10777/400000: train loss: 0.145388
epoch:234, batch15, load frames use: 7.14514017105s
step 10778/400000: train loss: 0.147629
epoch:234, batch16, load frames use: 7.3160378933s
step 10779/400000: train loss: 0.123779
epoch:234, batch17, load frames use: 5.88173604012s
step 10780/400000: train loss: 0.131384
epoch:234, batch18, load frames use: 5.74536299706s
step 10781/400000: train loss: 0.135600
epoch:234, batch19, load frames use: 5.77940893173s
step 10782/400000: train loss: 0.142018
epoch:234, batch20, load frames use: 8.12701916695s
step 10783/400000: train loss: 0.151007
epoch:234, batch21, load frames use: 5.98578000069s
step 10784/400000: train loss: 0.142169
epoch:234, batch22, load frames use: 6.81539201736s
step 10785/400000: train loss: 0.142407
epoch:234, batch23, load frames use: 5.99682402611s
step 10786/400000: train loss: 0.143672
epoch:234, batch24, load frames use: 5.79134297371s
step 10787/400000: train loss: 0.160800
epoch:234, batch25, load frames use: 5.76215910912s
step 10788/400000: train loss: 0.141994
epoch:234, batch26, load frames use: 6.79864907265s
step 10789/400000: train loss: 0.147296
epoch:234, batch27, load frames use: 6.67092490196s
step 10790/400000: train loss: 0.150113
epoch:234, batch28, load frames use: 5.41050219536s
step 10791/400000: train loss: 0.144757
epoch:234, batch29, load frames use: 6.32966017723s
step 10792/400000: train loss: 0.161000
epoch:234, batch30, load frames use: 6.16397690773s
step 10793/400000: train loss: 0.146245
epoch:234, batch31, load frames use: 6.27159690857s
step 10794/400000: train loss: 0.135842
epoch:234, batch32, load frames use: 5.51339292526s
step 10795/400000: train loss: 0.151532
epoch:234, batch33, load frames use: 5.86233901978s
step 10796/400000: train loss: 0.143848
epoch:234, batch34, load frames use: 6.12618994713s
step 10797/400000: train loss: 0.131040
epoch:234, batch35, load frames use: 5.79039001465s
step 10798/400000: train loss: 0.146739
epoch:234, batch36, load frames use: 8.34652400017s
step 10799/400000: train loss: 0.125490
epoch:234, batch37, load frames use: 6.26089382172s
step 10800/400000: train loss: 0.131915
step 10800/400000: validation loss: 0.136441
epoch:234, batch38, load frames use: 5.71393609047s
step 10801/400000: train loss: 0.134022
epoch:234, batch39, load frames use: 7.28347301483s
step 10802/400000: train loss: 0.147889
epoch:234, batch40, load frames use: 5.14981412888s
step 10803/400000: train loss: 0.150706
epoch:234, batch41, load frames use: 6.01614904404s
step 10804/400000: train loss: 0.142311
epoch:234, batch42, load frames use: 5.53220391273s
step 10805/400000: train loss: 0.134417
epoch:234, batch43, load frames use: 5.058177948s
step 10806/400000: train loss: 0.125904
epoch:234, batch44, load frames use: 5.6984629631s
step 10807/400000: train loss: 0.157427
epoch:234, batch45, load frames use: 5.63989901543s
step 10808/400000: train loss: 0.144610
epoch:234, batch46, load frames use: 5.82634711266s
step 10809/400000: train loss: 0.140346
epoch:235, batch1, load frames use: 7.29674983025s
step 10810/400000: train loss: 0.153206
epoch:235, batch2, load frames use: 6.64270114899s
step 10811/400000: train loss: 0.131185
epoch:235, batch3, load frames use: 5.80018806458s
step 10812/400000: train loss: 0.136455
epoch:235, batch4, load frames use: 6.44962596893s
step 10813/400000: train loss: 0.127996
epoch:235, batch5, load frames use: 6.44159698486s
step 10814/400000: train loss: 0.143864
epoch:235, batch6, load frames use: 6.33539581299s
step 10815/400000: train loss: 0.135563
epoch:235, batch7, load frames use: 6.0903699398s
step 10816/400000: train loss: 0.134442
epoch:235, batch8, load frames use: 6.06604909897s
step 10817/400000: train loss: 0.154594
epoch:235, batch9, load frames use: 6.4006319046s
step 10818/400000: train loss: 0.135231
epoch:235, batch10, load frames use: 6.62159204483s
step 10819/400000: train loss: 0.137915
epoch:235, batch11, load frames use: 5.63887906075s
step 10820/400000: train loss: 0.113131
epoch:235, batch12, load frames use: 5.64710307121s
step 10821/400000: train loss: 0.129192
epoch:235, batch13, load frames use: 5.23288583755s
step 10822/400000: train loss: 0.141950
epoch:235, batch14, load frames use: 5.08230495453s
step 10823/400000: train loss: 0.137390
epoch:235, batch15, load frames use: 5.98434615135s
step 10824/400000: train loss: 0.139567
epoch:235, batch16, load frames use: 5.88206505775s
step 10825/400000: train loss: 0.140841
epoch:235, batch17, load frames use: 5.285820961s
step 10826/400000: train loss: 0.130683
epoch:235, batch18, load frames use: 5.06006407738s
step 10827/400000: train loss: 0.145473
epoch:235, batch19, load frames use: 5.82284998894s
step 10828/400000: train loss: 0.142140
epoch:235, batch20, load frames use: 6.87317419052s
step 10829/400000: train loss: 0.135594
epoch:235, batch21, load frames use: 5.65051412582s
step 10830/400000: train loss: 0.134088
epoch:235, batch22, load frames use: 5.67081904411s
step 10831/400000: train loss: 0.141178
epoch:235, batch23, load frames use: 8.84938001633s
step 10832/400000: train loss: 0.144133
epoch:235, batch24, load frames use: 5.20291304588s
step 10833/400000: train loss: 0.129348
epoch:235, batch25, load frames use: 5.56299591064s
step 10834/400000: train loss: 0.138667
epoch:235, batch26, load frames use: 6.47749090195s
step 10835/400000: train loss: 0.138486
epoch:235, batch27, load frames use: 6.12455821037s
step 10836/400000: train loss: 0.144697
epoch:235, batch28, load frames use: 5.62689185143s
step 10837/400000: train loss: 0.124983
epoch:235, batch29, load frames use: 5.39331507683s
step 10838/400000: train loss: 0.136794
epoch:235, batch30, load frames use: 5.65875387192s
step 10839/400000: train loss: 0.144999
epoch:235, batch31, load frames use: 5.68304920197s
step 10840/400000: train loss: 0.132162
epoch:235, batch32, load frames use: 6.06591796875s
step 10841/400000: train loss: 0.130082
epoch:235, batch33, load frames use: 5.93611979485s
step 10842/400000: train loss: 0.134076
epoch:235, batch34, load frames use: 7.71141600609s
step 10843/400000: train loss: 0.126759
epoch:235, batch35, load frames use: 6.11380720139s
step 10844/400000: train loss: 0.132312
epoch:235, batch36, load frames use: 6.78359508514s
step 10845/400000: train loss: 0.137954
epoch:235, batch37, load frames use: 6.37276291847s
step 10846/400000: train loss: 0.129181
epoch:235, batch38, load frames use: 5.87491583824s
step 10847/400000: train loss: 0.133853
epoch:235, batch39, load frames use: 5.78945493698s
step 10848/400000: train loss: 0.159429
epoch:235, batch40, load frames use: 6.15433597565s
step 10849/400000: train loss: 0.130606
epoch:235, batch41, load frames use: 5.70504403114s
step 10850/400000: train loss: 0.137036
epoch:235, batch42, load frames use: 6.41514587402s
step 10851/400000: train loss: 0.134155
epoch:235, batch43, load frames use: 6.43481111526s
step 10852/400000: train loss: 0.153491
epoch:235, batch44, load frames use: 5.54329299927s
step 10853/400000: train loss: 0.150156
epoch:235, batch45, load frames use: 6.49707007408s
step 10854/400000: train loss: 0.147601
epoch:235, batch46, load frames use: 5.90760588646s
step 10855/400000: train loss: 0.134990
epoch:236, batch1, load frames use: 6.00490021706s
step 10856/400000: train loss: 0.133253
epoch:236, batch2, load frames use: 6.35945701599s
step 10857/400000: train loss: 0.146849
epoch:236, batch3, load frames use: 5.70988702774s
step 10858/400000: train loss: 0.146253
epoch:236, batch4, load frames use: 7.43333816528s
step 10859/400000: train loss: 0.143255
epoch:236, batch5, load frames use: 5.66422581673s
step 10860/400000: train loss: 0.133061
epoch:236, batch6, load frames use: 6.40547013283s
step 10861/400000: train loss: 0.124668
epoch:236, batch7, load frames use: 6.23072600365s
step 10862/400000: train loss: 0.135096
epoch:236, batch8, load frames use: 6.3305850029s
step 10863/400000: train loss: 0.128343
epoch:236, batch9, load frames use: 6.07443404198s
step 10864/400000: train loss: 0.157231
epoch:236, batch10, load frames use: 6.21612501144s
step 10865/400000: train loss: 0.137895
epoch:236, batch11, load frames use: 6.13075184822s
step 10866/400000: train loss: 0.127069
epoch:236, batch12, load frames use: 5.77260994911s
step 10867/400000: train loss: 0.124235
epoch:236, batch13, load frames use: 6.89446997643s
step 10868/400000: train loss: 0.143725
epoch:236, batch14, load frames use: 5.99958610535s
step 10869/400000: train loss: 0.133332
epoch:236, batch15, load frames use: 6.78796005249s
step 10870/400000: train loss: 0.138961
epoch:236, batch16, load frames use: 5.5863699913s
step 10871/400000: train loss: 0.141646
epoch:236, batch17, load frames use: 6.47822999954s
step 10872/400000: train loss: 0.135345
epoch:236, batch18, load frames use: 6.35131502151s
step 10873/400000: train loss: 0.131906
epoch:236, batch19, load frames use: 5.8709859848s
step 10874/400000: train loss: 0.122183
epoch:236, batch20, load frames use: 8.20570588112s
step 10875/400000: train loss: 0.126204
epoch:236, batch21, load frames use: 4.9409430027s
step 10876/400000: train loss: 0.140312
epoch:236, batch22, load frames use: 6.05961418152s
step 10877/400000: train loss: 0.133101
epoch:236, batch23, load frames use: 6.24618411064s
step 10878/400000: train loss: 0.141493
epoch:236, batch24, load frames use: 6.00885391235s
step 10879/400000: train loss: 0.143198
epoch:236, batch25, load frames use: 5.65265798569s
step 10880/400000: train loss: 0.159708
epoch:236, batch26, load frames use: 5.75957298279s
step 10881/400000: train loss: 0.139101
epoch:236, batch27, load frames use: 5.77945399284s
step 10882/400000: train loss: 0.140645
epoch:236, batch28, load frames use: 6.53652596474s
step 10883/400000: train loss: 0.146698
epoch:236, batch29, load frames use: 5.83327794075s
step 10884/400000: train loss: 0.123990
epoch:236, batch30, load frames use: 6.78065109253s
step 10885/400000: train loss: 0.115450
epoch:236, batch31, load frames use: 8.30923700333s
step 10886/400000: train loss: 0.122320
epoch:236, batch32, load frames use: 5.73821687698s
step 10887/400000: train loss: 0.126502
epoch:236, batch33, load frames use: 6.22312998772s
step 10888/400000: train loss: 0.146149
epoch:236, batch34, load frames use: 6.01415014267s
step 10889/400000: train loss: 0.125931
epoch:236, batch35, load frames use: 4.71597003937s
step 10890/400000: train loss: 0.138176
epoch:236, batch36, load frames use: 5.26134991646s
step 10891/400000: train loss: 0.138512
epoch:236, batch37, load frames use: 6.11090898514s
step 10892/400000: train loss: 0.147056
epoch:236, batch38, load frames use: 5.44292902946s
step 10893/400000: train loss: 0.136857
epoch:236, batch39, load frames use: 5.58959889412s
step 10894/400000: train loss: 0.140889
epoch:236, batch40, load frames use: 5.37260890007s
step 10895/400000: train loss: 0.130762
epoch:236, batch41, load frames use: 5.38500213623s
step 10896/400000: train loss: 0.152387
epoch:236, batch42, load frames use: 5.24770903587s
step 10897/400000: train loss: 0.151381
epoch:236, batch43, load frames use: 6.24805283546s
step 10898/400000: train loss: 0.150316
epoch:236, batch44, load frames use: 5.47608208656s
step 10899/400000: train loss: 0.150324
epoch:236, batch45, load frames use: 6.24787187576s
step 10900/400000: train loss: 0.139427
step 10900/400000: validation loss: 0.136418
epoch:236, batch46, load frames use: 5.8393778801s
step 10901/400000: train loss: 0.144992
epoch:237, batch1, load frames use: 6.32060408592s
step 10902/400000: train loss: 0.133884
epoch:237, batch2, load frames use: 7.57376503944s
step 10903/400000: train loss: 0.132056
epoch:237, batch3, load frames use: 6.42706298828s
step 10904/400000: train loss: 0.143589
epoch:237, batch4, load frames use: 6.33302092552s
step 10905/400000: train loss: 0.132064
epoch:237, batch5, load frames use: 6.0565071106s
step 10906/400000: train loss: 0.148504
epoch:237, batch6, load frames use: 5.84035205841s
step 10907/400000: train loss: 0.144830
epoch:237, batch7, load frames use: 7.16057515144s
step 10908/400000: train loss: 0.142660
epoch:237, batch8, load frames use: 5.64839506149s
step 10909/400000: train loss: 0.128692
epoch:237, batch9, load frames use: 5.15053105354s
step 10910/400000: train loss: 0.150427
epoch:237, batch10, load frames use: 5.69401097298s
step 10911/400000: train loss: 0.134113
epoch:237, batch11, load frames use: 6.37647414207s
step 10912/400000: train loss: 0.126839
epoch:237, batch12, load frames use: 6.49591112137s
step 10913/400000: train loss: 0.129475
epoch:237, batch13, load frames use: 5.56365799904s
step 10914/400000: train loss: 0.133594
epoch:237, batch14, load frames use: 6.37760090828s
step 10915/400000: train loss: 0.128945
epoch:237, batch15, load frames use: 5.33965611458s
step 10916/400000: train loss: 0.147829
epoch:237, batch16, load frames use: 5.94541692734s
step 10917/400000: train loss: 0.128908
epoch:237, batch17, load frames use: 6.36631989479s
step 10918/400000: train loss: 0.138190
epoch:237, batch18, load frames use: 6.85994696617s
step 10919/400000: train loss: 0.129186
epoch:237, batch19, load frames use: 5.55784296989s
step 10920/400000: train loss: 0.145909
epoch:237, batch20, load frames use: 6.12464499474s
step 10921/400000: train loss: 0.134100
epoch:237, batch21, load frames use: 5.6558880806s
step 10922/400000: train loss: 0.138400
epoch:237, batch22, load frames use: 6.03595495224s
step 10923/400000: train loss: 0.129900
epoch:237, batch23, load frames use: 5.63360691071s
step 10924/400000: train loss: 0.116368
epoch:237, batch24, load frames use: 4.75738787651s
step 10925/400000: train loss: 0.137918
epoch:237, batch25, load frames use: 5.15327000618s
step 10926/400000: train loss: 0.140524
epoch:237, batch26, load frames use: 5.95891308784s
step 10927/400000: train loss: 0.137505
epoch:237, batch27, load frames use: 6.85335087776s
step 10928/400000: train loss: 0.133639
epoch:237, batch28, load frames use: 6.58128213882s
step 10929/400000: train loss: 0.122542
epoch:237, batch29, load frames use: 5.23412203789s
step 10930/400000: train loss: 0.137669
epoch:237, batch30, load frames use: 5.78229498863s
step 10931/400000: train loss: 0.137875
epoch:237, batch31, load frames use: 6.79946398735s
step 10932/400000: train loss: 0.146026
epoch:237, batch32, load frames use: 5.1376991272s
step 10933/400000: train loss: 0.129502
epoch:237, batch33, load frames use: 6.17893910408s
step 10934/400000: train loss: 0.145153
epoch:237, batch34, load frames use: 5.93615698814s
step 10935/400000: train loss: 0.141989
epoch:237, batch35, load frames use: 5.80645108223s
step 10936/400000: train loss: 0.130786
epoch:237, batch36, load frames use: 5.65587210655s
step 10937/400000: train loss: 0.161561
epoch:237, batch37, load frames use: 6.36324095726s
step 10938/400000: train loss: 0.153362
epoch:237, batch38, load frames use: 5.93606686592s
step 10939/400000: train loss: 0.133286
epoch:237, batch39, load frames use: 5.80865812302s
step 10940/400000: train loss: 0.144819
epoch:237, batch40, load frames use: 7.06182909012s
step 10941/400000: train loss: 0.150397
epoch:237, batch41, load frames use: 5.01224684715s
step 10942/400000: train loss: 0.122093
epoch:237, batch42, load frames use: 6.67432713509s
step 10943/400000: train loss: 0.148980
epoch:237, batch43, load frames use: 5.4745490551s
step 10944/400000: train loss: 0.131548
epoch:237, batch44, load frames use: 5.3368370533s
step 10945/400000: train loss: 0.133968
epoch:237, batch45, load frames use: 5.18498110771s
step 10946/400000: train loss: 0.160764
epoch:237, batch46, load frames use: 7.16277098656s
step 10947/400000: train loss: 0.133268
epoch:238, batch1, load frames use: 6.20828199387s
step 10948/400000: train loss: 0.131179
epoch:238, batch2, load frames use: 6.14361310005s
step 10949/400000: train loss: 0.126531
epoch:238, batch3, load frames use: 6.43480110168s
step 10950/400000: train loss: 0.135964
epoch:238, batch4, load frames use: 6.82095599174s
step 10951/400000: train loss: 0.130631
epoch:238, batch5, load frames use: 5.59540200233s
step 10952/400000: train loss: 0.131741
epoch:238, batch6, load frames use: 6.1165561676s
step 10953/400000: train loss: 0.145938
epoch:238, batch7, load frames use: 6.28028702736s
step 10954/400000: train loss: 0.138008
epoch:238, batch8, load frames use: 5.76154088974s
step 10955/400000: train loss: 0.128590
epoch:238, batch9, load frames use: 5.8560860157s
step 10956/400000: train loss: 0.129811
epoch:238, batch10, load frames use: 6.18168711662s
step 10957/400000: train loss: 0.136521
epoch:238, batch11, load frames use: 6.08698987961s
step 10958/400000: train loss: 0.146940
epoch:238, batch12, load frames use: 6.50201892853s
step 10959/400000: train loss: 0.147939
epoch:238, batch13, load frames use: 6.60854005814s
step 10960/400000: train loss: 0.139216
epoch:238, batch14, load frames use: 5.69167089462s
step 10961/400000: train loss: 0.142552
epoch:238, batch15, load frames use: 5.6265540123s
step 10962/400000: train loss: 0.135576
epoch:238, batch16, load frames use: 6.77628684044s
step 10963/400000: train loss: 0.137800
epoch:238, batch17, load frames use: 6.12120103836s
step 10964/400000: train loss: 0.154519
epoch:238, batch18, load frames use: 6.028028965s
step 10965/400000: train loss: 0.139026
epoch:238, batch19, load frames use: 6.39521694183s
step 10966/400000: train loss: 0.144698
epoch:238, batch20, load frames use: 5.65233302116s
step 10967/400000: train loss: 0.152258
epoch:238, batch21, load frames use: 5.98603200912s
step 10968/400000: train loss: 0.151422
epoch:238, batch22, load frames use: 5.588534832s
step 10969/400000: train loss: 0.144567
epoch:238, batch23, load frames use: 6.44286084175s
step 10970/400000: train loss: 0.164252
epoch:238, batch24, load frames use: 5.48824310303s
step 10971/400000: train loss: 0.154418
epoch:238, batch25, load frames use: 5.85476899147s
step 10972/400000: train loss: 0.140689
epoch:238, batch26, load frames use: 5.97094511986s
step 10973/400000: train loss: 0.137123
epoch:238, batch27, load frames use: 7.40331983566s
step 10974/400000: train loss: 0.141340
epoch:238, batch28, load frames use: 6.07788705826s
step 10975/400000: train loss: 0.136929
epoch:238, batch29, load frames use: 6.32935214043s
step 10976/400000: train loss: 0.135434
epoch:238, batch30, load frames use: 5.80701303482s
step 10977/400000: train loss: 0.158512
epoch:238, batch31, load frames use: 5.63451695442s
step 10978/400000: train loss: 0.139928
epoch:238, batch32, load frames use: 5.21920084953s
step 10979/400000: train loss: 0.133420
epoch:238, batch33, load frames use: 6.15775918961s
step 10980/400000: train loss: 0.150487
epoch:238, batch34, load frames use: 7.1976518631s
step 10981/400000: train loss: 0.140317
epoch:238, batch35, load frames use: 5.02584695816s
step 10982/400000: train loss: 0.139528
epoch:238, batch36, load frames use: 5.3913769722s
step 10983/400000: train loss: 0.148339
epoch:238, batch37, load frames use: 6.08226919174s
step 10984/400000: train loss: 0.146400
epoch:238, batch38, load frames use: 6.18473005295s
step 10985/400000: train loss: 0.152905
epoch:238, batch39, load frames use: 5.70964193344s
step 10986/400000: train loss: 0.145170
epoch:238, batch40, load frames use: 5.73973894119s
step 10987/400000: train loss: 0.138326
epoch:238, batch41, load frames use: 7.18865704536s
step 10988/400000: train loss: 0.142841
epoch:238, batch42, load frames use: 6.24586415291s
step 10989/400000: train loss: 0.143162
epoch:238, batch43, load frames use: 5.09116387367s
step 10990/400000: train loss: 0.143608
epoch:238, batch44, load frames use: 5.22176504135s
step 10991/400000: train loss: 0.150916
epoch:238, batch45, load frames use: 5.23111104965s
step 10992/400000: train loss: 0.150982
epoch:238, batch46, load frames use: 5.221118927s
step 10993/400000: train loss: 0.141601
epoch:239, batch1, load frames use: 5.11351799965s
step 10994/400000: train loss: 0.141552
epoch:239, batch2, load frames use: 5.79504799843s
step 10995/400000: train loss: 0.120718
epoch:239, batch3, load frames use: 5.75406885147s
step 10996/400000: train loss: 0.147796
epoch:239, batch4, load frames use: 6.71231102943s
step 10997/400000: train loss: 0.137394
epoch:239, batch5, load frames use: 5.71193313599s
step 10998/400000: train loss: 0.138054
epoch:239, batch6, load frames use: 5.89908909798s
step 10999/400000: train loss: 0.143342
epoch:239, batch7, load frames use: 5.34569811821s
step 11000/400000: train loss: 0.135956
step 11000/400000: validation loss: 0.151096
epoch:239, batch8, load frames use: 8.47728204727s
step 11001/400000: train loss: 0.111885
epoch:239, batch9, load frames use: 5.53345394135s
step 11002/400000: train loss: 0.129683
epoch:239, batch10, load frames use: 5.50478100777s
step 11003/400000: train loss: 0.134598
epoch:239, batch11, load frames use: 5.49033379555s
step 11004/400000: train loss: 0.126732
epoch:239, batch12, load frames use: 5.34995794296s
step 11005/400000: train loss: 0.137580
epoch:239, batch13, load frames use: 5.26097893715s
step 11006/400000: train loss: 0.136925
epoch:239, batch14, load frames use: 5.4564011097s
step 11007/400000: train loss: 0.144190
epoch:239, batch15, load frames use: 5.29303812981s
step 11008/400000: train loss: 0.126469
epoch:239, batch16, load frames use: 5.30275797844s
step 11009/400000: train loss: 0.140879
epoch:239, batch17, load frames use: 5.89757204056s
step 11010/400000: train loss: 0.146098
epoch:239, batch18, load frames use: 5.47098302841s
step 11011/400000: train loss: 0.154273
epoch:239, batch19, load frames use: 5.08937692642s
step 11012/400000: train loss: 0.128676
epoch:239, batch20, load frames use: 5.58450603485s
step 11013/400000: train loss: 0.138023
epoch:239, batch21, load frames use: 6.75900483131s
step 11014/400000: train loss: 0.139025
epoch:239, batch22, load frames use: 6.64397907257s
step 11015/400000: train loss: 0.136212
epoch:239, batch23, load frames use: 5.88048696518s
step 11016/400000: train loss: 0.122453
epoch:239, batch24, load frames use: 5.73136210442s
step 11017/400000: train loss: 0.126425
epoch:239, batch25, load frames use: 6.4202311039s
step 11018/400000: train loss: 0.131800
epoch:239, batch26, load frames use: 6.17313885689s
step 11019/400000: train loss: 0.134503
epoch:239, batch27, load frames use: 6.20949196815s
step 11020/400000: train loss: 0.150044
epoch:239, batch28, load frames use: 6.1198091507s
step 11021/400000: train loss: 0.137385
epoch:239, batch29, load frames use: 5.65059900284s
step 11022/400000: train loss: 0.146636
epoch:239, batch30, load frames use: 6.23975110054s
step 11023/400000: train loss: 0.152301
epoch:239, batch31, load frames use: 5.46837496758s
step 11024/400000: train loss: 0.148868
epoch:239, batch32, load frames use: 6.35100317001s
step 11025/400000: train loss: 0.136466
epoch:239, batch33, load frames use: 6.48593997955s
step 11026/400000: train loss: 0.149398
epoch:239, batch34, load frames use: 6.2764878273s
step 11027/400000: train loss: 0.134326
epoch:239, batch35, load frames use: 6.01796007156s
step 11028/400000: train loss: 0.137285
epoch:239, batch36, load frames use: 8.03445601463s
step 11029/400000: train loss: 0.159169
epoch:239, batch37, load frames use: 6.66120290756s
step 11030/400000: train loss: 0.116962
epoch:239, batch38, load frames use: 5.96070194244s
step 11031/400000: train loss: 0.132869
epoch:239, batch39, load frames use: 5.78049206734s
step 11032/400000: train loss: 0.143524
epoch:239, batch40, load frames use: 6.2569770813s
step 11033/400000: train loss: 0.135261
epoch:239, batch41, load frames use: 5.35285997391s
step 11034/400000: train loss: 0.138941
epoch:239, batch42, load frames use: 6.86525988579s
step 11035/400000: train loss: 0.155138
epoch:239, batch43, load frames use: 5.95296406746s
step 11036/400000: train loss: 0.159592
epoch:239, batch44, load frames use: 5.49947285652s
step 11037/400000: train loss: 0.139643
epoch:239, batch45, load frames use: 5.72758507729s
step 11038/400000: train loss: 0.140461
epoch:239, batch46, load frames use: 6.5921869278s
step 11039/400000: train loss: 0.143679
epoch:240, batch1, load frames use: 5.7702589035s
step 11040/400000: train loss: 0.129146
epoch:240, batch2, load frames use: 5.27109098434s
step 11041/400000: train loss: 0.133974
epoch:240, batch3, load frames use: 6.80661606789s
step 11042/400000: train loss: 0.141916
epoch:240, batch4, load frames use: 5.83499383926s
step 11043/400000: train loss: 0.134970
epoch:240, batch5, load frames use: 5.7591149807s
step 11044/400000: train loss: 0.114653
epoch:240, batch6, load frames use: 7.54042220116s
step 11045/400000: train loss: 0.133859
epoch:240, batch7, load frames use: 5.99249982834s
step 11046/400000: train loss: 0.161048
epoch:240, batch8, load frames use: 5.56830596924s
step 11047/400000: train loss: 0.134410
epoch:240, batch9, load frames use: 6.36238908768s
step 11048/400000: train loss: 0.131807
epoch:240, batch10, load frames use: 5.44914007187s
step 11049/400000: train loss: 0.131408
epoch:240, batch11, load frames use: 5.65086698532s
step 11050/400000: train loss: 0.147748
epoch:240, batch12, load frames use: 5.46176981926s
step 11051/400000: train loss: 0.150435
epoch:240, batch13, load frames use: 6.21143698692s
step 11052/400000: train loss: 0.134246
epoch:240, batch14, load frames use: 6.43119096756s
step 11053/400000: train loss: 0.121504
epoch:240, batch15, load frames use: 5.9967019558s
step 11054/400000: train loss: 0.129068
epoch:240, batch16, load frames use: 5.89541912079s
step 11055/400000: train loss: 0.128790
epoch:240, batch17, load frames use: 7.40959405899s
step 11056/400000: train loss: 0.148623
epoch:240, batch18, load frames use: 5.75423908234s
step 11057/400000: train loss: 0.129176
epoch:240, batch19, load frames use: 6.00564312935s
step 11058/400000: train loss: 0.141863
epoch:240, batch20, load frames use: 5.83992600441s
step 11059/400000: train loss: 0.140037
epoch:240, batch21, load frames use: 5.77473092079s
step 11060/400000: train loss: 0.119457
epoch:240, batch22, load frames use: 6.13729095459s
step 11061/400000: train loss: 0.142264
epoch:240, batch23, load frames use: 6.74715805054s
step 11062/400000: train loss: 0.156248
epoch:240, batch24, load frames use: 5.70882105827s
step 11063/400000: train loss: 0.119457
epoch:240, batch25, load frames use: 5.40304589272s
step 11064/400000: train loss: 0.140184
epoch:240, batch26, load frames use: 6.04575610161s
step 11065/400000: train loss: 0.137063
epoch:240, batch27, load frames use: 5.69169902802s
step 11066/400000: train loss: 0.141434
epoch:240, batch28, load frames use: 6.26362919807s
step 11067/400000: train loss: 0.143205
epoch:240, batch29, load frames use: 5.85850596428s
step 11068/400000: train loss: 0.140524
epoch:240, batch30, load frames use: 7.01674890518s
step 11069/400000: train loss: 0.143685
epoch:240, batch31, load frames use: 5.77381706238s
step 11070/400000: train loss: 0.131679
epoch:240, batch32, load frames use: 6.21445584297s
step 11071/400000: train loss: 0.135794
epoch:240, batch33, load frames use: 7.54904007912s
step 11072/400000: train loss: 0.143305
epoch:240, batch34, load frames use: 6.27574396133s
step 11073/400000: train loss: 0.142304
epoch:240, batch35, load frames use: 6.16564798355s
step 11074/400000: train loss: 0.135850
epoch:240, batch36, load frames use: 5.22545790672s
step 11075/400000: train loss: 0.138806
epoch:240, batch37, load frames use: 5.08286190033s
step 11076/400000: train loss: 0.124726
epoch:240, batch38, load frames use: 5.90549302101s
step 11077/400000: train loss: 0.138424
epoch:240, batch39, load frames use: 5.70643997192s
step 11078/400000: train loss: 0.136766
epoch:240, batch40, load frames use: 6.13834881783s
step 11079/400000: train loss: 0.122252
epoch:240, batch41, load frames use: 5.52074599266s
step 11080/400000: train loss: 0.139600
epoch:240, batch42, load frames use: 5.94808793068s
step 11081/400000: train loss: 0.153964
epoch:240, batch43, load frames use: 6.56441497803s
step 11082/400000: train loss: 0.132407
epoch:240, batch44, load frames use: 6.915776968s
step 11083/400000: train loss: 0.139382
epoch:240, batch45, load frames use: 6.52679586411s
step 11084/400000: train loss: 0.119588
epoch:240, batch46, load frames use: 5.28611302376s
step 11085/400000: train loss: 0.140245
epoch:241, batch1, load frames use: 6.23303294182s
step 11086/400000: train loss: 0.131909
epoch:241, batch2, load frames use: 5.25657296181s
step 11087/400000: train loss: 0.131906
epoch:241, batch3, load frames use: 5.63910508156s
step 11088/400000: train loss: 0.135470
epoch:241, batch4, load frames use: 5.09430098534s
step 11089/400000: train loss: 0.133678
epoch:241, batch5, load frames use: 6.50965619087s
step 11090/400000: train loss: 0.124939
epoch:241, batch6, load frames use: 6.48268008232s
step 11091/400000: train loss: 0.133373
epoch:241, batch7, load frames use: 6.08563089371s
step 11092/400000: train loss: 0.146117
epoch:241, batch8, load frames use: 6.01396417618s
step 11093/400000: train loss: 0.133006
epoch:241, batch9, load frames use: 5.72697496414s
step 11094/400000: train loss: 0.139645
epoch:241, batch10, load frames use: 6.78958296776s
step 11095/400000: train loss: 0.135064
epoch:241, batch11, load frames use: 6.48106002808s
step 11096/400000: train loss: 0.153434
epoch:241, batch12, load frames use: 6.62522697449s
step 11097/400000: train loss: 0.135826
epoch:241, batch13, load frames use: 6.40006494522s
step 11098/400000: train loss: 0.149525
epoch:241, batch14, load frames use: 5.81418609619s
step 11099/400000: train loss: 0.139477
epoch:241, batch15, load frames use: 5.86173391342s
step 11100/400000: train loss: 0.132814
step 11100/400000: validation loss: 0.120801
2018-04-17 23:41:30.805951: E tensorflow/core/util/events_writer.cc:162] The events file /data/junting/DL4CV_COMSW4995_006/final_project/models/log_0.05_2500_0.96/events.out.tfevents.1523977877.pineapple has disappeared.
2018-04-17 23:41:30.806013: E tensorflow/core/util/events_writer.cc:131] Failed to flush 1 events to /data/junting/DL4CV_COMSW4995_006/final_project/models/log_0.05_2500_0.96/events.out.tfevents.1523977877.pineapple
epoch:241, batch16, load frames use: 6.58200907707s
step 11101/400000: train loss: 0.141343
epoch:241, batch17, load frames use: 5.92583990097s
step 11102/400000: train loss: 0.137001
epoch:241, batch18, load frames use: 5.94096016884s
step 11103/400000: train loss: 0.136936
epoch:241, batch19, load frames use: 5.96223187447s
step 11104/400000: train loss: 0.145611
epoch:241, batch20, load frames use: 6.47692108154s
step 11105/400000: train loss: 0.122073
epoch:241, batch21, load frames use: 7.40576815605s
step 11106/400000: train loss: 0.132747
epoch:241, batch22, load frames use: 6.77353715897s
step 11107/400000: train loss: 0.156937
epoch:241, batch23, load frames use: 5.3951010704s
step 11108/400000: train loss: 0.145113
epoch:241, batch24, load frames use: 7.16586685181s
step 11109/400000: train loss: 0.159485
epoch:241, batch25, load frames use: 5.85081195831s
step 11110/400000: train loss: 0.138977
epoch:241, batch26, load frames use: 5.74033880234s
step 11111/400000: train loss: 0.150416
epoch:241, batch27, load frames use: 6.43544387817s
step 11112/400000: train loss: 0.145442
epoch:241, batch28, load frames use: 5.54884505272s
step 11113/400000: train loss: 0.137121
epoch:241, batch29, load frames use: 5.36305713654s
step 11114/400000: train loss: 0.148915
epoch:241, batch30, load frames use: 5.85376286507s
step 11115/400000: train loss: 0.151305
epoch:241, batch31, load frames use: 5.81113910675s
step 11116/400000: train loss: 0.148723
epoch:241, batch32, load frames use: 5.95393610001s
step 11117/400000: train loss: 0.151210
epoch:241, batch33, load frames use: 6.18197584152s
step 11118/400000: train loss: 0.153153
epoch:241, batch34, load frames use: 6.12171912193s
step 11119/400000: train loss: 0.136759
epoch:241, batch35, load frames use: 5.46262288094s
step 11120/400000: train loss: 0.149624
epoch:241, batch36, load frames use: 5.55872297287s
step 11121/400000: train loss: 0.138395
epoch:241, batch37, load frames use: 5.34770607948s
step 11122/400000: train loss: 0.149045
epoch:241, batch38, load frames use: 4.89252591133s
step 11123/400000: train loss: 0.146687
epoch:241, batch39, load frames use: 5.34721493721s
step 11124/400000: train loss: 0.146943
epoch:241, batch40, load frames use: 5.49528288841s
step 11125/400000: train loss: 0.140928
epoch:241, batch41, load frames use: 5.97332692146s
step 11126/400000: train loss: 0.141573
epoch:241, batch42, load frames use: 6.15969395638s
step 11127/400000: train loss: 0.148273
epoch:241, batch43, load frames use: 7.96608400345s
step 11128/400000: train loss: 0.162990
epoch:241, batch44, load frames use: 6.48820614815s
step 11129/400000: train loss: 0.143869
epoch:241, batch45, load frames use: 6.28295111656s
step 11130/400000: train loss: 0.133484
epoch:241, batch46, load frames use: 5.96191883087s
step 11131/400000: train loss: 0.132128
epoch:242, batch1, load frames use: 6.37016296387s
step 11132/400000: train loss: 0.131200
epoch:242, batch2, load frames use: 8.01157093048s
step 11133/400000: train loss: 0.141616
epoch:242, batch3, load frames use: 5.90755200386s
step 11134/400000: train loss: 0.121632
epoch:242, batch4, load frames use: 5.50919294357s
step 11135/400000: train loss: 0.128894
epoch:242, batch5, load frames use: 4.67521309853s
step 11136/400000: train loss: 0.148839
epoch:242, batch6, load frames use: 5.3620159626s
step 11137/400000: train loss: 0.134598
epoch:242, batch7, load frames use: 6.3073720932s
step 11138/400000: train loss: 0.131771
epoch:242, batch8, load frames use: 5.62396788597s
step 11139/400000: train loss: 0.127935
epoch:242, batch9, load frames use: 5.64096903801s
step 11140/400000: train loss: 0.127655
epoch:242, batch10, load frames use: 5.84014010429s
step 11141/400000: train loss: 0.136133
epoch:242, batch11, load frames use: 4.95693588257s
step 11142/400000: train loss: 0.135765
epoch:242, batch12, load frames use: 5.82405400276s
step 11143/400000: train loss: 0.141698
epoch:242, batch13, load frames use: 4.84955596924s
step 11144/400000: train loss: 0.136742
epoch:242, batch14, load frames use: 5.85917687416s
step 11145/400000: train loss: 0.117262
epoch:242, batch15, load frames use: 5.02983784676s
step 11146/400000: train loss: 0.148632
epoch:242, batch16, load frames use: 5.91639614105s
step 11147/400000: train loss: 0.136330
epoch:242, batch17, load frames use: 5.94591999054s
step 11148/400000: train loss: 0.129964
epoch:242, batch18, load frames use: 5.79617786407s
step 11149/400000: train loss: 0.131431
epoch:242, batch19, load frames use: 9.08142113686s
step 11150/400000: train loss: 0.132052
epoch:242, batch20, load frames use: 5.80274391174s
step 11151/400000: train loss: 0.131698
epoch:242, batch21, load frames use: 6.23885607719s
step 11152/400000: train loss: 0.133550
epoch:242, batch22, load frames use: 5.85373497009s
step 11153/400000: train loss: 0.136565
epoch:242, batch23, load frames use: 5.85927605629s
step 11154/400000: train loss: 0.138143
epoch:242, batch24, load frames use: 6.45536208153s
step 11155/400000: train loss: 0.146537
epoch:242, batch25, load frames use: 6.12120008469s
step 11156/400000: train loss: 0.149802
epoch:242, batch26, load frames use: 6.2278649807s
step 11157/400000: train loss: 0.136888
epoch:242, batch27, load frames use: 5.50275683403s
step 11158/400000: train loss: 0.149589
epoch:242, batch28, load frames use: 6.14328503609s
step 11159/400000: train loss: 0.149314
epoch:242, batch29, load frames use: 6.15495800972s
step 11160/400000: train loss: 0.155531
epoch:242, batch30, load frames use: 5.93099308014s
step 11161/400000: train loss: 0.142869
epoch:242, batch31, load frames use: 6.12679791451s
step 11162/400000: train loss: 0.152230
epoch:242, batch32, load frames use: 6.24368190765s
step 11163/400000: train loss: 0.139991
epoch:242, batch33, load frames use: 5.81541085243s
step 11164/400000: train loss: 0.173317
epoch:242, batch34, load frames use: 6.45274209976s
step 11165/400000: train loss: 0.137206
epoch:242, batch35, load frames use: 6.90739893913s
step 11166/400000: train loss: 0.139253
epoch:242, batch36, load frames use: 5.8171081543s
step 11167/400000: train loss: 0.167250
epoch:242, batch37, load frames use: 6.02712988853s
step 11168/400000: train loss: 0.130722
epoch:242, batch38, load frames use: 5.56797099113s
step 11169/400000: train loss: 0.165944
epoch:242, batch39, load frames use: 6.12109804153s
step 11170/400000: train loss: 0.133423
epoch:242, batch40, load frames use: 5.98099899292s
step 11171/400000: train loss: 0.168308
epoch:242, batch41, load frames use: 6.91711592674s
step 11172/400000: train loss: 0.163785
epoch:242, batch42, load frames use: 5.32443284988s
step 11173/400000: train loss: 0.164898
epoch:242, batch43, load frames use: 5.40376615524s
step 11174/400000: train loss: 0.132592
epoch:242, batch44, load frames use: 6.68364286423s
step 11175/400000: train loss: 0.138481
epoch:242, batch45, load frames use: 5.95410203934s
step 11176/400000: train loss: 0.151824
epoch:242, batch46, load frames use: 5.15641593933s
step 11177/400000: train loss: 0.148696
epoch:243, batch1, load frames use: 5.92085909843s
step 11178/400000: train loss: 0.136546
epoch:243, batch2, load frames use: 5.38742113113s
step 11179/400000: train loss: 0.121842
epoch:243, batch3, load frames use: 6.32085204124s
step 11180/400000: train loss: 0.134243
epoch:243, batch4, load frames use: 5.2281870842s
step 11181/400000: train loss: 0.143975
epoch:243, batch5, load frames use: 6.26053094864s
step 11182/400000: train loss: 0.145193
epoch:243, batch6, load frames use: 5.68941378593s
step 11183/400000: train loss: 0.141751
epoch:243, batch7, load frames use: 5.29106497765s
step 11184/400000: train loss: 0.136584
epoch:243, batch8, load frames use: 5.60106396675s
step 11185/400000: train loss: 0.148015
epoch:243, batch9, load frames use: 6.08963489532s
step 11186/400000: train loss: 0.142830
epoch:243, batch10, load frames use: 5.84722208977s
step 11187/400000: train loss: 0.138787
epoch:243, batch11, load frames use: 6.13872003555s
step 11188/400000: train loss: 0.154612
epoch:243, batch12, load frames use: 6.45263791084s
step 11189/400000: train loss: 0.152255
epoch:243, batch13, load frames use: 5.2195520401s
step 11190/400000: train loss: 0.139289
epoch:243, batch14, load frames use: 6.38986706734s
step 11191/400000: train loss: 0.140109
epoch:243, batch15, load frames use: 6.05133605003s
step 11192/400000: train loss: 0.142203
epoch:243, batch16, load frames use: 5.84292006493s
step 11193/400000: train loss: 0.141351
epoch:243, batch17, load frames use: 6.367374897s
step 11194/400000: train loss: 0.143868
epoch:243, batch18, load frames use: 6.06670808792s
step 11195/400000: train loss: 0.150552
epoch:243, batch19, load frames use: 5.44009280205s
step 11196/400000: train loss: 0.138887
epoch:243, batch20, load frames use: 6.07014107704s
step 11197/400000: train loss: 0.137176
epoch:243, batch21, load frames use: 5.75429105759s
step 11198/400000: train loss: 0.136043
epoch:243, batch22, load frames use: 7.2319791317s
step 11199/400000: train loss: 0.146171
epoch:243, batch23, load frames use: 6.0981118679s
step 11200/400000: train loss: 0.137558
step 11200/400000: validation loss: 0.138450
epoch:243, batch24, load frames use: 5.98344111443s
step 11201/400000: train loss: 0.138499
epoch:243, batch25, load frames use: 5.8418610096s
step 11202/400000: train loss: 0.129765
epoch:243, batch26, load frames use: 5.22999000549s
step 11203/400000: train loss: 0.140018
epoch:243, batch27, load frames use: 5.51954984665s
step 11204/400000: train loss: 0.142360
epoch:243, batch28, load frames use: 7.67800188065s
step 11205/400000: train loss: 0.143404
epoch:243, batch29, load frames use: 6.10463500023s
step 11206/400000: train loss: 0.121818
epoch:243, batch30, load frames use: 6.4927251339s
step 11207/400000: train loss: 0.138739
epoch:243, batch31, load frames use: 6.53548407555s
step 11208/400000: train loss: 0.130290
epoch:243, batch32, load frames use: 5.4987411499s
step 11209/400000: train loss: 0.131652
epoch:243, batch33, load frames use: 5.14030885696s
step 11210/400000: train loss: 0.147550
epoch:243, batch34, load frames use: 4.84394788742s
step 11211/400000: train loss: 0.120344
epoch:243, batch35, load frames use: 5.12758898735s
step 11212/400000: train loss: 0.131926
epoch:243, batch36, load frames use: 6.72913885117s
step 11213/400000: train loss: 0.144892
epoch:243, batch37, load frames use: 6.18578982353s
step 11214/400000: train loss: 0.152811
epoch:243, batch38, load frames use: 5.98508000374s
step 11215/400000: train loss: 0.148821
epoch:243, batch39, load frames use: 7.06000304222s
step 11216/400000: train loss: 0.153904
epoch:243, batch40, load frames use: 5.99586296082s
step 11217/400000: train loss: 0.127057
epoch:243, batch41, load frames use: 6.00109696388s
step 11218/400000: train loss: 0.130371
epoch:243, batch42, load frames use: 5.90185308456s
step 11219/400000: train loss: 0.136388
epoch:243, batch43, load frames use: 6.25558805466s
step 11220/400000: train loss: 0.134086
epoch:243, batch44, load frames use: 7.64256286621s
step 11221/400000: train loss: 0.146428
epoch:243, batch45, load frames use: 6.57973599434s
step 11222/400000: train loss: 0.130922
epoch:243, batch46, load frames use: 6.24433588982s
step 11223/400000: train loss: 0.140662
epoch:244, batch1, load frames use: 5.37176990509s
step 11224/400000: train loss: 0.133435
epoch:244, batch2, load frames use: 5.58390498161s
step 11225/400000: train loss: 0.141877
epoch:244, batch3, load frames use: 5.39706301689s
step 11226/400000: train loss: 0.130143
epoch:244, batch4, load frames use: 4.80934810638s
step 11227/400000: train loss: 0.114253
epoch:244, batch5, load frames use: 5.6618771553s
step 11228/400000: train loss: 0.149092
epoch:244, batch6, load frames use: 6.66098403931s
step 11229/400000: train loss: 0.135348
epoch:244, batch7, load frames use: 5.89961409569s
step 11230/400000: train loss: 0.128311
epoch:244, batch8, load frames use: 6.1356818676s
step 11231/400000: train loss: 0.129839
epoch:244, batch9, load frames use: 7.39122486115s
step 11232/400000: train loss: 0.142168
epoch:244, batch10, load frames use: 5.79725408554s
step 11233/400000: train loss: 0.130240
epoch:244, batch11, load frames use: 5.64278697968s
step 11234/400000: train loss: 0.138457
epoch:244, batch12, load frames use: 5.68804597855s
step 11235/400000: train loss: 0.127233
epoch:244, batch13, load frames use: 5.41148781776s
step 11236/400000: train loss: 0.139766
epoch:244, batch14, load frames use: 5.92430996895s
step 11237/400000: train loss: 0.131618
epoch:244, batch15, load frames use: 6.26994299889s
step 11238/400000: train loss: 0.118890
epoch:244, batch16, load frames use: 5.99824810028s
step 11239/400000: train loss: 0.141865
epoch:244, batch17, load frames use: 5.79013991356s
step 11240/400000: train loss: 0.158336
epoch:244, batch18, load frames use: 6.20847296715s
step 11241/400000: train loss: 0.140720
epoch:244, batch19, load frames use: 5.4803879261s
step 11242/400000: train loss: 0.131678
epoch:244, batch20, load frames use: 7.02791285515s
step 11243/400000: train loss: 0.134475
epoch:244, batch21, load frames use: 6.46576309204s
step 11244/400000: train loss: 0.134802
epoch:244, batch22, load frames use: 6.17707300186s
step 11245/400000: train loss: 0.129615
epoch:244, batch23, load frames use: 5.4535639286s
step 11246/400000: train loss: 0.140067
epoch:244, batch24, load frames use: 5.90587592125s
step 11247/400000: train loss: 0.135627
epoch:244, batch25, load frames use: 20.0403029919s
step 11248/400000: train loss: 0.138350
epoch:244, batch26, load frames use: 7.43473505974s
step 11249/400000: train loss: 0.136063
epoch:244, batch27, load frames use: 6.02319383621s
step 11250/400000: train loss: 0.138214
epoch:244, batch28, load frames use: 6.76938199997s
step 11251/400000: train loss: 0.127455
epoch:244, batch29, load frames use: 6.81344199181s
step 11252/400000: train loss: 0.125564
epoch:244, batch30, load frames use: 6.66637897491s
step 11253/400000: train loss: 0.157481
epoch:244, batch31, load frames use: 7.14004015923s
step 11254/400000: train loss: 0.156353
epoch:244, batch32, load frames use: 6.21496796608s
step 11255/400000: train loss: 0.120649
epoch:244, batch33, load frames use: 8.5734360218s
step 11256/400000: train loss: 0.147583
epoch:244, batch34, load frames use: 6.62452793121s
step 11257/400000: train loss: 0.146774
epoch:244, batch35, load frames use: 6.41118311882s
step 11258/400000: train loss: 0.146061
epoch:244, batch36, load frames use: 6.53529596329s
step 11259/400000: train loss: 0.145455
epoch:244, batch37, load frames use: 6.28906702995s
step 11260/400000: train loss: 0.120963
epoch:244, batch38, load frames use: 7.79203486443s
step 11261/400000: train loss: 0.147819
epoch:244, batch39, load frames use: 5.3782351017s
step 11262/400000: train loss: 0.145734
epoch:244, batch40, load frames use: 6.13133788109s
step 11263/400000: train loss: 0.148507
epoch:244, batch41, load frames use: 6.08207821846s
step 11264/400000: train loss: 0.137277
epoch:244, batch42, load frames use: 5.82449197769s
step 11265/400000: train loss: 0.146764
epoch:244, batch43, load frames use: 4.83647584915s
step 11266/400000: train loss: 0.126058
epoch:244, batch44, load frames use: 5.70288896561s
step 11267/400000: train loss: 0.138037
epoch:244, batch45, load frames use: 5.87405705452s
step 11268/400000: train loss: 0.142027
epoch:244, batch46, load frames use: 5.7444460392s
step 11269/400000: train loss: 0.150523
epoch:245, batch1, load frames use: 5.96685099602s
step 11270/400000: train loss: 0.135700
epoch:245, batch2, load frames use: 5.57796788216s
step 11271/400000: train loss: 0.131204
epoch:245, batch3, load frames use: 7.9983458519s
step 11272/400000: train loss: 0.132127
epoch:245, batch4, load frames use: 5.79191994667s
step 11273/400000: train loss: 0.128651
epoch:245, batch5, load frames use: 6.19550704956s
step 11274/400000: train loss: 0.121514
epoch:245, batch6, load frames use: 6.25363087654s
step 11275/400000: train loss: 0.120207
epoch:245, batch7, load frames use: 6.34437584877s
step 11276/400000: train loss: 0.131238
epoch:245, batch8, load frames use: 5.66495895386s
step 11277/400000: train loss: 0.134143
epoch:245, batch9, load frames use: 5.5007379055s
step 11278/400000: train loss: 0.121646
epoch:245, batch10, load frames use: 6.07752013206s
step 11279/400000: train loss: 0.123277
epoch:245, batch11, load frames use: 6.11122012138s
step 11280/400000: train loss: 0.140331
epoch:245, batch12, load frames use: 5.69575500488s
step 11281/400000: train loss: 0.134609
epoch:245, batch13, load frames use: 6.69779396057s
step 11282/400000: train loss: 0.144766
epoch:245, batch14, load frames use: 5.61880302429s
step 11283/400000: train loss: 0.132012
epoch:245, batch15, load frames use: 6.00754499435s
step 11284/400000: train loss: 0.151377
epoch:245, batch16, load frames use: 6.6901910305s
step 11285/400000: train loss: 0.117488
epoch:245, batch17, load frames use: 6.74996614456s
step 11286/400000: train loss: 0.130461
epoch:245, batch18, load frames use: 5.66092014313s
step 11287/400000: train loss: 0.133781
epoch:245, batch19, load frames use: 6.19788599014s
step 11288/400000: train loss: 0.128769
epoch:245, batch20, load frames use: 6.16405916214s
step 11289/400000: train loss: 0.124752
epoch:245, batch21, load frames use: 6.53981804848s
step 11290/400000: train loss: 0.143725
epoch:245, batch22, load frames use: 5.85257411003s
step 11291/400000: train loss: 0.124112
epoch:245, batch23, load frames use: 6.59292697906s
step 11292/400000: train loss: 0.133202
epoch:245, batch24, load frames use: 6.68270516396s
step 11293/400000: train loss: 0.119662
epoch:245, batch25, load frames use: 6.91591906548s
step 11294/400000: train loss: 0.142217
epoch:245, batch26, load frames use: 6.07057189941s
step 11295/400000: train loss: 0.130357
epoch:245, batch27, load frames use: 5.75699400902s
step 11296/400000: train loss: 0.140031
epoch:245, batch28, load frames use: 6.46124696732s
step 11297/400000: train loss: 0.136761
epoch:245, batch29, load frames use: 6.33746504784s
step 11298/400000: train loss: 0.134780
epoch:245, batch30, load frames use: 6.71613192558s
step 11299/400000: train loss: 0.139179
epoch:245, batch31, load frames use: 6.03811001778s
step 11300/400000: train loss: 0.129398
step 11300/400000: validation loss: 0.133973
epoch:245, batch32, load frames use: 5.99238085747s
step 11301/400000: train loss: 0.140559
epoch:245, batch33, load frames use: 5.96305394173s
step 11302/400000: train loss: 0.145153
epoch:245, batch34, load frames use: 7.72071099281s
step 11303/400000: train loss: 0.151166
epoch:245, batch35, load frames use: 6.35984706879s
step 11304/400000: train loss: 0.133298
epoch:245, batch36, load frames use: 6.33105111122s
step 11305/400000: train loss: 0.148909
epoch:245, batch37, load frames use: 5.98089408875s
step 11306/400000: train loss: 0.131502
epoch:245, batch38, load frames use: 5.70355987549s
step 11307/400000: train loss: 0.148029
epoch:245, batch39, load frames use: 6.39241909981s
step 11308/400000: train loss: 0.145971
epoch:245, batch40, load frames use: 6.24894785881s
step 11309/400000: train loss: 0.120758
epoch:245, batch41, load frames use: 6.07528686523s
step 11310/400000: train loss: 0.122444
epoch:245, batch42, load frames use: 6.06606197357s
step 11311/400000: train loss: 0.148575
epoch:245, batch43, load frames use: 6.25740289688s
step 11312/400000: train loss: 0.139238
epoch:245, batch44, load frames use: 6.38018321991s
step 11313/400000: train loss: 0.145998
epoch:245, batch45, load frames use: 6.38148999214s
step 11314/400000: train loss: 0.128283
epoch:245, batch46, load frames use: 6.4550511837s
step 11315/400000: train loss: 0.157305
epoch:246, batch1, load frames use: 6.19429588318s
step 11316/400000: train loss: 0.148531
epoch:246, batch2, load frames use: 5.81619405746s
step 11317/400000: train loss: 0.128205
epoch:246, batch3, load frames use: 6.63467693329s
step 11318/400000: train loss: 0.138750
epoch:246, batch4, load frames use: 6.44260406494s
step 11319/400000: train loss: 0.134402
epoch:246, batch5, load frames use: 6.11704802513s
step 11320/400000: train loss: 0.128046
epoch:246, batch6, load frames use: 6.28494000435s
step 11321/400000: train loss: 0.117963
epoch:246, batch7, load frames use: 7.16556119919s
step 11322/400000: train loss: 0.154377
epoch:246, batch8, load frames use: 5.91632080078s
step 11323/400000: train loss: 0.121433
epoch:246, batch9, load frames use: 6.25848984718s
step 11324/400000: train loss: 0.125265
epoch:246, batch10, load frames use: 6.69980788231s
step 11325/400000: train loss: 0.144694
epoch:246, batch11, load frames use: 5.72347784042s
step 11326/400000: train loss: 0.146195
epoch:246, batch12, load frames use: 6.83800697327s
step 11327/400000: train loss: 0.116200
epoch:246, batch13, load frames use: 6.69283890724s
step 11328/400000: train loss: 0.142059
epoch:246, batch14, load frames use: 5.6019449234s
step 11329/400000: train loss: 0.142854
epoch:246, batch15, load frames use: 5.81766700745s
step 11330/400000: train loss: 0.124384
epoch:246, batch16, load frames use: 5.77520608902s
step 11331/400000: train loss: 0.128851
epoch:246, batch17, load frames use: 5.81992816925s
step 11332/400000: train loss: 0.127894
epoch:246, batch18, load frames use: 5.82298207283s
step 11333/400000: train loss: 0.141791
epoch:246, batch19, load frames use: 6.24841499329s
step 11334/400000: train loss: 0.129581
epoch:246, batch20, load frames use: 6.24956202507s
step 11335/400000: train loss: 0.155222
epoch:246, batch21, load frames use: 5.88559889793s
step 11336/400000: train loss: 0.136085
epoch:246, batch22, load frames use: 5.93148088455s
step 11337/400000: train loss: 0.145916
epoch:246, batch23, load frames use: 6.40296697617s
step 11338/400000: train loss: 0.131719
epoch:246, batch24, load frames use: 6.40024995804s
step 11339/400000: train loss: 0.125448
epoch:246, batch25, load frames use: 6.26402401924s
step 11340/400000: train loss: 0.153616
epoch:246, batch26, load frames use: 6.46158289909s
step 11341/400000: train loss: 0.144092
epoch:246, batch27, load frames use: 5.55725884438s
step 11342/400000: train loss: 0.145242
epoch:246, batch28, load frames use: 6.69184088707s
step 11343/400000: train loss: 0.129342
epoch:246, batch29, load frames use: 6.5736489296s
step 11344/400000: train loss: 0.148702
epoch:246, batch30, load frames use: 6.51011490822s
step 11345/400000: train loss: 0.138818
epoch:246, batch31, load frames use: 6.12365603447s
step 11346/400000: train loss: 0.153253
epoch:246, batch32, load frames use: 6.05674409866s
step 11347/400000: train loss: 0.126334
epoch:246, batch33, load frames use: 5.82302308083s
step 11348/400000: train loss: 0.124333
epoch:246, batch34, load frames use: 7.41716694832s
step 11349/400000: train loss: 0.175425
epoch:246, batch35, load frames use: 5.93971204758s
step 11350/400000: train loss: 0.134381
epoch:246, batch36, load frames use: 6.1209628582s
step 11351/400000: train loss: 0.141525
epoch:246, batch37, load frames use: 7.00350379944s
step 11352/400000: train loss: 0.138013
epoch:246, batch38, load frames use: 6.05074191093s
step 11353/400000: train loss: 0.140355
epoch:246, batch39, load frames use: 7.54478502274s
step 11354/400000: train loss: 0.131262
epoch:246, batch40, load frames use: 5.0478219986s
step 11355/400000: train loss: 0.138585
epoch:246, batch41, load frames use: 6.01952004433s
step 11356/400000: train loss: 0.142792
epoch:246, batch42, load frames use: 5.81551504135s
step 11357/400000: train loss: 0.139088
epoch:246, batch43, load frames use: 6.76311707497s
step 11358/400000: train loss: 0.121754
epoch:246, batch44, load frames use: 7.23210215569s
step 11359/400000: train loss: 0.156697
epoch:246, batch45, load frames use: 6.12634396553s
step 11360/400000: train loss: 0.150484
epoch:246, batch46, load frames use: 6.40273094177s
step 11361/400000: train loss: 0.146149
epoch:247, batch1, load frames use: 6.71138596535s
step 11362/400000: train loss: 0.122324
epoch:247, batch2, load frames use: 6.79685306549s
step 11363/400000: train loss: 0.122231
epoch:247, batch3, load frames use: 7.72656011581s
step 11364/400000: train loss: 0.134237
epoch:247, batch4, load frames use: 7.1431210041s
step 11365/400000: train loss: 0.136478
epoch:247, batch5, load frames use: 5.98056197166s
step 11366/400000: train loss: 0.123439
epoch:247, batch6, load frames use: 6.04407715797s
step 11367/400000: train loss: 0.135722
epoch:247, batch7, load frames use: 5.95116019249s
step 11368/400000: train loss: 0.155156
epoch:247, batch8, load frames use: 6.91431999207s
step 11369/400000: train loss: 0.141836
epoch:247, batch9, load frames use: 5.64222478867s
step 11370/400000: train loss: 0.133410
epoch:247, batch10, load frames use: 6.14231801033s
step 11371/400000: train loss: 0.134711
epoch:247, batch11, load frames use: 6.10097908974s
step 11372/400000: train loss: 0.137380
epoch:247, batch12, load frames use: 6.34446287155s
step 11373/400000: train loss: 0.125347
epoch:247, batch13, load frames use: 5.91981005669s
step 11374/400000: train loss: 0.144925
epoch:247, batch14, load frames use: 6.50866413116s
step 11375/400000: train loss: 0.140061
epoch:247, batch15, load frames use: 5.97465109825s
step 11376/400000: train loss: 0.128951
epoch:247, batch16, load frames use: 6.2459499836s
step 11377/400000: train loss: 0.116901
epoch:247, batch17, load frames use: 6.06767320633s
step 11378/400000: train loss: 0.130922
epoch:247, batch18, load frames use: 7.7620871067s
step 11379/400000: train loss: 0.134388
epoch:247, batch19, load frames use: 6.24676609039s
step 11380/400000: train loss: 0.129060
epoch:247, batch20, load frames use: 6.02054595947s
step 11381/400000: train loss: 0.161283
epoch:247, batch21, load frames use: 5.50999307632s
step 11382/400000: train loss: 0.124537
epoch:247, batch22, load frames use: 6.83742189407s
step 11383/400000: train loss: 0.128985
epoch:247, batch23, load frames use: 7.58831191063s
step 11384/400000: train loss: 0.135794
epoch:247, batch24, load frames use: 6.80649900436s
step 11385/400000: train loss: 0.151591
epoch:247, batch25, load frames use: 6.32567405701s
step 11386/400000: train loss: 0.135862
epoch:247, batch26, load frames use: 6.31686496735s
step 11387/400000: train loss: 0.137895
epoch:247, batch27, load frames use: 6.56078195572s
step 11388/400000: train loss: 0.136665
epoch:247, batch28, load frames use: 6.17670416832s
step 11389/400000: train loss: 0.123237
epoch:247, batch29, load frames use: 5.28126120567s
step 11390/400000: train loss: 0.132072
epoch:247, batch30, load frames use: 5.95976018906s
step 11391/400000: train loss: 0.136527
epoch:247, batch31, load frames use: 5.8136138916s
step 11392/400000: train loss: 0.131736
epoch:247, batch32, load frames use: 6.81123781204s
step 11393/400000: train loss: 0.134546
epoch:247, batch33, load frames use: 6.12971496582s
step 11394/400000: train loss: 0.152095
epoch:247, batch34, load frames use: 5.38822007179s
step 11395/400000: train loss: 0.127005
epoch:247, batch35, load frames use: 5.36977410316s
step 11396/400000: train loss: 0.132145
epoch:247, batch36, load frames use: 5.97713494301s
step 11397/400000: train loss: 0.144065
epoch:247, batch37, load frames use: 6.60320305824s
step 11398/400000: train loss: 0.149986
epoch:247, batch38, load frames use: 7.7012732029s
step 11399/400000: train loss: 0.132802
epoch:247, batch39, load frames use: 5.56434702873s
step 11400/400000: train loss: 0.136342
step 11400/400000: validation loss: 0.145650
epoch:247, batch40, load frames use: 6.55436706543s
step 11401/400000: train loss: 0.137645
epoch:247, batch41, load frames use: 6.45010280609s
step 11402/400000: train loss: 0.139627
epoch:247, batch42, load frames use: 5.72566604614s
step 11403/400000: train loss: 0.137409
epoch:247, batch43, load frames use: 6.18617010117s
step 11404/400000: train loss: 0.152375
epoch:247, batch44, load frames use: 7.08562803268s
step 11405/400000: train loss: 0.127019
epoch:247, batch45, load frames use: 6.3832449913s
step 11406/400000: train loss: 0.129474
epoch:247, batch46, load frames use: 6.46740007401s
step 11407/400000: train loss: 0.130984
epoch:248, batch1, load frames use: 6.48685002327s
step 11408/400000: train loss: 0.119749
epoch:248, batch2, load frames use: 5.46047711372s
step 11409/400000: train loss: 0.131590
epoch:248, batch3, load frames use: 6.72683501244s
step 11410/400000: train loss: 0.125970
epoch:248, batch4, load frames use: 6.50323414803s
step 11411/400000: train loss: 0.128767
epoch:248, batch5, load frames use: 6.25208902359s
step 11412/400000: train loss: 0.134803
epoch:248, batch6, load frames use: 6.35240983963s
step 11413/400000: train loss: 0.136525
epoch:248, batch7, load frames use: 6.56572294235s
step 11414/400000: train loss: 0.136530
epoch:248, batch8, load frames use: 6.18077921867s
step 11415/400000: train loss: 0.139983
epoch:248, batch9, load frames use: 6.02185297012s
step 11416/400000: train loss: 0.132756
epoch:248, batch10, load frames use: 5.94509100914s
step 11417/400000: train loss: 0.132143
epoch:248, batch11, load frames use: 6.19679808617s
step 11418/400000: train loss: 0.159746
epoch:248, batch12, load frames use: 7.71040511131s
step 11419/400000: train loss: 0.148653
epoch:248, batch13, load frames use: 5.23398399353s
step 11420/400000: train loss: 0.124200
epoch:248, batch14, load frames use: 6.07879495621s
step 11421/400000: train loss: 0.127607
epoch:248, batch15, load frames use: 6.99582004547s
step 11422/400000: train loss: 0.125208
epoch:248, batch16, load frames use: 6.64057016373s
step 11423/400000: train loss: 0.152273
epoch:248, batch17, load frames use: 6.8982000351s
step 11424/400000: train loss: 0.144781
epoch:248, batch18, load frames use: 6.40556001663s
step 11425/400000: train loss: 0.133984
epoch:248, batch19, load frames use: 6.15234589577s
step 11426/400000: train loss: 0.130614
epoch:248, batch20, load frames use: 5.60787200928s
step 11427/400000: train loss: 0.147632
epoch:248, batch21, load frames use: 6.72673296928s
step 11428/400000: train loss: 0.129721
epoch:248, batch22, load frames use: 6.71868515015s
step 11429/400000: train loss: 0.129872
epoch:248, batch23, load frames use: 5.84547305107s
step 11430/400000: train loss: 0.138219
epoch:248, batch24, load frames use: 7.19515109062s
step 11431/400000: train loss: 0.125704
epoch:248, batch25, load frames use: 5.94328904152s
step 11432/400000: train loss: 0.148031
epoch:248, batch26, load frames use: 6.89744520187s
step 11433/400000: train loss: 0.138266
epoch:248, batch27, load frames use: 7.28600001335s
step 11434/400000: train loss: 0.137118
epoch:248, batch28, load frames use: 6.86929798126s
step 11435/400000: train loss: 0.148739
epoch:248, batch29, load frames use: 6.35429692268s
step 11436/400000: train loss: 0.143194
epoch:248, batch30, load frames use: 5.85718607903s
step 11437/400000: train loss: 0.120629
epoch:248, batch31, load frames use: 6.97723913193s
step 11438/400000: train loss: 0.127258
epoch:248, batch32, load frames use: 7.95025014877s
step 11439/400000: train loss: 0.140419
epoch:248, batch33, load frames use: 6.09584784508s
step 11440/400000: train loss: 0.124917
epoch:248, batch34, load frames use: 6.7993581295s
step 11441/400000: train loss: 0.128023
epoch:248, batch35, load frames use: 5.92272090912s
step 11442/400000: train loss: 0.149171
epoch:248, batch36, load frames use: 6.63209199905s
step 11443/400000: train loss: 0.121047
epoch:248, batch37, load frames use: 7.80830287933s
step 11444/400000: train loss: 0.129763
epoch:248, batch38, load frames use: 7.11937618256s
step 11445/400000: train loss: 0.127149
epoch:248, batch39, load frames use: 5.79852199554s
step 11446/400000: train loss: 0.125013
epoch:248, batch40, load frames use: 6.01475310326s
step 11447/400000: train loss: 0.145581
epoch:248, batch41, load frames use: 5.7537021637s
step 11448/400000: train loss: 0.136916
epoch:248, batch42, load frames use: 8.01138901711s
step 11449/400000: train loss: 0.140514
epoch:248, batch43, load frames use: 6.69955611229s
step 11450/400000: train loss: 0.124550
epoch:248, batch44, load frames use: 5.75437307358s
step 11451/400000: train loss: 0.149345
epoch:248, batch45, load frames use: 6.27815890312s
step 11452/400000: train loss: 0.140270
epoch:248, batch46, load frames use: 5.7140390873s
step 11453/400000: train loss: 0.137677
epoch:249, batch1, load frames use: 6.29826521873s
step 11454/400000: train loss: 0.141288
epoch:249, batch2, load frames use: 7.07289099693s
step 11455/400000: train loss: 0.139322
epoch:249, batch3, load frames use: 6.3277118206s
step 11456/400000: train loss: 0.113048
epoch:249, batch4, load frames use: 5.97221207619s
step 11457/400000: train loss: 0.134401
epoch:249, batch5, load frames use: 6.4436211586s
step 11458/400000: train loss: 0.140712
epoch:249, batch6, load frames use: 6.85576605797s
step 11459/400000: train loss: 0.140458
epoch:249, batch7, load frames use: 5.05978488922s
step 11460/400000: train loss: 0.125340
epoch:249, batch8, load frames use: 5.38309288025s
step 11461/400000: train loss: 0.134146
epoch:249, batch9, load frames use: 5.92349100113s
step 11462/400000: train loss: 0.140521
epoch:249, batch10, load frames use: 5.65023112297s
step 11463/400000: train loss: 0.133524
epoch:249, batch11, load frames use: 4.99709916115s
step 11464/400000: train loss: 0.133357
epoch:249, batch12, load frames use: 5.45915603638s
step 11465/400000: train loss: 0.132224
epoch:249, batch13, load frames use: 5.14124107361s
step 11466/400000: train loss: 0.122649
epoch:249, batch14, load frames use: 6.09489393234s
step 11467/400000: train loss: 0.164074
epoch:249, batch15, load frames use: 5.06839108467s
step 11468/400000: train loss: 0.141564
epoch:249, batch16, load frames use: 5.3088889122s
step 11469/400000: train loss: 0.133848
epoch:249, batch17, load frames use: 7.60724186897s
step 11470/400000: train loss: 0.139119
epoch:249, batch18, load frames use: 6.8677880764s
step 11471/400000: train loss: 0.137334
epoch:249, batch19, load frames use: 6.08197999001s
step 11472/400000: train loss: 0.146934
epoch:249, batch20, load frames use: 6.02926778793s
step 11473/400000: train loss: 0.128446
epoch:249, batch21, load frames use: 5.7733168602s
step 11474/400000: train loss: 0.137591
epoch:249, batch22, load frames use: 7.02353787422s
step 11475/400000: train loss: 0.149122
epoch:249, batch23, load frames use: 7.42756700516s
step 11476/400000: train loss: 0.141131
epoch:249, batch24, load frames use: 5.78870797157s
step 11477/400000: train loss: 0.135879
epoch:249, batch25, load frames use: 6.12033009529s
step 11478/400000: train loss: 0.123565
epoch:249, batch26, load frames use: 6.26335597038s
step 11479/400000: train loss: 0.126436
epoch:249, batch27, load frames use: 5.86786389351s
step 11480/400000: train loss: 0.141563
epoch:249, batch28, load frames use: 5.58670592308s
step 11481/400000: train loss: 0.143761
epoch:249, batch29, load frames use: 5.33750200272s
step 11482/400000: train loss: 0.158173
epoch:249, batch30, load frames use: 5.69949007034s
step 11483/400000: train loss: 0.142379
epoch:249, batch31, load frames use: 6.20967197418s
step 11484/400000: train loss: 0.140928
epoch:249, batch32, load frames use: 5.52888798714s
step 11485/400000: train loss: 0.139893
epoch:249, batch33, load frames use: 7.67317485809s
step 11486/400000: train loss: 0.128441
epoch:249, batch34, load frames use: 6.99873018265s
step 11487/400000: train loss: 0.118425
epoch:249, batch35, load frames use: 6.07139396667s
step 11488/400000: train loss: 0.123744
epoch:249, batch36, load frames use: 6.29207897186s
step 11489/400000: train loss: 0.137352
epoch:249, batch37, load frames use: 6.70057010651s
step 11490/400000: train loss: 0.147771
epoch:249, batch38, load frames use: 5.97532105446s
step 11491/400000: train loss: 0.129403
epoch:249, batch39, load frames use: 6.24943709373s
step 11492/400000: train loss: 0.137176
epoch:249, batch40, load frames use: 5.70745897293s
step 11493/400000: train loss: 0.146245
epoch:249, batch41, load frames use: 5.89527893066s
step 11494/400000: train loss: 0.129243
epoch:249, batch42, load frames use: 5.90207600594s
step 11495/400000: train loss: 0.123084
epoch:249, batch43, load frames use: 5.09463119507s
step 11496/400000: train loss: 0.138107
epoch:249, batch44, load frames use: 5.33328413963s
step 11497/400000: train loss: 0.145503
epoch:249, batch45, load frames use: 5.46155905724s
step 11498/400000: train loss: 0.139248
epoch:249, batch46, load frames use: 6.4607000351s
step 11499/400000: train loss: 0.145640
epoch:250, batch1, load frames use: 6.46702814102s
step 11500/400000: train loss: 0.140118
step 11500/400000: validation loss: 0.138043
epoch:250, batch2, load frames use: 5.77582001686s
step 11501/400000: train loss: 0.133149
epoch:250, batch3, load frames use: 6.45167303085s
step 11502/400000: train loss: 0.137723
epoch:250, batch4, load frames use: 6.05785012245s
step 11503/400000: train loss: 0.128758
epoch:250, batch5, load frames use: 7.34314918518s
step 11504/400000: train loss: 0.121294
epoch:250, batch6, load frames use: 5.78422880173s
step 11505/400000: train loss: 0.125478
epoch:250, batch7, load frames use: 7.1123008728s
step 11506/400000: train loss: 0.128820
epoch:250, batch8, load frames use: 6.94966101646s
step 11507/400000: train loss: 0.151412
epoch:250, batch9, load frames use: 6.55551791191s
step 11508/400000: train loss: 0.126372
epoch:250, batch10, load frames use: 6.67772197723s
step 11509/400000: train loss: 0.122042
epoch:250, batch11, load frames use: 6.20169496536s
step 11510/400000: train loss: 0.150100
epoch:250, batch12, load frames use: 7.30362796783s
step 11511/400000: train loss: 0.143631
epoch:250, batch13, load frames use: 6.20590114594s
step 11512/400000: train loss: 0.120421
epoch:250, batch14, load frames use: 6.12637996674s
step 11513/400000: train loss: 0.126961
epoch:250, batch15, load frames use: 6.18411803246s
step 11514/400000: train loss: 0.143047
epoch:250, batch16, load frames use: 5.88715815544s
step 11515/400000: train loss: 0.126803
epoch:250, batch17, load frames use: 8.1831099987s
step 11516/400000: train loss: 0.125626
epoch:250, batch18, load frames use: 5.82208108902s
step 11517/400000: train loss: 0.134830
epoch:250, batch19, load frames use: 6.4343059063s
step 11518/400000: train loss: 0.127936
epoch:250, batch20, load frames use: 6.00897192955s
step 11519/400000: train loss: 0.135450
epoch:250, batch21, load frames use: 6.19605493546s
step 11520/400000: train loss: 0.140709
epoch:250, batch22, load frames use: 7.49037981033s
step 11521/400000: train loss: 0.133386
epoch:250, batch23, load frames use: 5.68944406509s
step 11522/400000: train loss: 0.150195
epoch:250, batch24, load frames use: 6.6967689991s
step 11523/400000: train loss: 0.131915
epoch:250, batch25, load frames use: 5.92828416824s
step 11524/400000: train loss: 0.133700
epoch:250, batch26, load frames use: 6.14886307716s
step 11525/400000: train loss: 0.160507
epoch:250, batch27, load frames use: 6.49319696426s
step 11526/400000: train loss: 0.128458
epoch:250, batch28, load frames use: 5.54246711731s
step 11527/400000: train loss: 0.148792
epoch:250, batch29, load frames use: 6.25924491882s
step 11528/400000: train loss: 0.121830
epoch:250, batch30, load frames use: 5.89386892319s
step 11529/400000: train loss: 0.126655
epoch:250, batch31, load frames use: 6.43952608109s
step 11530/400000: train loss: 0.143436
epoch:250, batch32, load frames use: 5.56152510643s
step 11531/400000: train loss: 0.144353
epoch:250, batch33, load frames use: 5.680519104s
step 11532/400000: train loss: 0.127131
epoch:250, batch34, load frames use: 5.86812901497s
step 11533/400000: train loss: 0.136827
epoch:250, batch35, load frames use: 6.78726887703s
step 11534/400000: train loss: 0.137106
epoch:250, batch36, load frames use: 6.39188408852s
step 11535/400000: train loss: 0.131216
epoch:250, batch37, load frames use: 7.43717288971s
step 11536/400000: train loss: 0.139283
epoch:250, batch38, load frames use: 6.51591897011s
step 11537/400000: train loss: 0.146274
epoch:250, batch39, load frames use: 5.96257901192s
step 11538/400000: train loss: 0.142493
epoch:250, batch40, load frames use: 6.69469308853s
step 11539/400000: train loss: 0.139978
epoch:250, batch41, load frames use: 6.79059100151s
step 11540/400000: train loss: 0.138385
epoch:250, batch42, load frames use: 5.66935610771s
step 11541/400000: train loss: 0.131497
epoch:250, batch43, load frames use: 5.75541114807s
step 11542/400000: train loss: 0.147735
epoch:250, batch44, load frames use: 6.12596797943s
step 11543/400000: train loss: 0.136650
epoch:250, batch45, load frames use: 5.89533495903s
step 11544/400000: train loss: 0.143087
epoch:250, batch46, load frames use: 6.0766749382s
step 11545/400000: train loss: 0.138400
epoch:251, batch1, load frames use: 6.01305985451s
step 11546/400000: train loss: 0.140174
epoch:251, batch2, load frames use: 6.36381101608s
step 11547/400000: train loss: 0.147252
epoch:251, batch3, load frames use: 6.63478183746s
step 11548/400000: train loss: 0.124317
epoch:251, batch4, load frames use: 5.74135184288s
step 11549/400000: train loss: 0.126735
epoch:251, batch5, load frames use: 6.28729009628s
step 11550/400000: train loss: 0.124367
epoch:251, batch6, load frames use: 6.12239599228s
step 11551/400000: train loss: 0.134179
epoch:251, batch7, load frames use: 6.27571487427s
step 11552/400000: train loss: 0.130734
epoch:251, batch8, load frames use: 5.67783403397s
step 11553/400000: train loss: 0.130221
epoch:251, batch9, load frames use: 5.65973091125s
step 11554/400000: train loss: 0.129288
epoch:251, batch10, load frames use: 5.56835198402s
step 11555/400000: train loss: 0.119037
epoch:251, batch11, load frames use: 6.22823095322s
step 11556/400000: train loss: 0.142251
epoch:251, batch12, load frames use: 7.41986203194s
step 11557/400000: train loss: 0.141154
epoch:251, batch13, load frames use: 6.81772303581s
step 11558/400000: train loss: 0.130638
epoch:251, batch14, load frames use: 5.82733011246s
step 11559/400000: train loss: 0.134848
epoch:251, batch15, load frames use: 6.22794389725s
step 11560/400000: train loss: 0.131748
epoch:251, batch16, load frames use: 5.67103600502s
step 11561/400000: train loss: 0.118471
epoch:251, batch17, load frames use: 6.59158992767s
step 11562/400000: train loss: 0.128535
epoch:251, batch18, load frames use: 5.86690783501s
step 11563/400000: train loss: 0.128925
epoch:251, batch19, load frames use: 5.91939687729s
step 11564/400000: train loss: 0.135497
epoch:251, batch20, load frames use: 6.24126005173s
step 11565/400000: train loss: 0.126910
epoch:251, batch21, load frames use: 5.42034482956s
step 11566/400000: train loss: 0.143786
epoch:251, batch22, load frames use: 5.93981790543s
step 11567/400000: train loss: 0.156579
epoch:251, batch23, load frames use: 5.7872569561s
step 11568/400000: train loss: 0.127086
epoch:251, batch24, load frames use: 6.58093690872s
step 11569/400000: train loss: 0.133246
epoch:251, batch25, load frames use: 5.97140693665s
step 11570/400000: train loss: 0.157810
epoch:251, batch26, load frames use: 6.19263195992s
step 11571/400000: train loss: 0.143134
epoch:251, batch27, load frames use: 5.91367197037s
step 11572/400000: train loss: 0.128927
epoch:251, batch28, load frames use: 6.30616807938s
step 11573/400000: train loss: 0.130794
epoch:251, batch29, load frames use: 6.21502304077s
step 11574/400000: train loss: 0.141115
epoch:251, batch30, load frames use: 5.87522816658s
step 11575/400000: train loss: 0.146278
epoch:251, batch31, load frames use: 5.85402393341s
step 11576/400000: train loss: 0.137201
epoch:251, batch32, load frames use: 6.53244781494s
step 11577/400000: train loss: 0.130728
epoch:251, batch33, load frames use: 5.64928507805s
step 11578/400000: train loss: 0.144837
epoch:251, batch34, load frames use: 6.90741300583s
step 11579/400000: train loss: 0.144335
epoch:251, batch35, load frames use: 5.54680490494s
step 11580/400000: train loss: 0.140473
epoch:251, batch36, load frames use: 6.08043599129s
step 11581/400000: train loss: 0.140456
epoch:251, batch37, load frames use: 7.26727199554s
step 11582/400000: train loss: 0.141743
epoch:251, batch38, load frames use: 5.87716984749s
step 11583/400000: train loss: 0.126335
epoch:251, batch39, load frames use: 6.18829107285s
step 11584/400000: train loss: 0.128875
epoch:251, batch40, load frames use: 6.03335905075s
step 11585/400000: train loss: 0.137104
epoch:251, batch41, load frames use: 6.25751209259s
step 11586/400000: train loss: 0.136428
epoch:251, batch42, load frames use: 6.06246900558s
step 11587/400000: train loss: 0.142171
epoch:251, batch43, load frames use: 6.45440387726s
step 11588/400000: train loss: 0.137635
epoch:251, batch44, load frames use: 6.121789217s
step 11589/400000: train loss: 0.134351
epoch:251, batch45, load frames use: 5.83700108528s
step 11590/400000: train loss: 0.148819
epoch:251, batch46, load frames use: 5.65787196159s
step 11591/400000: train loss: 0.150624
epoch:252, batch1, load frames use: 5.87446403503s
step 11592/400000: train loss: 0.136754
epoch:252, batch2, load frames use: 5.75788187981s
step 11593/400000: train loss: 0.139578
epoch:252, batch3, load frames use: 5.89904093742s
step 11594/400000: train loss: 0.130056
epoch:252, batch4, load frames use: 5.78273105621s
step 11595/400000: train loss: 0.125236
epoch:252, batch5, load frames use: 6.51991796494s
step 11596/400000: train loss: 0.143036
epoch:252, batch6, load frames use: 6.73244380951s
step 11597/400000: train loss: 0.129059
epoch:252, batch7, load frames use: 5.8385579586s
step 11598/400000: train loss: 0.127046
epoch:252, batch8, load frames use: 5.77809906006s
step 11599/400000: train loss: 0.120420
epoch:252, batch9, load frames use: 7.26575803757s
step 11600/400000: train loss: 0.145629
step 11600/400000: validation loss: 0.135753
epoch:252, batch10, load frames use: 6.78851413727s
step 11601/400000: train loss: 0.137372
epoch:252, batch11, load frames use: 6.40273189545s
step 11602/400000: train loss: 0.132251
epoch:252, batch12, load frames use: 6.86043214798s
step 11603/400000: train loss: 0.164184
epoch:252, batch13, load frames use: 7.12861800194s
step 11604/400000: train loss: 0.127540
epoch:252, batch14, load frames use: 6.00901412964s
step 11605/400000: train loss: 0.129199
epoch:252, batch15, load frames use: 7.47375917435s
step 11606/400000: train loss: 0.140864
epoch:252, batch16, load frames use: 7.5719640255s
step 11607/400000: train loss: 0.137257
epoch:252, batch17, load frames use: 7.12195587158s
step 11608/400000: train loss: 0.128700
epoch:252, batch18, load frames use: 6.64513301849s
step 11609/400000: train loss: 0.129682
epoch:252, batch19, load frames use: 6.39242386818s
step 11610/400000: train loss: 0.158179
epoch:252, batch20, load frames use: 6.68002700806s
step 11611/400000: train loss: 0.118108
epoch:252, batch21, load frames use: 7.17815995216s
step 11612/400000: train loss: 0.132636
epoch:252, batch22, load frames use: 7.21183085442s
step 11613/400000: train loss: 0.128827
epoch:252, batch23, load frames use: 7.01079106331s
step 11614/400000: train loss: 0.132967
epoch:252, batch24, load frames use: 6.55951786041s
step 11615/400000: train loss: 0.165006
epoch:252, batch25, load frames use: 7.67097902298s
step 11616/400000: train loss: 0.121947
epoch:252, batch26, load frames use: 5.55667901039s
step 11617/400000: train loss: 0.119686
epoch:252, batch27, load frames use: 6.39118504524s
step 11618/400000: train loss: 0.133737
epoch:252, batch28, load frames use: 6.57151699066s
step 11619/400000: train loss: 0.135808
epoch:252, batch29, load frames use: 5.96834492683s
step 11620/400000: train loss: 0.142865
epoch:252, batch30, load frames use: 6.35213685036s
step 11621/400000: train loss: 0.154505
epoch:252, batch31, load frames use: 6.70055913925s
step 11622/400000: train loss: 0.134149
epoch:252, batch32, load frames use: 5.43539595604s
step 11623/400000: train loss: 0.149776
epoch:252, batch33, load frames use: 6.4044008255s
step 11624/400000: train loss: 0.136758
epoch:252, batch34, load frames use: 5.7310628891s
step 11625/400000: train loss: 0.142215
epoch:252, batch35, load frames use: 5.91980099678s
step 11626/400000: train loss: 0.131425
epoch:252, batch36, load frames use: 6.03329205513s
step 11627/400000: train loss: 0.133470
epoch:252, batch37, load frames use: 5.98236298561s
step 11628/400000: train loss: 0.133214
epoch:252, batch38, load frames use: 6.37803292274s
step 11629/400000: train loss: 0.127946
epoch:252, batch39, load frames use: 6.47066783905s
step 11630/400000: train loss: 0.118808
epoch:252, batch40, load frames use: 6.09800505638s
step 11631/400000: train loss: 0.143867
epoch:252, batch41, load frames use: 5.86210298538s
step 11632/400000: train loss: 0.126421
epoch:252, batch42, load frames use: 6.10886907578s
step 11633/400000: train loss: 0.121680
epoch:252, batch43, load frames use: 6.12655305862s
step 11634/400000: train loss: 0.133278
epoch:252, batch44, load frames use: 6.17403697968s
step 11635/400000: train loss: 0.139048
epoch:252, batch45, load frames use: 6.51313495636s
step 11636/400000: train loss: 0.120786
epoch:252, batch46, load frames use: 6.201212883s
step 11637/400000: train loss: 0.154288
epoch:253, batch1, load frames use: 6.90928697586s
step 11638/400000: train loss: 0.118797
epoch:253, batch2, load frames use: 6.02127599716s
step 11639/400000: train loss: 0.122925
epoch:253, batch3, load frames use: 5.5281150341s
step 11640/400000: train loss: 0.133531
epoch:253, batch4, load frames use: 5.70284295082s
step 11641/400000: train loss: 0.128652
epoch:253, batch5, load frames use: 6.6265308857s
step 11642/400000: train loss: 0.136696
epoch:253, batch6, load frames use: 5.54617190361s
step 11643/400000: train loss: 0.108534
epoch:253, batch7, load frames use: 6.27161192894s
step 11644/400000: train loss: 0.138275
epoch:253, batch8, load frames use: 5.64812493324s
step 11645/400000: train loss: 0.128693
epoch:253, batch9, load frames use: 6.66345381737s
step 11646/400000: train loss: 0.129243
epoch:253, batch10, load frames use: 6.023250103s
step 11647/400000: train loss: 0.126924
epoch:253, batch11, load frames use: 6.04354786873s
step 11648/400000: train loss: 0.125189
epoch:253, batch12, load frames use: 5.72590398788s
step 11649/400000: train loss: 0.122979
epoch:253, batch13, load frames use: 5.74676203728s
step 11650/400000: train loss: 0.130539
epoch:253, batch14, load frames use: 6.77785515785s
step 11651/400000: train loss: 0.128676
epoch:253, batch15, load frames use: 5.90046000481s
step 11652/400000: train loss: 0.122213
epoch:253, batch16, load frames use: 6.01384401321s
step 11653/400000: train loss: 0.126581
epoch:253, batch17, load frames use: 6.0831952095s
step 11654/400000: train loss: 0.138331
epoch:253, batch18, load frames use: 6.93764615059s
step 11655/400000: train loss: 0.135824
epoch:253, batch19, load frames use: 5.45425391197s
step 11656/400000: train loss: 0.134621
epoch:253, batch20, load frames use: 6.33065080643s
step 11657/400000: train loss: 0.115335
epoch:253, batch21, load frames use: 6.00328993797s
step 11658/400000: train loss: 0.140916
epoch:253, batch22, load frames use: 5.21239495277s
step 11659/400000: train loss: 0.134556
epoch:253, batch23, load frames use: 5.52934002876s
step 11660/400000: train loss: 0.129765
epoch:253, batch24, load frames use: 5.97347402573s
step 11661/400000: train loss: 0.139777
epoch:253, batch25, load frames use: 6.08887505531s
step 11662/400000: train loss: 0.136797
epoch:253, batch26, load frames use: 5.76324915886s
step 11663/400000: train loss: 0.126665
epoch:253, batch27, load frames use: 5.4537460804s
step 11664/400000: train loss: 0.122702
epoch:253, batch28, load frames use: 5.56247186661s
step 11665/400000: train loss: 0.149437
epoch:253, batch29, load frames use: 5.37313294411s
step 11666/400000: train loss: 0.146015
epoch:253, batch30, load frames use: 8.78369998932s
step 11667/400000: train loss: 0.130440
epoch:253, batch31, load frames use: 5.64287185669s
step 11668/400000: train loss: 0.135921
epoch:253, batch32, load frames use: 6.57737207413s
step 11669/400000: train loss: 0.140357
epoch:253, batch33, load frames use: 5.47861790657s
step 11670/400000: train loss: 0.151340
epoch:253, batch34, load frames use: 5.86756896973s
step 11671/400000: train loss: 0.130408
epoch:253, batch35, load frames use: 6.2084980011s
step 11672/400000: train loss: 0.137594
epoch:253, batch36, load frames use: 5.79515004158s
step 11673/400000: train loss: 0.139576
epoch:253, batch37, load frames use: 7.03972506523s
step 11674/400000: train loss: 0.127109
epoch:253, batch38, load frames use: 5.43651199341s
step 11675/400000: train loss: 0.136849
epoch:253, batch39, load frames use: 6.07942795753s
step 11676/400000: train loss: 0.138936
epoch:253, batch40, load frames use: 5.94895505905s
step 11677/400000: train loss: 0.138791
epoch:253, batch41, load frames use: 4.94125795364s
step 11678/400000: train loss: 0.137712
epoch:253, batch42, load frames use: 5.28542590141s
step 11679/400000: train loss: 0.142657
epoch:253, batch43, load frames use: 6.09515023232s
step 11680/400000: train loss: 0.150355
epoch:253, batch44, load frames use: 5.77783107758s
step 11681/400000: train loss: 0.134947
epoch:253, batch45, load frames use: 5.36242699623s
step 11682/400000: train loss: 0.145225
epoch:253, batch46, load frames use: 5.23911595345s
step 11683/400000: train loss: 0.132573
epoch:254, batch1, load frames use: 5.92814588547s
step 11684/400000: train loss: 0.142812
epoch:254, batch2, load frames use: 6.01215815544s
step 11685/400000: train loss: 0.131236
epoch:254, batch3, load frames use: 5.52260518074s
step 11686/400000: train loss: 0.149618
epoch:254, batch4, load frames use: 5.60034799576s
step 11687/400000: train loss: 0.136859
epoch:254, batch5, load frames use: 5.91122889519s
step 11688/400000: train loss: 0.129478
epoch:254, batch6, load frames use: 5.18990802765s
step 11689/400000: train loss: 0.138929
epoch:254, batch7, load frames use: 5.91369199753s
step 11690/400000: train loss: 0.121013
epoch:254, batch8, load frames use: 5.65825605392s
step 11691/400000: train loss: 0.133145
epoch:254, batch9, load frames use: 5.63176417351s
step 11692/400000: train loss: 0.129220
epoch:254, batch10, load frames use: 5.47040009499s
step 11693/400000: train loss: 0.131579
epoch:254, batch11, load frames use: 5.65486311913s
step 11694/400000: train loss: 0.141972
epoch:254, batch12, load frames use: 6.59324288368s
step 11695/400000: train loss: 0.132869
epoch:254, batch13, load frames use: 6.76296520233s
step 11696/400000: train loss: 0.144671
epoch:254, batch14, load frames use: 5.67073392868s
step 11697/400000: train loss: 0.151685
epoch:254, batch15, load frames use: 5.58722805977s
step 11698/400000: train loss: 0.138375
epoch:254, batch16, load frames use: 6.3162779808s
step 11699/400000: train loss: 0.130179
epoch:254, batch17, load frames use: 6.20786809921s
step 11700/400000: train loss: 0.133014
step 11700/400000: validation loss: 0.123920
epoch:254, batch18, load frames use: 6.56254196167s
step 11701/400000: train loss: 0.149646
epoch:254, batch19, load frames use: 6.3751180172s
step 11702/400000: train loss: 0.138268
epoch:254, batch20, load frames use: 5.49742698669s
step 11703/400000: train loss: 0.126191
epoch:254, batch21, load frames use: 5.69436883926s
step 11704/400000: train loss: 0.132415
epoch:254, batch22, load frames use: 8.04911208153s
step 11705/400000: train loss: 0.118394
epoch:254, batch23, load frames use: 6.22850990295s
step 11706/400000: train loss: 0.139254
epoch:254, batch24, load frames use: 6.8807580471s
step 11707/400000: train loss: 0.142228
epoch:254, batch25, load frames use: 6.05094313622s
step 11708/400000: train loss: 0.133318
epoch:254, batch26, load frames use: 6.37696409225s
step 11709/400000: train loss: 0.137823
epoch:254, batch27, load frames use: 6.74375104904s
step 11710/400000: train loss: 0.142369
epoch:254, batch28, load frames use: 5.9788210392s
step 11711/400000: train loss: 0.122425
epoch:254, batch29, load frames use: 6.11734104156s
step 11712/400000: train loss: 0.150957
epoch:254, batch30, load frames use: 5.96805000305s
step 11713/400000: train loss: 0.143299
epoch:254, batch31, load frames use: 5.91979813576s
step 11714/400000: train loss: 0.132215
epoch:254, batch32, load frames use: 6.45578408241s
step 11715/400000: train loss: 0.157608
epoch:254, batch33, load frames use: 5.90894293785s
step 11716/400000: train loss: 0.130974
epoch:254, batch34, load frames use: 6.50905704498s
step 11717/400000: train loss: 0.137736
epoch:254, batch35, load frames use: 5.7801618576s
step 11718/400000: train loss: 0.135156
epoch:254, batch36, load frames use: 6.76014494896s
step 11719/400000: train loss: 0.134699
epoch:254, batch37, load frames use: 5.67702507973s
step 11720/400000: train loss: 0.146241
epoch:254, batch38, load frames use: 5.57629990578s
step 11721/400000: train loss: 0.136573
epoch:254, batch39, load frames use: 6.37333202362s
step 11722/400000: train loss: 0.144332
epoch:254, batch40, load frames use: 5.80125904083s
step 11723/400000: train loss: 0.135871
epoch:254, batch41, load frames use: 6.48174214363s
step 11724/400000: train loss: 0.147652
epoch:254, batch42, load frames use: 5.73905014992s
step 11725/400000: train loss: 0.115959
epoch:254, batch43, load frames use: 6.94468808174s
step 11726/400000: train loss: 0.135482
epoch:254, batch44, load frames use: 5.7615480423s
step 11727/400000: train loss: 0.151081
epoch:254, batch45, load frames use: 6.17340087891s
step 11728/400000: train loss: 0.130211
epoch:254, batch46, load frames use: 6.07583880424s
step 11729/400000: train loss: 0.138193
epoch:255, batch1, load frames use: 5.93103003502s
step 11730/400000: train loss: 0.143168
epoch:255, batch2, load frames use: 6.58530902863s
step 11731/400000: train loss: 0.131566
epoch:255, batch3, load frames use: 5.71792387962s
step 11732/400000: train loss: 0.110942
epoch:255, batch4, load frames use: 6.35625910759s
step 11733/400000: train loss: 0.131579
epoch:255, batch5, load frames use: 6.79261684418s
step 11734/400000: train loss: 0.144816
epoch:255, batch6, load frames use: 5.86117196083s
step 11735/400000: train loss: 0.125271
epoch:255, batch7, load frames use: 5.08459091187s
step 11736/400000: train loss: 0.150923
epoch:255, batch8, load frames use: 7.26936984062s
step 11737/400000: train loss: 0.141779
epoch:255, batch9, load frames use: 5.45631408691s
step 11738/400000: train loss: 0.127500
epoch:255, batch10, load frames use: 6.55316400528s
step 11739/400000: train loss: 0.121646
epoch:255, batch11, load frames use: 5.1043419838s
step 11740/400000: train loss: 0.117038
epoch:255, batch12, load frames use: 5.60313987732s
step 11741/400000: train loss: 0.131816
epoch:255, batch13, load frames use: 5.74350094795s
step 11742/400000: train loss: 0.133470
epoch:255, batch14, load frames use: 5.40941119194s
step 11743/400000: train loss: 0.131191
epoch:255, batch15, load frames use: 6.77986288071s
step 11744/400000: train loss: 0.127722
epoch:255, batch16, load frames use: 6.69405293465s
step 11745/400000: train loss: 0.126915
epoch:255, batch17, load frames use: 6.20674300194s
step 11746/400000: train loss: 0.146263
epoch:255, batch18, load frames use: 6.35259509087s
step 11747/400000: train loss: 0.139671
epoch:255, batch19, load frames use: 6.08808708191s
step 11748/400000: train loss: 0.127642
epoch:255, batch20, load frames use: 6.08770990372s
step 11749/400000: train loss: 0.131857
epoch:255, batch21, load frames use: 6.78116989136s
step 11750/400000: train loss: 0.156486
epoch:255, batch22, load frames use: 6.02448606491s
step 11751/400000: train loss: 0.120099
epoch:255, batch23, load frames use: 6.22780394554s
step 11752/400000: train loss: 0.148253
epoch:255, batch24, load frames use: 7.41184210777s
step 11753/400000: train loss: 0.134991
epoch:255, batch25, load frames use: 6.27444410324s
step 11754/400000: train loss: 0.132354
epoch:255, batch26, load frames use: 7.16742396355s
step 11755/400000: train loss: 0.139225
epoch:255, batch27, load frames use: 6.49988722801s
step 11756/400000: train loss: 0.127635
epoch:255, batch28, load frames use: 5.66919898987s
step 11757/400000: train loss: 0.128363
epoch:255, batch29, load frames use: 7.74942302704s
step 11758/400000: train loss: 0.136374
epoch:255, batch30, load frames use: 6.31589007378s
step 11759/400000: train loss: 0.132345
epoch:255, batch31, load frames use: 7.15866589546s
step 11760/400000: train loss: 0.136715
epoch:255, batch32, load frames use: 6.40042495728s
step 11761/400000: train loss: 0.114653
epoch:255, batch33, load frames use: 6.94032883644s
step 11762/400000: train loss: 0.125106
epoch:255, batch34, load frames use: 6.07032704353s
step 11763/400000: train loss: 0.135236
epoch:255, batch35, load frames use: 6.06711316109s
step 11764/400000: train loss: 0.133109
epoch:255, batch36, load frames use: 6.10444402695s
step 11765/400000: train loss: 0.124138
epoch:255, batch37, load frames use: 5.84060001373s
step 11766/400000: train loss: 0.139864
epoch:255, batch38, load frames use: 5.69573616982s
step 11767/400000: train loss: 0.135463
epoch:255, batch39, load frames use: 5.69505596161s
step 11768/400000: train loss: 0.128487
epoch:255, batch40, load frames use: 5.48910689354s
step 11769/400000: train loss: 0.146056
epoch:255, batch41, load frames use: 5.91950583458s
step 11770/400000: train loss: 0.136759
epoch:255, batch42, load frames use: 5.96738696098s
step 11771/400000: train loss: 0.137319
epoch:255, batch43, load frames use: 5.92382001877s
step 11772/400000: train loss: 0.135833
epoch:255, batch44, load frames use: 5.88044500351s
step 11773/400000: train loss: 0.154561
epoch:255, batch45, load frames use: 5.42280602455s
step 11774/400000: train loss: 0.145157
epoch:255, batch46, load frames use: 6.69881296158s
step 11775/400000: train loss: 0.128968
epoch:256, batch1, load frames use: 5.86563491821s
step 11776/400000: train loss: 0.139273
epoch:256, batch2, load frames use: 5.69492697716s
step 11777/400000: train loss: 0.133088
epoch:256, batch3, load frames use: 5.31408715248s
step 11778/400000: train loss: 0.131787
epoch:256, batch4, load frames use: 7.49905085564s
step 11779/400000: train loss: 0.155700
epoch:256, batch5, load frames use: 5.49354410172s
step 11780/400000: train loss: 0.142329
epoch:256, batch6, load frames use: 6.66717290878s
step 11781/400000: train loss: 0.124016
epoch:256, batch7, load frames use: 6.42991495132s
step 11782/400000: train loss: 0.126323
